{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F_DgERhjqZP0"
   },
   "source": [
    "# Chapter 2. 텍스트 전처리 (Text preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fwQfuYwNzU_C"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "n85zlo82z5An",
    "outputId": "de63d3e5-da5d-4a2c-d193-b09bdc3bcfaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.x selected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "33OVUxCiuNtk"
   },
   "source": [
    "# v01. 토큰화 (Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hqv-cEwKuQQE"
   },
   "source": [
    "- 자연어 처리에서 크롤링 등으로 얻어낸 코퍼스 데이터가 필요에 맞게 전처리되지 않은 상태라면, 해당 데이터를 사용하고자 하는 용도에 맞게 다음 3가지 작업을 하게 된다.\n",
    "  - 토큰화 (Tokenization)\n",
    "  - 정제 (Cleaning)\n",
    "  - 정규화 (Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZHqyLUUGwWai"
   },
   "source": [
    "**토큰화 (tokenization)**\n",
    "\n",
    "- 주어진 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업\n",
    "- 토큰의 단위 : 의미 있는 단위로 토큰을 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "46h3PTQpwipu"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3J4C5Q32wjW6"
   },
   "source": [
    "## 1.1 단어 토큰화 (Word Tokenization)\n",
    "\n",
    "- 토큰의 기준을 단어(word)로 하는 경우, 단어 토큰화(word tokenization)라고 한다.\n",
    "- 여기서 단어(word)는 단어 단위 이외에도 단어구, 의미를 갖는 문자열로도 간주되기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5PB0PbMw1Mj"
   },
   "source": [
    "**구두점 (punctuation)**\n",
    "\n",
    "- 온점(`.`), 콤마(`,`), 물음표(`?`), 세미콜론(`;`), 느낌표(`!`) 등과 같은 기호를 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jmOubcfwxE-W"
   },
   "source": [
    "- 보통 토큰화 작업은 단순히 구두점이나 특수문자를 전부 제거하는 정제(cleaning) 작업을 수행하는 것만으로 해결되지 않는다.\n",
    "- 구두점이나 특수문자를 전부 제거하면 토큰이 의미를 잃어버리는 경우가 발생하기도 한다.\n",
    "- 심지어 띄어쓰기 단위로 자르면 사실상 단어 토큰이 구분되는 영어와 달리, 한국어는 띄어쓰기만으로는 단어 토큰을 구분하기 어렵다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C89utPiFyaXR"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2VLfG13zyky-"
   },
   "source": [
    "## 1.2 토큰화 중 생기는 선택의 순간\n",
    "\n",
    "- 토큰화를 할 때, 예상하지 못한 경우가 있어서 **토큰화의 기준을 생각**해봐야 하는 경우가 발생한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R03Fd0YSyvP-"
   },
   "source": [
    "- 원하는 결과가 나오도록 토큰화 도구를 직접 설계할 수도 있지만, 기존에 공개된 도구들을 사용하였을 때의 결과가 사용자의 목적과 일치한다면 해당 도구를 사용할 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aGKInzQRy5bO"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HWGO950Wy6KG"
   },
   "source": [
    "### 1.2.1 `NLTK`\n",
    "\n",
    "- 영어 코퍼스를 토큰화하기 위한 도구들을 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "Z1DB-0YKzSLI",
    "outputId": "dc3cc742-9c8b-47df-b534-25fedcd6387b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "gJr5IR7yzGjV",
    "outputId": "56dd3068-2f16-4d6c-85d3-201a0bbcf89b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize  \n",
    "print(word_tokenize(\"Don't be fooled by the dark sounding name, \\\n",
    "Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "BcAdnbN-zMC2",
    "outputId": "111bc203-08dc-4602-96a2-75c071f5b962"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer  \n",
    "print(WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, \\\n",
    "Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o8Khhs3vzlV0"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0NUm6duozqJM"
   },
   "source": [
    "### 1.2.2 `text_to_word_sequence`\n",
    "\n",
    "- 케라스에서 지원하는 토큰화 도구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "tmEFZTAEzv3g",
    "outputId": "1dee7919-0d77-4399-d166-d5797c30a8cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "print(text_to_word_sequence(\"Don't be fooled by the dark sounding name, \\\n",
    "Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m5gXow91zy1U"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jeSpBFq00GIz"
   },
   "source": [
    "## 1.3 토큰화에서 고려해야 할 사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PQiGIHJ00Kkl"
   },
   "source": [
    "### 1.3.1 구두점이나 특수 문자를 단순 제외해서는 안 된다.\n",
    "\n",
    "- 코퍼스에 대한 정제 작업을 하다보면, 구두점조차도 하나의 토큰으로 분류하기도 한다.\n",
    "- ex1) 온점(`.`)\n",
    "  - 문장의 경계를 알 수 있는 데 도움이 됨  \n",
    "\n",
    "\n",
    "- ex2) 특수 문자의 달러(`$`)\n",
    "  - \"$45.55\"와 같이 가격을 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bc4l76Z40ws5"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PGoYMTCw0y0C"
   },
   "source": [
    "### 1.3.2 줄임말과 단어 내에 띄어쓰기가 있는 경우\n",
    "\n",
    "- 영어권 언어의 아포스트로피(`'`)는 압축된 단어를 다시 펼치는 역할을 하기도 한다.\n",
    "- \"New York\" 이라는 단어와 같이 하나의 단어이지만 중간에 띄어쓰기가 존재하는 경우도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x0vFxfUl1ItR"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PfPtjcEg491a"
   },
   "source": [
    "### 1.3.3 표준 토큰화 예제\n",
    "\n",
    "- 표준으로 쓰이고 있는 토큰화 방법 중 하나인 Penn Treebank Tokenization의 규칙에 대해 소개하고, 토큰화의 결과를 확인\n",
    "\n",
    "> 규칙 1. 하이푼으로 구성된 단어는 하나로 유지한다.  \n",
    "규칙 2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리해준다.\n",
    "\n",
    "- 해당 표준에 아래의 문장을 input으로 넣는다.\n",
    "\n",
    "> \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "Wd_wcch35QE5",
    "outputId": "0e40b413-da93-4574-c607-5bbde18e6606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "text=\"Starting a home-based restaurant may be an ideal. \\\n",
    "it doesn't have a food chain or restaurant of their own.\"\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8VvtLs-M5bbq"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IpXw1qxv5eAJ"
   },
   "source": [
    "## 1.4 문장 토큰화 (Sentence Tokenization)\n",
    "\n",
    "- 토큰의 단위가 문장(sentence)일 때, 어떻게 토큰화를 수행해야 할까?\n",
    "- 이 작업은 갖고 있는 코퍼스 내에서 문장 단위로 구분하는 작업이다.\n",
    "- 때로는 문장 분류(sentence segmentation)라고도 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wWb6Xmgu5zqh"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KkrxHiW_50sg"
   },
   "source": [
    "### 1.4.1 코퍼스의 문장 단위 분류 방법\n",
    "\n",
    "- 직관적으로 생각해봤을 때는 `?`나 온점(`.`)이나 `!` 기준으로 문장을 잘라내면 될 것 같다.\n",
    "  - `!`나 `?`는 문장의 구분을 위한 꽤 명확한 구분자(boundary) 역할을 한다.\n",
    "  - 하지만 온점은 문장의 끝이 아니더라도 등장할 수 있다.\n",
    "\n",
    "> Ex1) IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 ukairia777@gmail.com로 결과 좀 보내줘. 그러고나서 점심 먹으러 가자.\n",
    "\n",
    "> Ex2) Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\n",
    "\n",
    "- 위와 같은 예제에서 볼 수 있듯이 사용하는 코퍼스가 어떤 국적의 언어인지, 또는 해당 코퍼스 내에서 특수문자들이 어떻게 사용되고 있는 지에 따라서 직접 규칙들을 정의해볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xPuAMy4z6k4o"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c67mVzA16m83"
   },
   "source": [
    "### 1.4.2 NLTK의 `sent_tokenize`\n",
    "\n",
    "- NLTK는 영어 문장의 토큰화를 수행하는 `sent_tokenize`를 지원한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "ccMEe9H-6uAX",
    "outputId": "48e9e9ff-2012-48e9-ccaf-81438e96105a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to mae sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text=\"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. \\\n",
    "Finally, the barber went up a mountain and almost to the edge of a cliff. \\\n",
    "He dug a hole in the midst of some reeds. He looked about, to mae sure no one was near.\"\n",
    "\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V77B5S8_6z2Z"
   },
   "source": [
    "- 성공적으로 모든 문장을 구분해내었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "s1ptQEcS67pW",
    "outputId": "f3312b9c-ed48-4934-d7a9-17785a1988b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7YUxOUex6_jw"
   },
   "source": [
    "- NLTK는 단순히 온점을 구분자로 하여 문장을 구분하지 않았기 때문에 \"Ph.D.\"를 문장 내의 단어로 인식하여 성공적으로 인식한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "53lk3jcJ7Io-"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-H5ppt277Kyu"
   },
   "source": [
    "### 1.4.3 한국어 문장 토큰화 도구\n",
    "\n",
    "- `KSS` (Korean Sentence Splitter)\n",
    "  - 박상길님이 개발한 한국어에 대한 문장 토큰화 도구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "colab_type": "code",
    "id": "C7P334Sn7VlC",
    "outputId": "3893498b-2a1d-483b-d49a-3befc880b0a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kss\n",
      "  Downloading https://files.pythonhosted.org/packages/e3/e1/ff733dfcdf26212b4a56fd144a407ee939cbb2f24e71c0bc1abaf808264a/kss-1.2.5.tar.gz\n",
      "Building wheels for collected packages: kss\n",
      "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kss: filename=kss-1.2.5-cp36-cp36m-linux_x86_64.whl size=247595 sha256=0df4a944e4c576b0f48ad7b79b88410f65cb0181b65cef01d1c8a96a998810d6\n",
      "  Stored in directory: /root/.cache/pip/wheels/ac/9c/07/cbce306cb767e7428e4da5301e55834937ed1984ba564ca993\n",
      "Successfully built kss\n",
      "Installing collected packages: kss\n",
      "Successfully installed kss-1.2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "-VXQXYOp7XDH",
    "outputId": "ac6c8eea-3f22-4ae3-86ac-df775e3003c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어려워요.', '농담아니에요.', '이제 해보면 알걸요?']\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "text='딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. \\\n",
    "농담아니에요. 이제 해보면 알걸요?'\n",
    "print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vkC8J9uc7gaV"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oS17Ia617ixd"
   },
   "source": [
    "## 1.5 이진 분류기 (Binary Classifier)\n",
    "\n",
    "- 문장 토큰화에서의 예외 사항을 발생시키는 온점의 처리를 위해서 입력에 따라 두 개의 클래스로 분류하는 이진 분류기(binary classifier)를 사용하기도 한다.\n",
    "\n",
    "**두 개의 클래스**\n",
    "\n",
    "1. 온점(`.`)이 단어의 일부분일 경우. 즉, 온점이 약어(abbreivation)로 쓰이는 경우\n",
    "2. 온점(`.`)이 정말로 문장의 구분자(boundary)일 경우를 의미할 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LV3POEqh793f"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cHxplZsJ7-5k"
   },
   "source": [
    "## 1.6 한국어에서의 토큰화의 어려움\n",
    "\n",
    "- 한국어는 영어와는 달리 띄어쓰기만으로는 토큰화를 하기에 부족하다.\n",
    "- 한국어의 경우에는 띄어쓰기 단위가 되는 단위를 '어절'이라고 하는데 즉, 어절 토큰화는 한국어 NLP에서 지양되고 있다.  \n",
    "(어절 토큰화와 단어 토큰화가 같지 않기 때문)\n",
    "- 그 근본적인 이유는 한국어가 영어와는 다른 형태를 가지는 언어인 교착어라는 점에서 기인한다.\n",
    "\n",
    "**교착어**\n",
    "\n",
    "- 조사, 어미 등을 붙여서 말을 만드는 언어를 말한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jlv7vCBO9eaT"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FVkus4er9g-Z"
   },
   "source": [
    "### 1.6.1 한국어는 교착어이다.\n",
    "\n",
    "#### 1.6.1.1 조사\n",
    "\n",
    "- 영어와 달리 한국어에는 **조사**라는 것이 존재한다.\n",
    "\n",
    "- ex) '그(he/him)'라는 주어나 목적어가 들어간 문장이 있다고 하자\n",
    "  - 이 경우, '그'라는 단어 하나에도 '그가', '그에게', '그를', '그와', '그는'과 같이 다양한 조사가 '그'라는 글자 뒤에 띄어쓰기 없이 바로 붙게 된다.\n",
    "  - 이때, '그'라는 글자 뒤에 붙는 것들을 **조사**라고 한다.\n",
    "\n",
    "- 자연어 처리를 하다보면 같은 단어임에도 서로 다른 조사가 붙어서 다른 단어로 인식이 되면 자연어 처리가 힘들고 번거로워지는 경우가 많다.\n",
    "- 대부분의 한국어 NLP에서 조사는 분리해줄 필요가 있다.\n",
    "- 한국어는 어절이 독립적인 단어로 구성되는 것이 아니라 조사 등의 무언가가 붙어있는 경우가 많아서 이를 전부 분리해줘야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cwRCcsiU_go_"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VGFBG2nL_sAX"
   },
   "source": [
    "#### 1.6.1.2 형태소 (morpheme)\n",
    "\n",
    "- 한국어 토큰화에서는 **형태소(morpheme)**란 개념을 반드시 이해해야 한다.\n",
    "\n",
    "**형태소**\n",
    "\n",
    "- 뜻을 가진 가장 작은 말의 단위"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aENODFPlAE63"
   },
   "source": [
    "**형태소의 두 가지 형태**\n",
    "\n",
    "1. **자립 형태소**\n",
    "  - 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소\n",
    "  - 그 자체로 단어가 된다.\n",
    "  - 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등이 있다.\n",
    "2. **의존 형태소**\n",
    "  - 다른 형태소와 결합하여 사용되는 형태소\n",
    "  - 접사, 어미, 조사, 어간을 말한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vztdioxHAGKM"
   },
   "source": [
    "> \"에디가 딥러닝책을 읽었다.\"\n",
    "\n",
    "- 위 문장을 형태소 단위로 분해하면 다음과 같다.\n",
    "\n",
    "- 자립 형태소\n",
    "  - 에디\n",
    "  - 딥러닝책\n",
    "- 의존 형태소\n",
    "  - -가\n",
    "  - -을\n",
    "  - 읽-\n",
    "  - -었\n",
    "  - -다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VdFVfFYhAe65"
   },
   "source": [
    "- 한국어에서 영어에서의 단어 토큰화와 유사한 형태를 얻으러면 어절 토큰화가 아니라 형태소 토큰화를 수행해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4o2M5yfSAmHX"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hwOWtTJGAm3X"
   },
   "source": [
    "### 1.6.2 한국어는 띄어쓰기가 영어보다 잘 지켜지지 않는다.\n",
    "\n",
    "- 한국어는 영어권 언어와 비교하여 띄어쓰기가 어렵고, 또 잘 지켜지지 않는 경향이 있다.\n",
    "- 그 이유의 가장 기본적인 견해는 한국어의 경우 띄어쓰기가 지켜지지 않아도 글을 쉽게 이해할 수 있는 언어라는 점이다.  \n",
    "  \n",
    "\n",
    "- 반면, 영어의 경우에는 띄어쓰기를 하지 않으면 쉽게 알아보기 어려운 문장들이 생긴다.\n",
    "- 이는 한국어와 영어의 **언어적 특성의 차이**에 기인한다.\n",
    "  - 한국어 : 모아쓰기 방식\n",
    "  - 영어 : 풀어쓰기 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uA_SJJX0BGDO"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zm34eGUMBGxK"
   },
   "source": [
    "## 1.7 품사 태깅 (Part-of-speech tagging)\n",
    "\n",
    "- 단어는 표기는 같지만, 품사에 따라서 단어의 의미가 달라지기도 한다.\n",
    "  - ex) \"못\"\n",
    "    - 명사 : 망치를 사용해서 목재 따위를 고정하는 물건을 의미\n",
    "    - 부사 : \"먹는다\", \"달린다\"와 같은 동작 동사를 할 수 없다는 의미\n",
    "- 단어의 의미를 제대로 파악하기 위해서는 해당 단어가 어떤 품사로 쓰였는 지 보는 것이 주요 지표가 될 수 있다.\n",
    "- 그에 따라 단어 토큰화 과정에서 각 단어가 어떤 품사로 쓰였는 지를 구분해놓기도 하는 데, 이 작업을 **품사 태깅(part-of-speech tagging)**이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aue_ODOvBnbZ"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R0t65VlzBoOY"
   },
   "source": [
    "## 1.8 NLTK와 KoNLPy를 이용한 영어, 한국어 토큰화 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4C2yxVrzBvU5"
   },
   "source": [
    "### 1.8.1 NLTK를 이용한 영어 토큰화\n",
    "\n",
    "- NLTK에서는 영어 코퍼스에 품사 태깅 기능을 지원하고 있다.\n",
    "- 품사를 어떻게 명명하고, 태깅하는지의 기준은 여러가지가 있다.\n",
    "- NLTK에서는 \"Penn Treebank POS Tags\"라는 기준을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "JC0vEBEoCA1J",
    "outputId": "f3a8b835-28dc-4424-aa76-3d950fefcdb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text=\"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "z1ZKg1QNCHTC",
    "outputId": "efdb7ab4-e74d-4f23-c846-7b0f82245b93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "sCR4ZskZCDHg",
    "outputId": "686279d5-ae85-4593-d05e-47833227b5c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('actively', 'RB'),\n",
       " ('looking', 'VBG'),\n",
       " ('for', 'IN'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('students', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('and', 'CC'),\n",
       " ('you', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('student', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "x=word_tokenize(text)\n",
    "pos_tag(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHAwwciKCFuR"
   },
   "source": [
    "- Penn Treebank POS Tags에서의 각각의 명칭의 의미\n",
    "  - PRP : 인칭 대명사\n",
    "  - VBP : 동사\n",
    "  - RB : 부사\n",
    "  - VBG : 현재부사\n",
    "  - IN : 전치사\n",
    "  - NNP : 고유명사\n",
    "  - NNS : 복수형 명사\n",
    "  - CC : 접속사\n",
    "  - DT : 관사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WU8I8U7SCfAB"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WnCQe5HPCfuR"
   },
   "source": [
    "### 1.8.2 KoNLPy\n",
    "\n",
    "- 한국어 자연어 처리를 위해서는 KoNLPy라는 파이썬 패키지를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "colab_type": "code",
    "id": "EwFGnLVDDSXg",
    "outputId": "c89568ce-9a00-4bea-a2ec-cdc05e4f9067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
      "\u001b[K     |████████████████████████████████| 19.4MB 221kB/s \n",
      "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 10.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
      "Collecting tweepy>=3.7.0\n",
      "  Downloading https://files.pythonhosted.org/packages/36/1b/2bd38043d22ade352fc3d3902cf30ce0e2f4bf285be3b304a2782a767aec/tweepy-3.8.0-py2.py3-none-any.whl\n",
      "Collecting JPype1>=0.7.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/90/a94a55a58edfd67360fef85894bfb136a2c28b2cc7227d3a44dc508d5900/JPype1-0.7.1-cp36-cp36m-manylinux1_x86_64.whl (2.3MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3MB 42.5MB/s \n",
      "\u001b[?25hCollecting colorama\n",
      "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.6 in /tensorflow-2.1.0/python3.6 (from konlpy) (1.18.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /tensorflow-2.1.0/python3.6 (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.7.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /tensorflow-2.1.0/python3.6 (from tweepy>=3.7.0->konlpy) (1.14.0)\n",
      "Requirement already satisfied: requests>=2.11.1 in /tensorflow-2.1.0/python3.6 (from tweepy>=3.7.0->konlpy) (2.22.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /tensorflow-2.1.0/python3.6 (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /tensorflow-2.1.0/python3.6 (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /tensorflow-2.1.0/python3.6 (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (1.25.7)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /tensorflow-2.1.0/python3.6 (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /tensorflow-2.1.0/python3.6 (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
      "Installing collected packages: beautifulsoup4, tweepy, JPype1, colorama, konlpy\n",
      "  Found existing installation: beautifulsoup4 4.6.3\n",
      "    Uninstalling beautifulsoup4-4.6.3:\n",
      "      Successfully uninstalled beautifulsoup4-4.6.3\n",
      "  Found existing installation: tweepy 3.6.0\n",
      "    Uninstalling tweepy-3.6.0:\n",
      "      Successfully uninstalled tweepy-3.6.0\n",
      "Successfully installed JPype1-0.7.1 beautifulsoup4-4.6.0 colorama-0.4.3 konlpy-0.5.2 tweepy-3.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l2_cCe2nCo4A"
   },
   "source": [
    "#### 1.8.2.1 KoNLPy를 통해 사용할 수 있는 형태소 분석기\n",
    "\n",
    "- Okt(Open Korea Text) : 기존에 Twitter라는 이름을 갖고 있었으나 0.5.0 버전부터 이름이 변경됨\n",
    "- 메캅(Mecab)\n",
    "- 코모란(Komoran)\n",
    "- 한나눔(Hannanum)\n",
    "- 꼬꼬마(Kkma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EA2S3VGeEfRz"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xL3QEb1rCxjX"
   },
   "source": [
    "#### 1.8.2.2 한국어 NLP에서의 형태소 분석기의 사용\n",
    "\n",
    "- 한국어 NLP에서 형태소 분석기를 사용한다는 것은 단어 토큰화가 아니라 정확히는 **형태소(morpheme) 단위로 형태소 토큰화(morpheme tokenization)를 수행**하게 됨을 뜻한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xGE4wBR_D4yP"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DIguncfUD6OO"
   },
   "source": [
    "\n",
    "#### 1.8.2.3 Okt 형태소 분석기를 사용한 토큰화 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "i4gtlNINDM_8",
    "outputId": "7f6979d5-610b-4c01-bcfb-7ef44a747817"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "# morphs() : 형태소 추출\n",
    "print(okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "hVVn-whLDQO3",
    "outputId": "8959793b-7e20-459f-e6ff-098829bd4f89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n"
     ]
    }
   ],
   "source": [
    "# pos() : 품사 태깅(Parts-of-speech tagging)\n",
    "print(okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "LDsTP70ZDr5P",
    "outputId": "770fab88-b339-4ffa-e276-16bd1a41f02f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "# nouns() : 명사 추출\n",
    "print(okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YmvYBVUlDvNF"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r4p50gfvECg0"
   },
   "source": [
    "#### 1.8.2.4 KoNLPy의 형태소 분석기들의 공통적인 특징\n",
    "\n",
    "- KoNLPy의 형태소 분석기들은 공통적으로 위 3가지의 메서드(`morphs()`, `pos()` `nouns()`)들을 제공하고 있다.\n",
    "- 위 예제에서 형태소 추출과 품사 태깅의 결과를 보면, **조사를 기본적으로 분리하고 있음**을 확인할 수 있다.\n",
    "- 그렇기 때문에 한국어 NLP에서 전처리에 형태소 분석기를 사용하는 것은 꽤 유용하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OYuRJIIYEt3y"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azYeRYBFEu80"
   },
   "source": [
    "#### 1.8.2.5 꼬꼬마 형태소 분석기를 사용한 토큰화 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qypqD-QbEyog"
   },
   "outputs": [],
   "source": [
    "SENTENCE = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "xkkWUS8CE35U",
    "outputId": "f7371ace-6b0d-4a78-bda1-ecf0d976a717"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "\n",
    "kkma = Kkma()\n",
    "\n",
    "# morphs()\n",
    "print(kkma.morphs(SENTENCE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "4uVloDCQFQdr",
    "outputId": "af90a7e4-cef2-4763-bdbf-5c9498a6cc3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n"
     ]
    }
   ],
   "source": [
    "# pos()\n",
    "print(kkma.pos(SENTENCE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "U-3pOhrJFXFr",
    "outputId": "9ca72999-6457-4660-d47f-1148049349a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "# nouns()\n",
    "print(kkma.nouns(SENTENCE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_oVMJ6lZFasb"
   },
   "source": [
    "- Okt 형태소 분석기와 결과가 다르다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QCNX6htsFe_6"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FzQ0I5EjFfqS"
   },
   "source": [
    "- 각 형태소 분석기는 성능과 결과가 다르게 나오기 때문에, 형태소 분석기의 선택은 사용하고자 하는 필요 용도에 어떤 형태소 분석기가 가장 적절한지를 판단하고 사용하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IMqtMdx1Fnsi"
   },
   "source": [
    "**메캅(Mecab)**\n",
    "\n",
    "- 속도를 중시할 때 주로 사용되는 형태소 분석기"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch02_v01_Tokenization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
