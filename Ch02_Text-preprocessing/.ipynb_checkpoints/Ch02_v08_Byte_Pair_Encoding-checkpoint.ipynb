{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o_iCa98S-vTw"
   },
   "source": [
    "# Ch02. 텍스트 전처리 (Text Preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BLCY2zPd-3UU"
   },
   "source": [
    "# v08. 단어 분리하기 (Byte Pair Encoding, BPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WfKds6ZT-6u0"
   },
   "source": [
    "- 기계에게 아무리 많은 단어를 학습시켜도, 세상의 모든 단어를 알려줄 수 없다.\n",
    "- 그리고 더 많은 단어를 알려주려고 하면 그만큼 계산 비용도 늘어난다는 부담이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KW6WHhg3YjxF"
   },
   "source": [
    "**OOV 문제**\n",
    "\n",
    "- 기계가 훈련 단계에서 학습한 단어들을 모아놓은 것  \n",
    "$\\rightarrow$ 단어 집합 (vocabulary)\n",
    "- 테스트 단계에서 기계가 미처 배우지 못한 모르는 단어가 등장  \n",
    "$\\rightarrow$ OOV(Out-Of-Vocabulary) (단어 집합에 없는 단어란 의미), UNK(Unknown Token)라고 표현하기도 함\n",
    "- 기계가 문제를 풀 때, 모르는 단어가 등장하면(사람도 마찬가지지만) 주어진 문제를 푸는 것이 훨씬 어려워진다.\n",
    "- 이와 같이 모르는 단어로 인해 문제를 제대로 풀지 못하는 상황을 **OOV 문제**라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lWSODuOnY24l"
   },
   "source": [
    "**단어 분리(Subword segmentation) 작업**\n",
    "\n",
    "- 하나의 단어는 의미있는 여러 내부 단어들(subwords)의 조합으로 구성된 경우가 많다.\n",
    "- 단어를 여러 단어로 분리해서 단어를 이해해보겠다는 의도를 가진 전처리 작업\n",
    "- 실제로, 언어의 특성에 따라 영어권 언어나 한국어는 단어 분리를 시도했을 때 어느정도 의미있는 단위로 나누는 것이 가능하다.\n",
    "- 이제부터 이런 작업을 하는 토크나이저를 **단어 분리 토크나이저**라고 명명한다.  \n",
    "  \n",
    "\n",
    "- 단어 분리는 기계가 아직 배운 적이 없는 단어에 대해 어느 정도 대처할 수 있도록 한다.\n",
    "- 기계 번역 등에서 주요 전처리로 사용되고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dLCKtNDejq3C"
   },
   "source": [
    "**OOV 문제를 완화하는 대표적인 단어 분리 토크나이저**\n",
    "\n",
    "- BPE(Byte Pair Encoding) 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zx6vm-mCj0dX"
   },
   "source": [
    "**실무에서 사용할 수 있는 단어 분리 토크나이저 구현체**\n",
    "\n",
    "- 센텐스피스(Sentencepiece)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ip1gEA2Cj7Rv"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Ua7Lwzvj8jX"
   },
   "source": [
    "## 8.1 BPE (Byte Pair Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "21gMNGEUkEVv"
   },
   "source": [
    "### 8.1.1 BPE 알고리즘\n",
    "\n",
    "- 1994년에 제안된 데이터 압축 알고리즘\n",
    "- 하지만 후에 자연어 처리의 단어 분리 알고리즘으로 응용됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dhyaPg4JkOHX"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ECdumC5DkO1X"
   },
   "source": [
    "### 8.1.2 BPE의 작동 방법\n",
    "\n",
    "- 아래와 같은 문자열이 주어졌을 때, BPE을 수행한다고 하자.\n",
    "\n",
    "```\n",
    "aaabdaaabac\n",
    "```\n",
    "\n",
    "- BPE은 기본적으로 연속적으로 가장 많이 등장한 글자의 쌍을 찾아서 하나의 글자로 병합하는 방식을 수행한다.\n",
    "- 여기서는 글자 대신 바이트(byte)라는 표현을 사용한다.\n",
    "- ex) 위의 문자열 중 가장 자주 등장하고 있는 바이트의 쌍(byte pair) $\\rightarrow$ **'aa'**  \n",
    "  \n",
    "\n",
    "- 이 **'aa'**라는 바이트의 쌍을 하나의 바이트인 **'Z'** 치환해보자.\n",
    "\n",
    "```\n",
    "ZabdZabac\n",
    "Z=aa\n",
    "```\n",
    "\n",
    "- 이제 위 문자열 중에 가장 많이 등장하고 있는 바이트의 쌍 $\\rightarrow$ **'ab'**\n",
    "- **'ab'**를 **'Y'**로 치환\n",
    "\n",
    "```\n",
    "ZYdZYac\n",
    "Y=ab\n",
    "Z=aa\n",
    "```\n",
    "\n",
    "- 이제 가장 많이 등장하고 있는 바이트의 쌍 $\\rightarrow$ **'ZY'**\n",
    "- **'ZY'**를 **'X'**로 치환\n",
    "\n",
    "```\n",
    "XdXac\n",
    "X=ZY\n",
    "Y=ab\n",
    "Z=aa\n",
    "```\n",
    "\n",
    "- 이제 더 이상 병합할 바이트의 쌍은 없으므로 BPE는 위의 결과를 최종 결과로 하여 종료된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9eRtxRH0k8II"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLEm6wgmlnqc"
   },
   "source": [
    "## 8.2 자연어 처리에서의 BPE(Byte Pair Encoding)\n",
    "\n",
    "- [논문 링크](https://arxiv.org/pdf/1508.07909.pdf)  \n",
    "  \n",
    "\n",
    "- 자연어 처리에서의 BPE $\\rightarrow$ 단어 분리(word segmentation) 알고리즘\n",
    "- 기존에 있던 단어를 분리한다는 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-7ILKxfYluh8"
   },
   "source": [
    "**BPE 요약**\n",
    "\n",
    "- 글자(character) 단위에서 점차적으로 단어 집합(vocabulary)을 만들어 내는 **Bottom up** 방식의 접근을 사용\n",
    "- 우선 훈련 데이터에 있는 단어들을 모든 글자(characters) 또는 유니코드(unicode) 단위로 단어 집합(vocabulary)를 만듬\n",
    "- 가장 많이 등장하는 유니그램을 하나의 유니그램으로 통합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-_-IhtubmgKn"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kLkAMfy4mg-T"
   },
   "source": [
    "### 8.2.1 기존의 접근\n",
    "\n",
    "- 어떤 훈련 데이터로부터 각 단어들의 빈도수를 카운트했다고 가정\n",
    "- 그리고 각 단어와 각 단어의 빈도수가 기록되어져 있는 해당 결과는 임의로 딕셔너리(dictionary)란 이름을 붙였다.\n",
    "\n",
    "```python\n",
    "# dictionary\n",
    "# 훈련 데이터에 있는 단어와 등장 빈도수\n",
    "low : 5, lower : 2, newest : 6, widest : 3\n",
    "```\n",
    "\n",
    "- 이 훈련 데이터에는 각각의 단어가 다음과 같이 등장하였음을 의미한다.\n",
    "  - 'low'란 단어가 5회\n",
    "  - 'lower'란 단어가 2회\n",
    "  - 'newest'란 단어가 6회\n",
    "  - 'widest'란 단어가 3회  \n",
    "\n",
    "\n",
    "- 그렇다면 딕셔너리로부터 이 훈련 데이터의 단어 집합(vocabulary)을 얻는 것은 간단하다.\n",
    "\n",
    "```python\n",
    "# vocabulary\n",
    "low, lower, newest, widest\n",
    "```\n",
    "\n",
    "- 단어 집합은 중복을 배제한 단어들의 집합을 의미한다.\n",
    "- 그리고 이 경우 테스트 과정에서 'lowest'란 단어가 등장한다면 기계는 이 단어를 학습한 적이 없으므로 해당 단어에 대해서 제대로 대응하지 못하는 OOV 문제가 발생한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HqqA40x4nkwa"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZzDUDy_3p_nm"
   },
   "source": [
    "### 8.2.2 BPE 알고리즘을 사용한 경우\n",
    "\n",
    "- 위의 딕셔너리에 BPE를 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "blsHMkHonmAx"
   },
   "source": [
    "#### 8.2.2.1 글자 단위 분리\n",
    "\n",
    "- 우선 딕셔너리의 모든 단어들을 글자(character) 단위로 분리한다.\n",
    "- 이 경우 딕셔너리는 아래와 같다.\n",
    "\n",
    "```python\n",
    "# dictionary\n",
    "l o w : 5,  l o w e r : 2,  n e w e s t : 6,  w i d e s t : 3\n",
    "```\n",
    "\n",
    "- 이제부터 딕셔너리는 자신 또한 업데이트되며 앞으로 단어 집합을 업데이트하기 위해 지속적으로 참고되는 참고 자료의 역할을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SXX-uqNEqE6V"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MTiSS1mvpxHW"
   },
   "source": [
    "#### 8.2.2.2 초기 단어 집합\n",
    "\n",
    "- 딕셔너리를 참고로 한 초기 단어 집합(vocabulary)\n",
    "  - **초기 구성은 글자 단위로 분리된 상태**\n",
    "\n",
    "```python\n",
    "# vocabulary\n",
    "l, o, w, e, r, n, w, s, t, i, d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Q2pqzp8p8Ts"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l2_m-cY6qPbz"
   },
   "source": [
    "#### 8.2.2.3 알고리즘 동작 횟수 지정\n",
    "\n",
    "- BPE의 특징은 알고리즘의 동작을 몇 회 반복(iteration)할 것인지를 사용자가 정한다는 점이다.\n",
    "- 여기서는 총 10회를 수행한다고 가정\n",
    "- 다시 말해 **가장 빈도수가 높은 유니그램의 쌍을 하나의 유니그램으로 통합**하는 과정을 총 10회 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ir3cJ4ltuVVn"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.2.2.4 알고리즘 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_odmWV-yqhc7"
   },
   "source": [
    "**1회**\n",
    "\n",
    "- 딕셔너리를 참고로 했을 때 빈도수가 9로 가장 높은 `(e,s)`의 쌍을 `es`로 통합\n",
    "\n",
    "```python\n",
    "# dictionary update!\n",
    "l o w : 5,\n",
    "l o w e r : 2,\n",
    "n e w es t : 6,\n",
    "w i d es t : 3\n",
    "```\n",
    "\n",
    "```python\n",
    "# vocabulary update!\n",
    "l, o, w, e, r, n, w, s, t, i, d, es\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3cbZR4oxq27a"
   },
   "source": [
    "**2회**\n",
    "\n",
    "- 빈도수가 9로 가장 높은 `(es, t)`의 쌍을 `est`로 통합\n",
    "\n",
    "```python\n",
    "# dictionary update!\n",
    "l o w : 5,\n",
    "l o w e r : 2,\n",
    "n e w est : 6,\n",
    "w i d est : 3\n",
    "```\n",
    "\n",
    "```python\n",
    "# vocabulary update!\n",
    "l, o, w, e, r, n, w, s, t, i, d, es, est\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bmY8m50_rFoa"
   },
   "source": [
    "**3회**\n",
    "\n",
    "- 빈도수가 7로 가장 높은 `(l, o)`의 쌍을 `lo`로 통합\n",
    "\n",
    "```python\n",
    "# dictionary update!\n",
    "lo w : 5,\n",
    "lo w e r : 2,\n",
    "n e w est : 6,\n",
    "w i d est : 3\n",
    "```\n",
    "\n",
    "```python\n",
    "# vocabulary update!\n",
    "l, o, w, e, r, n, w, s, t, i, d, es, est, lo\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DsOBo8Oxtall"
   },
   "source": [
    "- 이와 같은 방식으로 총 10회 반복하였을 때 얻은 딕셔너리와 단어 집합은 아래와 같다.\n",
    "\n",
    "```python\n",
    "# dictionary update!\n",
    "low : 5,\n",
    "low e r : 2,\n",
    "newest : 6,\n",
    "widest : 3\n",
    "```\n",
    "\n",
    "```python\n",
    "# vocabulary update!\n",
    "l, o, w, e, r, n, w, s, t, i, d, es, est, lo, low, ne, new, newest, wi, wid, widest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VZADx3BKtqKt"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 8.2.2.5 결과\n",
    "\n",
    "- 이 경우 테스트 과정에서 `'lowest'`란 단어가 등장한다면, BPE 알고리즘을 사용한 위의 단어 집합에서는 더이상 `'lowest'`는 OOV가 아니다.\n",
    "  - 기계는 우선 `'lowest'`를 전부 글자 단위로 분할한다.\n",
    "  - 즉, `'l, o, w, e, s, t'`가 된다.\n",
    "  - 그리고 기계는 위의 단어 집합을 참고로 하여 `'low'`와 `'est'`를 찾아낸다.\n",
    "  - 즉, `'lowest'`를 기계는 `'low'`와 `'est'` 두 단어로 인코딩한다.\n",
    "  - 그리고 이 두 단어는 둘 다 단어 집합에 있는 단어이므로 OOV가 아니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z4QEa7p-uR8G"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.2.3 코드 실습하기\n",
    "\n",
    "- 논문에서 공개한 코드를 통해 실습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PlL4OkAIumC-"
   },
   "outputs": [],
   "source": [
    "import re, collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eUACJ1o4unUd"
   },
   "source": [
    "<br>\n",
    "\n",
    "- BPE를 몇 회 수행할 것인지를 정한다.\n",
    "- 여기서는 10회로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wHScNEmSuthG"
   },
   "outputs": [],
   "source": [
    "num_merges = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ko8CrKLAuyGr"
   },
   "source": [
    "<br>\n",
    "\n",
    "- BPE에 사용할 단어가 `low, lower, newest, widest`일 때, BPE의 입력으로 사용하는 실제 단어 집합은 아래와 같다.\n",
    "  - `</w>`는 단어의 맨 끝에 붙이는 특수 문자\n",
    "  - 각 단어는 글자(character) 단위로 분리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rvpCH_k1yrOT"
   },
   "outputs": [],
   "source": [
    "vocab = {'l o w </w>' : 5,\n",
    "         'l o w e r </w>' : 2,\n",
    "         'n e w e s t </w>':6,\n",
    "         'w i d e s t </w>':3\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ic--cH6xytdE"
   },
   "source": [
    "<br>\n",
    "\n",
    "- BPE의 코드는 아래와 같다.\n",
    "- 알고리즘은 위에서 설명했던 것과 동일하게 가장 빈도수가 높은 유니그램의 쌍을 하나의 유니그램으로 통합하는 과정이다.\n",
    "- `num_merges`회 반복한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "rOPVwzfFy4fe",
    "outputId": "0f48f67b-32ed-419f-9a68-0071b5c4c264"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e', 's')\n",
      "('es', 't')\n",
      "('est', '</w>')\n",
      "('l', 'o')\n",
      "('lo', 'w')\n",
      "('n', 'e')\n",
      "('ne', 'w')\n",
      "('new', 'est</w>')\n",
      "('low', '</w>')\n",
      "('w', 'i')\n"
     ]
    }
   ],
   "source": [
    "def get_stats(vocab):\n",
    "  pairs = collections.defaultdict(int)\n",
    "\n",
    "  for word, freq in vocab.items():\n",
    "    symbols = word.split()\n",
    "\n",
    "    for i in range(len(symbols) - 1):\n",
    "      pairs[symbols[i], symbols[i+1]] += freq\n",
    "\n",
    "  return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "  v_out = {}\n",
    "  bigram = re.escape(' '.join(pair))\n",
    "  p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "\n",
    "  for word in v_in:\n",
    "    w_out = p.sub(''.join(pair), word)\n",
    "    v_out[w_out] = v_in[word]\n",
    "\n",
    "  return v_out\n",
    "\n",
    "for i in range(num_merges):\n",
    "  pairs = get_stats(vocab)\n",
    "  best = max(pairs, key=pairs.get)\n",
    "  vocab = merge_vocab(best, vocab)\n",
    "  print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GZ8pNYy3z7jF"
   },
   "source": [
    "- `e`와 `s`의 쌍은 초기 단어 집합에서 총 9회 등장했다.\n",
    "- 그렇기 때문에 `es`로 통합된다.\n",
    "- 그 다음으로는 `es`와 `t`의 쌍을, 그 다음으로는 `est`와 `</w>`의 쌍을 통합시킨다.\n",
    "- 빈도수가 가장 높은 순서대로 통합하는 이 과정을 `num_merges` 회 반복한 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgX1ZpSS0O9B"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 8.3 WPM (Wordpiece Model)\n",
    "\n",
    "- [WPM의 아이디어를 제시한 논문](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/37842.pdf)\n",
    "- [구글이 위 WPM을 변형하여 번역기에 사용했다는 논문](https://arxiv.org/pdf/1609.08144.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hUQegxQ80jxg"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.3.1 또 다른 단어 분리 토크나이저\n",
    "\n",
    "- 기존의 BPE 외에도 아래와 같은 단어 분리 토크나이저들이 존재한다.\n",
    "  - WPM(Wordpiece Model)\n",
    "  - Unigram Language Model Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8LA4ESV2xfb"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MbAuiG7t2y00"
   },
   "source": [
    "### 8.3.2 WPM\n",
    "\n",
    "- 구글은 WPM을 일종의 BPE의 변형으로 소개한다.\n",
    "- WPM은 BPE과는 달리 빈도수가 아니라 **우도(likelihood)**를 통해서 단어를 분리한다.\n",
    "- 구글은 자신들의 구글 변역기에서 WPM이 수행된 결과에 대해서 기술했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GHUnTg4A3HNO"
   },
   "source": [
    "**WPM을 수행하기 이전의 문장**\n",
    "\n",
    "```\n",
    "Jet makers feud over seat width with big orders at stake\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wS70OTBt3M4k"
   },
   "source": [
    "**WPM을 수행한 결과 (wordpieces)**\n",
    "\n",
    "```\n",
    "_J et _makers _fe ud _over _seat _width _with _big _orders _at _stake\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m65TFEaO3Tn7"
   },
   "source": [
    "- `Jet`는 `J`와 `et`로 나누어짐\n",
    "- `feud`는 `fe`와 `ud`로 나누어짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMLvMvtP3dJY"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.3.3 WPM의 단어 분리 방법\n",
    "\n",
    "- WPM은 모든 단어의 맨 앞에 `_`를 붙이고, 단어는 내부단어(subword)로 통계에 기반하여 띄어쓰기로 분리한다.\n",
    "- 여기서 언더바 `_`는 문장 복원을 위한 장치이다.\n",
    "  - ex) `Jet` $\\rightarrow$ `_J et`와 같이 기존에 없던 띄어쓰기가 추가되어 내부 단어(subwords)들을 구분하는 구분자 역할을 하고 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "87hK-THq3gJ9"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.3.4 기본에 있던 띄어쓰기와 구분자 역할의 띄어쓰기 구별하는 방법\n",
    "\n",
    "- 이 역할을 수행하는 것이 단어들 앞에 붙은 언더바 `_`이다.\n",
    "- WPM이 수행된 결과로부터 다시 수행 전의 결과로 되돌리는 방법\n",
    "  - 현재 있는 모든 띄어쓰기를 전부 제거하고, 언더바를 띄어쓰기로 바꾸면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BuYN1skW4kBn"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 8.4 센텐스피스 (Sentencepiece)\n",
    "\n",
    "- [논문](https://arxiv.org/pdf/1808.06226.pdf)\n",
    "- [센텐스피스 깃허브](https://github.com/google/sentencepiece)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "piUR75984vKA"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.4.1 실무에서 단어 분리를 위해 사용하는 구현체\n",
    "\n",
    "- 구글의 센텐스피스(Sentencepiece)를 사용\n",
    "- 구글은 BPE 알고리즘과 Unigram Language Model Tokenizer를 구현한 센텐스피스를 깃허브에 공개했다.\n",
    "- 기존의 BPE 알고리즘 논문 저자 또한 BPE 코드를 깃허브에 공개하기는 했지만, 이를 실무에 사용하기에는 속도가 매우 느리므로 센텐스피스를 사용하는 것을 권장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_pOJkfd5RqK"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.4.2 센텐스피스의 또 다른 장점\n",
    "\n",
    "- 단어 분리 알고리즘을 사용하기 위해서, 데이터에 단어 토큰화를 먼저 진행한 상태여야 한다  \n",
    "$\\rightarrow$ 이 단어 분리 알고리즘을 모든 언어에 사용하는 것은 쉽지 않다.\n",
    "- 영어와 달리 한국어와 같은 언어는 단어 토큰화부터가 쉽지 않다.  \n",
    "\n",
    "\n",
    "- 그런데, 이런 사전 토큰화 작업(pretokenization)없이 전처리를 하지 않은 데이터(raw data)에 바로 단어 분리 토크나이저를 사용할 수 있다면, 이 토크나이저는 그 어떤 언어에도 적용할 수 있는 토크나이저가 될 것이다.  \n",
    "  \n",
    "\n",
    "- 센텐스피스는 이 이점을 살려서 구현됐다.\n",
    "- 센텐스피스는 사전 토큰화 작업없이 단어 분리 토큰화를 수행하므로 언어에 종속되지 않는다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch02_v08_Byte-Pair_Encoding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
