{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7p6BaI5rGVH6"
   },
   "source": [
    "# Ch02. 텍스트 전처리 (Text preprocessing)\n",
    "\n",
    "# v02. 정제(Cleaning) and 정규화(Normalization)\n",
    "\n",
    "- 토큰화(tokenization) : 코퍼스에서 용도에 맞게 토큰을 분류하는 작업\n",
    "- 토큰화 작업 전, 후에는 텍스트 데이터를 용도에 맞게 정제(cleaning) 및 정규화(normalization)하는 일이 항상 함께한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IgqmYeQ7JrxC"
   },
   "source": [
    "**정제 및 정규화의 목적**\n",
    "\n",
    "- 정제(cleaning)\n",
    "  - 갖고 있는 코퍼스로부터 노이즈 데이터를 제거  \n",
    "\n",
    "\n",
    "- 정규화(normalization)\n",
    "  - 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cElbwvFyKQ0B"
   },
   "source": [
    "**정제 작업의 시점**\n",
    "\n",
    "- 정제 작업은 토큰화 작업에 방해가 되는 부분들을 배제시키고 토큰화 작업을 수행하기 위해서 토큰화 작업보다 앞서 이루어지기도 한다.\n",
    "- 하지만, 토큰화 작업 이후에도 여전히 남아있는 노이즈들을 제거하기 위해 지속적으로 이루어지기도 한다.\n",
    "- 환벽한 정제 작업은 어려우므로, 대부분의 경우 \"이 정도면 됐다.\"라는 일종의 합의점을 찾기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2qWVJX67SgY2"
   },
   "source": [
    "**정제 및 정규화 기법들**\n",
    "\n",
    "1. 규칙에 기반한 표기가 다른 단어들의 통합\n",
    "2. 대, 소문자 통합\n",
    "3. 불필요한 단어의 제거 (Romoving Unnecessary Words)\n",
    "4. 정규 표현식 (Regular Expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zc7xDbnwK8Di"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6vF8ankxLApp"
   },
   "source": [
    "## 2.1 규칙에 기반한 표기가 다른 단어들의 통합\n",
    "\n",
    "- 필요에 따라 직접 정의할 수 있는 정규화 규칙의 예\n",
    "  - 같은 의미를 갖고있음에도, 표기가 다른 단어들을 하나의 단어로 정규화하는 방법\n",
    "\n",
    "\n",
    "- ex) USA와 US는 같은 의미를 가지므로, 하나의 단어로 정규화할 수 있다.\n",
    "- ex) uh-huh와 uhhuh는 형태는 다르지만 같은 의미를 갖고 있다.  \n",
    "  \n",
    "\n",
    "- 표기가 다른 단어들을 통합하는 방법\n",
    "  - 어간 추출(stemming)\n",
    "  - 표제어 추출(lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KvmiqdZzQ5PX"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J5WuPioTSzR6"
   },
   "source": [
    "## 2.2 대, 소문자 통합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YXwDZfoKQ6Rs"
   },
   "source": [
    "### 2.2.1 대문자를 소문자로 변환\n",
    "- 영어권 언어에서 대, 소문자를 통합하는 것은 단어의 개수를 줄일 수 있는 또 다른 정규화 방법이다.\n",
    "- 영어권 언어에서 대문자는 문장의 맨 앞 등과 같은 특정 상황에서만 쓰이고, 대부분의 글은 소문자로 작성되기 때문  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QcJ6N1G6Q-FX"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAL2VBtAR5SC"
   },
   "source": [
    "### 2.2.2 대문자와 소문자가 구분되어야 하는 경우\n",
    "\n",
    "- 그렇다고 해서 대문자와 소문자를 무작정 통합해서는 안된다.\n",
    "- 대문자와 소문자가 구분되어야 하는 경우도 있기 때문\n",
    "  - ex) \"미국\"을 뜻하는 단어 \"US\" vs \"우리\"를 뜻하는 \"us\"\n",
    "  - ex) 회사 이름 (General Motors)\n",
    "  - ex) 사람 이름 (Bush)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Loqn-ETBR9m9"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SF2RmnJPR-QS"
   },
   "source": [
    "### 2.2.3 일부만 소문자로 변환\n",
    "\n",
    "- 문장의 맨 앞에서 나오는 단어의 대문자만 소문자로 바꾸고, 다른 단어들은 전부 대문자인 상태로 놔둔다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lLlDy19zSIoy"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pzZ6qyrPSJ0J"
   },
   "source": [
    "### 2.2.4 소문자 변환 사용 시점 결정 시퀀스 모델\n",
    "\n",
    "- 이러한 작업은 더 많은 변수를 사용해서 소문자 변환을 언제 사용할 지 결정하는 **머신 러닝 시퀀스 모델**로 더 정확하게 진행시킬 수 있다.\n",
    "- 그렇지만 예외 사항을 크게 고려하지 않고, 모든 코퍼스를 소문자로 바꾸는 것이 종종 더 실용적인 해결책이 되기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XeYKcQbkScSJ"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lUjX_5CFSd5J"
   },
   "source": [
    "## 2.3 불필요한 단어의 제거 (Removing Unnecessary Words)\n",
    "\n",
    "**노이즈 데이터(noise data)**\n",
    "\n",
    "- 정제 작업에서 제거해야하는 대상\n",
    "- 자연어가 아니면서 아무 의미도 갖지 않는 글자들(특수 문자 등)\n",
    "- 분석하고자 하는 목적에 맞지 않는 불필요 단어들\n",
    "\n",
    "**불필요한 단어들을 제거하는 방법**\n",
    "\n",
    "- 불용어 제거\n",
    "- 등장 빈도가 적은 단어 제거\n",
    "- 길이가 짧은 단어들 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6LwXsdZTOKt"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bJY5WwILTPHH"
   },
   "source": [
    "### 2.3.1 등장 빈도가 적은 단어 (Removing Rare words)\n",
    "\n",
    "- ex) 입력된 메일이 정상 메일인 지 스팸 메일인 지를 분류하는 스팸 메일 분류기 설계\n",
    "  - 총 100,000개의 메일을 가지고 정상 메일에서 주로 등장하는 단어와 스팸 메일에서 주로 등장하는 단어를 가지고 설계\n",
    "  - 이 때 100,000개의 메일 데이터에서 총 합 5번 밖에 등장하지 않는 단어가 있다.\n",
    "  - 이 단어는 직관적으로 분류에 거의 도움이 되지 않을 것임을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3vXstvc-ToYh"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ZffOkCxTpM_"
   },
   "source": [
    "### 2.3.2 길이가 짧은 단어 (Removing words with very a short length)\n",
    "\n",
    "- 영어권 언어에서는 길이가 짧은 단어를 삭제하는 것만으로도 어느 정도 자연어 처리에서 크게 의미가 없는 단어들을 제거하는 효과를 볼 수 있다고 알려져 있다.\n",
    "- 길이가 짧은 단어를 제거하는 2차 이유\n",
    "  - 길이를 조건으로 텍스트를 삭제하면서 단어가 아닌 구두점들까지도 한꺼번에 제거하기 위함  \n",
    "\n",
    "  \n",
    "- 하지만 한국어에서는 길이가 짧은 단어라고 삭제하는 방법은 크게 유효하지 않을 수 있다.\n",
    "  - 한국어 단어는 한자어가 많다. (한 글자만으로도 이미 의미를 가진 경우가 많음)  \n",
    "\n",
    "\n",
    "- 영어는 길이가 2~3 이하인 단어를 제거하는 것만으로도 크게 의미를 갖지 못하는 단어를 줄이는 효과를 갖고 있다.\n",
    "  - 길이가 1인 단어 제거  \n",
    "  $\\rightarrow$ 관사 `'a'`와 주어로 쓰이는 `'I'`가 제거됨\n",
    "  - 길이가 2인 단어 제거  \n",
    "  $\\rightarrow$ `'it'`, `'at'`, `'to'`, `'on'`, `'in'`, `'by'` 등과 같은 대부분의 불용어에 해당되는 단어들이 제거됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Hm186XgwUzz3",
    "outputId": "94aa45a2-76a8-44a7-ccff-419e3488a8c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was wondering anyone out there could enlighten this car.\n"
     ]
    }
   ],
   "source": [
    "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
    "import re\n",
    "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
    "\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "print(shortword.sub('', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P_9t3MM1VACM"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S48uoqBRVCck"
   },
   "source": [
    "## 2.4 정규 표현식 (Regular Expression)\n",
    "\n",
    "- 얻어낸 코퍼스에서 노이즈 데이터의 특징을 잡아낼 수 있다면, 정규 표현식을 통해서 이를 제거할 수 있다.\n",
    "  - ex) HTML 문서의 HTML 태그 제거"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch02_v02_Cleaning-and-Normalization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
