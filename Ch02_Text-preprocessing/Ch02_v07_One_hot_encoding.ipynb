{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9kV_AwFmqPL6"
   },
   "source": [
    "# Ch02. 텍스트 전처리 (Text Preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UtQ7igp3qU3r"
   },
   "source": [
    "# v07. 원-핫 인코딩 (One-hot encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmzjcdb8qYhq"
   },
   "source": [
    "**원-핫 인코딩 (One-hot encoding)**\n",
    "\n",
    "- 자연어 처리에서 문자를 숫자로 바꾸는 기법\n",
    "- 단어를 표현하는 가장 기본적인 표현 방법\n",
    "- 머신러닝, 딥러닝을 하기 위해 반드시 배워야 하는 표현 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mEXybeUsujdK"
   },
   "source": [
    "**단어 집합(vocabulary)**\n",
    "\n",
    "- 서로 다른 단어들의 집합\n",
    "- 단어 집합에서는 기본적으로 book과 books와 같이 단어의 변형 형태도 다른 단어로 간주한다.\n",
    "- 원-핫 인코딩을 위해서 먼저 해야 할 일은 단어 집합을 만드는 일이다.\n",
    "- 텍스트의 모든 단어를 중복을 허용하지 않고 모아놓으면 이를 단어 집합이라고 한다.\n",
    "- 그리고 이 단어 집합에 고유한 숫자를 부여하는 정수 인코딩을 진행한다.\n",
    "- 텍스트에 단어가 5,000개가 존재한다면, 단어 집합의 크기는 5,000이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GTpCPl0_vEEk"
   },
   "source": [
    "- 이제 각 단어에 고유한 정수 인덱스를 부여했다.\n",
    "- 이 숫자로 바뀐 단어들을 **벡터**로 다루고 싶다면 어떻게 해야 할까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2R8O3MV0vKlI"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PMRj4XO3vOhw"
   },
   "source": [
    "## 7.1 원-핫 인코딩(One-hot encoding)이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0TK7WluUvRRY"
   },
   "source": [
    "### 7.1.1 원-핫 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S6FID0jXvafZ"
   },
   "source": [
    "- 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식\n",
    "- 이렇게 표현된 벡터를 **원-핫 벡터(One-hot vector)**라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YaiBhRBRv1vI"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYA8RCZBv2aY"
   },
   "source": [
    "### 7.1.2 원-핫 인코딩의 2가지 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YevEhOG2v6K4"
   },
   "source": [
    "1. 각 단어에 고유한 인덱스를 부여한다. (정수 인코딩)\n",
    "2. 표현하고 싶은 단어의 인덱스의 위치에 1을 부여하고, 다른 단어의 인덱스 위치에는 0을 부여한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-VANMhpqwByg"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJszrL3swDUn"
   },
   "source": [
    "### 7.1.3 원-핫 벡터 만들지 예제\n",
    "\n",
    "> 문장 : \"나는 자연어 처리를 배운다\"\n",
    "\n",
    "- 위 문장에 대해서 원-핫 인코딩을 진행하는 코드는 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "colab_type": "code",
    "id": "IuOQKedDwXMm",
    "outputId": "5602a522-a32b-43cd-8418-979a22081d2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
      "\u001b[K     |████████████████████████████████| 19.4MB 1.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
      "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.17.5)\n",
      "Collecting colorama\n",
      "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
      "Collecting tweepy>=3.7.0\n",
      "  Downloading https://files.pythonhosted.org/packages/36/1b/2bd38043d22ade352fc3d3902cf30ce0e2f4bf285be3b304a2782a767aec/tweepy-3.8.0-py2.py3-none-any.whl\n",
      "Collecting beautifulsoup4==4.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 11.4MB/s \n",
      "\u001b[?25hCollecting JPype1>=0.7.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/90/a94a55a58edfd67360fef85894bfb136a2c28b2cc7227d3a44dc508d5900/JPype1-0.7.1-cp36-cp36m-manylinux1_x86_64.whl (2.3MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3MB 57.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.7.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.12.0)\n",
      "Requirement already satisfied: requests>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.21.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
      "Installing collected packages: colorama, tweepy, beautifulsoup4, JPype1, konlpy\n",
      "  Found existing installation: tweepy 3.6.0\n",
      "    Uninstalling tweepy-3.6.0:\n",
      "      Successfully uninstalled tweepy-3.6.0\n",
      "  Found existing installation: beautifulsoup4 4.6.3\n",
      "    Uninstalling beautifulsoup4-4.6.3:\n",
      "      Successfully uninstalled beautifulsoup4-4.6.3\n",
      "Successfully installed JPype1-0.7.1 beautifulsoup4-4.6.0 colorama-0.4.3 konlpy-0.5.2 tweepy-3.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4XAS0e8DxFaG"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBOebVtWxGrp"
   },
   "source": [
    "#### 7.1.3.1 문장에 대한 토큰화\n",
    "\n",
    "- KoNLPy의 Okt 형태소 분석기를 통해서 문장에 대한 토큰화를 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ExSby4kOwKUw",
    "outputId": "174278be-506c-42c7-9c85-9d71666d3957"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', '자연어', '처리', '를', '배운다']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "token = okt.morphs(\"나는 자연어 처리를 배운다\")\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D1Hxo1yBw1Xj"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Zju-VbqxNjz"
   },
   "source": [
    "#### 7.1.3.2 고유 인덱스 부여\n",
    "\n",
    "- 각 토큰에 대해서 고유한 인덱스(index)를 부여했다.\n",
    "- 빈도수 순서대로 단어를 정렬하여 고유한 인덱스를 부여하는 작업이 사용되기도 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qp4rke4UwpU-"
   },
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "\n",
    "for voca in token:\n",
    "  if voca not in word2index.keys():\n",
    "    word2index[voca] = len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "gGAxpDRnwx32",
    "outputId": "316f89e4-e948-4d1c-e1cd-d539cceaec98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
     ]
    }
   ],
   "source": [
    "print(word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nMPsY5FMwzE2"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2pMEOF5SxCpe"
   },
   "source": [
    "#### 7.1.3.3 원-핫 벡터 생성 함수\n",
    "\n",
    "- 토큰을 입력하면 해당 토큰에 대한 원-핫 벡터를 만들어내는 함수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P8K4sPVNxDa-"
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(word, word2index):\n",
    "  one_hot_vector = [0]*(len(word2index))\n",
    "  index = word2index[word]\n",
    "  one_hot_vector[index] = 1\n",
    "  return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "qDx8CNCHxpMo",
    "outputId": "e84589dc-8e14-4fde-9126-8e43fe1231ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding(\"자연어\", word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z6f295uRxsod"
   },
   "source": [
    "- 해당 함수에 \"자연어\"라는 토큰을 입력으로 넣었더니 `[0, 0, 1, 0, 0, 0]`라는 벡터가 출력됐다.\n",
    "- 자연어는 단어 집합에서 인덱스가 2이므로, 자연어를 표현하는 원-핫 벡터는 인덱스 2의 값이 1이며, 나머지 값은 0인 벡터가 나온다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jbksV_Jkx4T0"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-GR9GXQnx4-k"
   },
   "source": [
    "## 7.2 케라스(Keras)를 이용한 원-핫 인코딩(One-hot encoding) : `to_categorical()`\n",
    "\n",
    "- 케라스는 원-핫 인코딩을 수행하는 유용한 도구 `to_categorical()`을 지원한다.\n",
    "- 이번에는 케라스만으로 정수 인코딩과 원-핫 인코딩을 순차적으로 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "T-UWqYPDyG5U",
    "outputId": "7afa894c-23c5-48b1-896f-a95e9e954437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.x selected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oNtjrO6CyIjM"
   },
   "outputs": [],
   "source": [
    "text=\"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AWZn302JyMMs"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3le2UR2TyOH8"
   },
   "source": [
    "### 7.2.1 정수 인코딩\n",
    "\n",
    "- 케라스 토크나이저(`Tokenizer`)를 이용한 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "MHdX5FdpyTLv",
    "outputId": "7c652f16-5477-4d31-f5b8-632504ca7964"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts([text])\n",
    "print(t.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o_NUQ2Bdyhdj"
   },
   "source": [
    "- 위와 같이 생성된 단어 집합(vocabulary)에 있는 단어들로만 구성된 텍스트가 있다.\n",
    "- 이 텍스트를 `texts_to_sequences()`를 통해서 정수 시퀀스로 변환 가능하다.\n",
    "- 생성된 단어 집합 내의 일부 단어들로만 구성된 서브 텍스트인 `sub_text`를 만들어서 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "WgHDLnjjy0LX",
    "outputId": "c15790ab-5d72-47d1-d6e2-6096138e6331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 1, 6, 3, 7]\n"
     ]
    }
   ],
   "source": [
    "sub_text = \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
    "\n",
    "encoded = t.texts_to_sequences([sub_text])[0]\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MO896_Eky95b"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oVMwizLk2Alk"
   },
   "source": [
    "### 7.2.2 원-핫 인코딩\n",
    "\n",
    "- 케라스는 정수 인코딩된 결과로부터 원-핫 인코딩을 수행하는 `to_categorical()`을 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "J3M4JzuL8N9-",
    "outputId": "17ce4a2e-aea4-4fa5-ef8b-fdfe17f1dc0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "one_hot = to_categorical(encoded)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NwTJVRte8Rxh"
   },
   "source": [
    "- 위의 결과는 \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"라는 문장이 `[2, 5, 1, 6, 3, 7]`로 정수 인코딩이 되고나서, 각각의 인코딩된 결과를 인덱스로 원-핫 인코딩이 수행된 모습을 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EkRsTtWT8g2g"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4kOQAyJq9Bq1"
   },
   "source": [
    "## 7.3 원-핫 인코딩(One-hot encoding)의 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uk21slPF8hlP"
   },
   "source": [
    "### 7.3.1 저장 공간 측면의 비효율성\n",
    "\n",
    "\n",
    "- 이러한 표현 방식은 단어의 개수가 늘어날 수록, 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다는 단점이 있다.  \n",
    "(벡터의 차원이 계속 늘어난다고 표현)\n",
    "- 원-핫 벡터는 단어 집합의 크기가 곧 벡터의 차원 수가 된다.\n",
    "- ex) 단어가 1,000개인 코퍼스를 가지고 원-핫 벡터를 만듬\n",
    "  - 모든 단어 각각은 모두 1,000개의 차원을 가진 벡터가 된다.\n",
    "  - 모든 단어 각각은 하나의 값만 1을 가지고, 999개의 값은 0의 값을 가지는 벡터가 된다.\n",
    "- 이는 저장 공간 측면에서는 매우 비효율적인 표현 방법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rt0CCV-l9GGK"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4l9pUHVn9PMe"
   },
   "source": [
    "### 7.3.2 단어의 유사도 표현 불가\n",
    "\n",
    "- 또한 원-핫 벡터는 단어의 유사도를 표현하지 못한다.\n",
    "- ex) 늑대, 호랑이, 강아지, 고양이라는 4개의 단어에 대해 원-핫 인코딩을 수행\n",
    "  - 각각 다음과 같은 원-핫 벡터를 부여받음\n",
    "    - 늑대 : `[1, 0, 0, 0]`\n",
    "    - 호랑이 : `[0, 1, 0, 0]`\n",
    "    - 강아지 : `[0, 0, 1, 0]`\n",
    "    - 고양이 : `[0, 0, 0, 1]`\n",
    "  - 이 때 원-핫 벡터로는 강아지가 늑대와 유사하고, 호랑이와 고양이가 유사하다는 것을 표현할 수 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e-cgVq_M9wBF"
   },
   "source": [
    "- 단어 간 유사성을 알 수 없다는 단점은 **검색 시스템** 등에서 심각한 문제이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Toy-5VOw93RV"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JDNaKpWN97p2"
   },
   "source": [
    "### 7.3.3 원-핫 인코딩의 단점을 극복하기 위한 기법\n",
    "\n",
    "- 이러한 단점을 해결하기 위해 단어의 잠재 의미를 반영하여 다차원 공간에 벡터화 하는 기법이 있다.\n",
    "\n",
    "1. **카운트 기반의 벡터화 방법**\n",
    "  - LSA\n",
    "  - HAL\n",
    "2. **예측 기반의 벡터화 방법**\n",
    "  - NNLM\n",
    "  - RNNLM\n",
    "  - Word2Vec\n",
    "  - FastText\n",
    "3. **카운트 기반과 예측 기반 두 가지 방법을 모두 사용하는 벡터화 방법**\n",
    "  - GloVe"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch02_v07_One-hot-encoding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
