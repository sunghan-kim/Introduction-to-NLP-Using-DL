{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2NIsrymPaP0T"
   },
   "source": [
    "# Ch03. 언어 모델 (Language Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BBkBPxhpaSi2"
   },
   "source": [
    "# v02. 통계적 언어 모델(Statistical Language Model, SLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gY0bYzHYaeN2"
   },
   "source": [
    "- 언어 모델의 전통적인 접근 방법인 **통계적 언어 모델**을 소개\n",
    "- 통계적 언어 모델이 통계적인 접근 방법으로 어떻게 언어를 모델링하는 지 확인\n",
    "- 통계적 언어 모델(Statistical Language Model)은 줄여서 SLM이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YclI32End-En"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 2.1 조건부 확률\n",
    "\n",
    "- 조건부 확률은 두 확률 $P(A)$, $P(B)$에 대해서 아래와 같은 관계를 갖는다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "P(B | A) = P(A, B) / P(A)\n",
    "$\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "P(A, B) = P(A) \\; P(B | A)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DBfA-wL1eUTe"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.1.1 조건부 확률의 연쇄 법칙(chain rule)\n",
    "\n",
    "- 더 많은 확률에 대해 일반화해보자.\n",
    "- 4개의 확률이 조건부 확률의 관계를 가질 때, 아래와 같이 표현할 수 있다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "P(A, B, C, D) = P(A) \\; P(B|A) \\; P(C|A,B) \\; P(D|A,B,C)\n",
    "$\n",
    "\n",
    "- 이를 조건부 확률의 **연쇄 법칙(chain rule)**이라고 한다.  \n",
    "  \n",
    "\n",
    "- 이제 4개가 아닌 $n$개에 대해 일반화를 해보자\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "P(x_1, x_2, x_3, \\cdots, x_n) = P(x_1) \\; P(x_2|x_1) \\; P(x_3|x_1,x_2) \\cdots P(x_n|x_1,\\cdots,x_{n-1})\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E_sy1vVJfHM_"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 2.2 문장에 대한 확률\n",
    "\n",
    "- 조건부 확률에 대한 정의를 통해 문장의 확률을 구해보자.  \n",
    "  \n",
    "\n",
    "> 'An adorable little boy is spreading smiles'\n",
    "\n",
    "- 위 문장의 확률 $P(An \\, adorable \\, little \\, boy \\, is \\, spreading \\, smiles)$를 식으로 표현해보자.  \n",
    "  \n",
    "\n",
    "- 각 단어는 **문맥**이라는 관계로 인해 이전 단어의 영향을 받아 나온 단어이다.\n",
    "- 그리고 모든 단어로부터 하나의 문장이 완성된다.\n",
    "- 그렇기 때문에 **문장의 확률을 구하고자 조건부 확률을 사용**한다.  \n",
    "  \n",
    "\n",
    "- 문장의 확률은 **각 단어들이 이전 단어가 주어졌을 때 다음 단어로 등장할 확률의 곱**으로 구성된다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "P(w_1, w_2, w_3, w_4, w_5, \\cdots, w_n) = \\prod_{n=1}^n P(w_n | w_1, \\cdots, w_{n-1})\n",
    "$\n",
    "\n",
    "- 위 문장에 해당 식을 적용해보면 다음과 같다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "\\begin{align*}\n",
    "P(An \\, adorable \\, little \\, boy \\, is \\, spreading \\, smiles) &= P(An) \\\\\n",
    "&\\times P(adorable \\; | \\; An) \\\\\n",
    "&\\times P(little \\; | \\; An \\, adorable) \\\\\n",
    "&\\times P(boy \\; | \\; An \\, adorable \\, little) \\\\\n",
    "&\\times P(is \\; | \\; An \\, adorable \\, little \\, boy) \\\\\n",
    "&\\times P(spreading \\; | \\; An \\, adorable \\, little \\, boy \\, is) \\\\\n",
    "&\\times P(smiles \\; | \\; An \\, adorable \\, little \\, boy is \\, spreading) \\\\\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "- 문장의 확률을 구하기 위해서 각 단어에 대한 예측 확률들을 곱한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XkicxBsEiJoq"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 2.3 카운트 기반의 접근\n",
    "\n",
    "- 문장의 확률을 구하기 위해서 다음 단어에 대한 예측 확률을 모두 곱한다는 것은 확인했다.\n",
    "- 그렇다면 SLM은 이전 단어로부터 다음 단어에 대한 확률은 어떻게 구할까?\n",
    "- 정답은 **카운트에 기반하여 확률을 계산**한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C1f8UC4aimT3"
   },
   "source": [
    "- \"An adorable little boy\"가 나왔을 때, \"is\"가 나올 확률 :\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "P(is \\; | \\; An \\, adorable \\, little \\, boy)\n",
    "$\n",
    "\n",
    "- 위의 확률을 구하는 방법은 아래와 같다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "P(is \\; | \\; An \\, adorable \\, little \\, boy) = {{count(An \\, adorable \\, little \\, boy \\, is)} \\over {count(An \\, adorable \\, little \\, boy)}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sNLjLjBajFqW"
   },
   "source": [
    "- ex) 기계가 학습한 코퍼스 데이터에서\n",
    "  - \"An adorable little boy\"가 100번 등장\n",
    "  - 그 다음에 \"is\"가 등장한 경우 = 30번\n",
    "  - 이 경우 $P(is \\; | \\; An \\, adorable \\, little \\, boy)$는 30%가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oht7aZzxjmF0"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KMxU0n-AkKBE"
   },
   "source": [
    "## 2.4 카운트 기반 접근의 한계 - 희소 문제(Sparsity Problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klvnmk2IjnSM"
   },
   "source": [
    "### 2.4.1 현실에서의 확률 분포\n",
    "\n",
    "- 언어 모델은 실생활에서 사용되는 언어의 확률 분포를 근사 모델링한다.\n",
    "- 현실에서도 \"An adorable little boy\"가 나왔을 때 \"is\"가 나올 확률이라는 것이 존재한다.\n",
    "- 이를 실제 자연어의 확률 분포, 현실에서의 확률 분포라고 명칭한다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OZTmlr19kPx6"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.4.2 언어 모델의 목표\n",
    "\n",
    "- 기계에게 많은 코퍼스를 훈련시켜서 언어 모델을 통해 현실에서의 확률 분포를 근사하는 것이 언어 모델의 목표이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Flo55P2OkWpG"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.4.3 희소 문제 (Sparsity Problem)\n",
    "\n",
    "- 그런데 카운트 기반으로 접근하려고 한다면 갖고 있는 코퍼스(corpus) 즉, 기계가 훈련하는 데이터는 정말 방대한 양이 필요하다.  \n",
    "  \n",
    "\n",
    "- 예를 들어 $P(is \\; | \\; An \\, adorable \\, little \\, boy)$를 구하는 경우에서 기계가 훈련한 코퍼스에 \"An adorable little boy is\"라는 단어 시퀀스가 없었다면 이 단어 시퀀스에 대한 확률은 0이 된다.\n",
    "- 또는 \"An adorable little boy\"라는 단어 시퀀스가 없었다면 분모가 0이 되어 확률은 정의되지 않는다.  \n",
    "  \n",
    "\n",
    "- 그렇다면 코퍼스에 단어 시퀀스가 없다고 해서 이 확률을 0 또는 정의되지 않는 확률이라고 하는 것이 정확한 모델링 방법일까?\n",
    "  - 그렇지 않다.\n",
    "  - 현실에선 \"An adorable little boy is\"라는 단어 시퀀스가 존재하고 또 문법에는 적합하므로 정답일 가능성 또한 높다.  \n",
    "\n",
    "  \n",
    "- 이와 같이 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제를 **희소 문제(sparsity problem)**라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iYmJnymFlaZD"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.4.4 희소 문제를 완화하는 방법\n",
    "\n",
    "- 여러 가지 일반화(generalization) 기법이 존재한다.\n",
    "  - n-gram\n",
    "  - 스무딩\n",
    "  - 백오프  \n",
    "\n",
    "\n",
    "- 하지만 희소 문제에 대한 근본적인 해결책은 되지 못했다.\n",
    "- 결국 이러한 한계로 인해 언어 모델의 트랜드는 통계적 언어 모델에서 인공 신경망 모델로 넘어가게 된다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch03_v02_Statistical-Language-Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
