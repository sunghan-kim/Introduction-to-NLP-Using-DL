{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHrmATN7nKJJ"
   },
   "source": [
    "# Ch03. 언어 모델 (Language Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XXEV_tIrnNWm"
   },
   "source": [
    "# v03. N-gram 언어 모델 (N-gram Language Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-i9aNGnnXgl"
   },
   "source": [
    "- n-gram 언어 모델은 여전히 카운트에 기반한 통계적 접근을 사용하고 있음  \n",
    "$\\rightarrow$ SLM의 일종이다.  \n",
    "  \n",
    "\n",
    "- 다만, 앞서 배운 언어 모델과는 달리 이전에 등장한 모든 단어를 고려하는 것이 아니라 **일부 단어만 고려**하는 접근 방법을 사용한다.\n",
    "- 그리고 이때 일부 단어를 몇 개 보느냐를 결정하는데 이것이 n-gram에서의 n이 가지는 의미이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0cuW7FV5n00T"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 3.1 코퍼스에서 카운트하지 못하는 경우의 감소\n",
    "\n",
    "- SLM의 한계\n",
    "  - 훈련 코퍼스에 확률을 계산하고 싶은 문장이나 단어가 없을 수 있다는 점\n",
    "  - 그리고 확률을 계산하고 싶은 문장이 길어질수록 갖고 있는 코퍼스에서 그 문장이 존재하지 않을 가능성이 높다.  \n",
    "  (카운트할 수 없을 가능성이 높다.)  \n",
    "\n",
    "\n",
    "- 그런데 다음과 같이 참고하는 단어들을 줄이면 카운트를 할 수 있을 가능성을 높일 수 있다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "P(is \\; | \\; An \\, adorable \\, little \\, boy) \\approx P(is \\; | \\; boy)\n",
    "$\n",
    "\n",
    "- 갖고 있는 코퍼스에 \"An adorable little boy is\"가 있을 가능성 보다는 \"boy is\"라는 더 짧은 단어 시퀀스가 존재할 가능성이 더 높다.  \n",
    "  \n",
    "\n",
    "- 아래와 같이 \"little boy\"가 나왔을 때 \"is\"가 나올 확률로 생각하는 것도 대안이다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "P(is \\; | \\; An \\, adorable \\, little \\, boy) \\approx P(is \\; | \\; little \\, boy)\n",
    "$\n",
    "\n",
    "- \"An adorable little boy\"가 나왔을 때 \"is\"가 나올 확률 계산\n",
    "  - \"An adorable little boy\"가 나온 횟수와 \"An adorable little boy is\"가 나온 횟수를 카운트해야 한다.  \n",
    "\n",
    "\n",
    "- 이제는 단어의 확률을 구하고자 기준 단어의 앞 단어를 전부 포함해서 카운트하는 것이 아닌, 앞 단어 중 임의의 개수만 포함해서 카운트하여 근사하는 것이다.\n",
    "- 이렇게 하면 갖고 있는 코퍼스에서 해당 단어의 시퀀스를 카운트할 확률이 높아진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rVA2USinphpD"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 3.2 N-gram\n",
    "\n",
    "- 위에서 임의의 개수를 정하기 위한 기준을 위해 사용됨\n",
    "- n개의 연속적인 단어 나열을 의미\n",
    "- 갖고 있는 코퍼스에서 n개의 단어 뭉치 단위로 끊어서 이를 하나의 토큰으로 간주한다.  \n",
    "  \n",
    "\n",
    "- ex) 각 n에 대해서 n-gram을 전부 구해보자.  \n",
    "  \n",
    "\n",
    "- **uni**grams(유니그램)\n",
    "  - an\n",
    "  - adorable\n",
    "  - little\n",
    "  - boy\n",
    "  - is\n",
    "  - spreading\n",
    "  - smiles  \n",
    "\n",
    "\n",
    "- **bi**grams(바이그램)\n",
    "  - an adorable\n",
    "  - adorable little\n",
    "  - little boy\n",
    "  - boy is\n",
    "  - is spreading\n",
    "  - spreading smiles  \n",
    "\n",
    "\n",
    "- **tri**grams(트라이그램)\n",
    "  - an adorable little\n",
    "  - adorable little boy\n",
    "  - little boy is\n",
    "  - boy is spreading\n",
    "  - is spreading smiles\n",
    "\n",
    "\n",
    "- **4**-grams\n",
    "  - an adorable little boy\n",
    "  - adorable little boy is\n",
    "  - little boy is spreading\n",
    "  - boy is spreading smiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "owYAk3iTpltJ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.2.1 n-gram을 이용한 언어 모델 설계\n",
    "\n",
    "- n-gram을 통한 언어 모델에서는 다음에 나올 단어의 예측은 오직 n-1개의 단어에만 의존한다.  \n",
    "  \n",
    "\n",
    "- ex) **\"An adorable little boy is spreading\"** 다음에 나올 단어 예측\n",
    "  - n=4 라고 한 4-gram을 이용한 언어 모델을 사용\n",
    "  - 이 경우, \"spreading\" 다음에 올 단어를 예측하는 것은 n-1에 해당되는 앞의 3개의 단어만을 고려한다.\n",
    "<img src=\"https://wikidocs.net/images/page/21692/n-gram.PNG\" />\n",
    "\n",
    "$\n",
    "\\qquad\\qquad\n",
    "P(w \\; | \\; boy \\, is \\, spreading) = { {count(boy \\, is \\, spreading \\, w)} \\over {count(boy \\, is \\, spreading)} }\n",
    "$\n",
    "\n",
    "- 갖고 있는 코퍼스에서 \n",
    "  - \"boy is spreading\"가 1,000번 등장\n",
    "  - \"boy is spreading insults\"가 500번 등장\n",
    "  - \"boy is spreading smiles\"가 200번 등장  \n",
    "\n",
    "\n",
    "- 그렇게 되면 \n",
    "  - \"boy is spreading\" 다음에 \"insults\"가 등장할 확률 = 50%\n",
    "  - \"boy is spreading\" 다음에 \"smiles\"가 등장할 확률 = 20%  \n",
    "\n",
    "\n",
    "- 확률적 선택에 따라 우리는 \"insults\"가 더 맞다고 판단하게 된다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "P(insults \\; | \\; boy \\, is \\, spreading) = 0.500\n",
    "$\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "P(smiles \\; | \\; boy \\, is \\, spreading) = 0.200\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FA2RZHpktxoj"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 3.3 N-gram Language Model의 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ozv2UFMOt2HN"
   },
   "source": [
    "- n-gram은 뒤의 단어 몇 개만 보다 보니 의도하고 싶은 대로 문장을 끝냊음하지 못하는 경우가 생긴다.\n",
    "- 문장을 읽다 보면 앞 부분과 뒷 부분의 문맥이 전혀 연결 안 되는 경우도 생길 수가 있다.\n",
    "- 결론적으로 전체 문장을 고려한 언어 모델보다는 정확도가 떨어질 수밖에 없다.\n",
    "- 이를 n-gram 모델에 대한 한계점을 정리한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "30n9vCdaY2is"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.1 희소 문제 (Sparsity Problem)\n",
    "\n",
    "- 문장에 존재하는 앞에 나온 단어를 모두 보는 것보다 일부 단어만을 보는 것으로 현실적으로 코퍼스에서 카운트할 수 있는 확률을 높일 수는 있었지만, n-gram 언어 모델도 여전히 n-gram에 대한 희소 문제가 존재한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "youzyYZ5aXnn"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.2 n을 선택하는 것은 trade-off 문제\n",
    "\n",
    "- 앞에서 몇 개의 단어를 볼지 n을 정하는 것은 trade-off가 존재한다.\n",
    "- 임의의 개수인 n을 1보다는 2로 선택하는 것은 거의 대부분의 경우에서 언어 모델의 성능을 높일 수 있다.  \n",
    "  \n",
    "\n",
    "- n을 크게 선택하면 실제 훈련 코퍼스에서 해당 n-gram을 카운트할 수 있는 확률은 적어지므로 희소 문제는 점점 심각해진다.\n",
    "- 또한 n이 커질수록 모델 사이즈가 커진다는 문제점도 있다.\n",
    "  - 기본적으로 코퍼스의 모든 n-gram에 대해서 카운트를 해야 하기 때문\n",
    "\n",
    "\n",
    "- n을 작게 선택하면 훈련 코퍼스에서 카운트는 잘 되겠지만 근사의 정확도는 현실의 확률분포와 멀어진다.\n",
    "- 그렇기 때문에 적절한 n을 선택해야 한다.\n",
    "- trade-off 문제로 인해 정확도를 높이려면 **n은 최대 5를 넘게 잡아서는 안 된다고 권장**되고 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ST3tmOwG2Xhk"
   },
   "source": [
    "<br>\n",
    "\n",
    "**n이 성능에 영향을 주는 것을 확인할 수 있는 예제**\n",
    "\n",
    "- 스탠퍼드 대학교의 공유 자료에 따르면, 월스트리트 저널에서 3,800만 개의 단어 토큰에 대하여 n-gram 언어 모델을 학습하고, 1,500만 개의 테스트 데이터에 대해서 테스트를 했을 때 다음과 같은 성능이 나왔다.  \n",
    "(펄플렉서티(perplexity) : 수치가 낮을수록 좋은 성능을 나타냄)\n",
    "\n",
    "| -          | Unigram | Bigram | Trigram |\n",
    "| ---------- | :-----: | :----: | :-----: |\n",
    "| Perplexity |   962   |  170   |   109   |\n",
    "\n",
    "- 위 결과는 n을 올릴 때마다 성능이 올라가는 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gRTshdkDjquW"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 3.4 적용 분야(Domain)에 맞는 코퍼스의 수집\n",
    "\n",
    "- 어떤 분야인지, 어떤 어플리케이션인지에 따라서 특정 단어들의 확률 분포는 당연히 다르다\n",
    "- 이 경우 언어 모델에 사용하는 코퍼스를 해당 도메인의 코퍼스를 사용한다면 당연히 언어 모델이 제대로 된 언어 생성을 할 가능성이 높아진다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eGX5s6D9kFdI"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 3.5 인공 신경망을 이용한 언어 모델(Neural Network Based Language Model)\n",
    "\n",
    "- N-gram Language Model의 한계점을 극복하기 위해 분모, 분자에 숫자를 더해서 카운트했을 때 0이 되는 것을 방지하는 등의 여러 일반화(generalization) 방법들이 존재한다.\n",
    "- 하지만 그럼에도 본질적으로 n-gram 언어 모델에 대한 취약점을 완전히 해결하지는 못했다.\n",
    "- 그래서 이를 위한 대안으로 N-gram Language Model보다 대체적으로 성능이 우수한 **인공 신경망을 이용한 언어 모델**이 많이 사용된다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch03_v03_N-gram-Language-Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
