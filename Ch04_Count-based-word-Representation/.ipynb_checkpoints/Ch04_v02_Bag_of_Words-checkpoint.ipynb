{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CB8hgfxnrxop"
   },
   "source": [
    "# Ch04. 카운트 기반의 단어 표현 (Count based word Representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o7W1lt40r3Wk"
   },
   "source": [
    "# v02. Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NIIz75akr5lD"
   },
   "source": [
    "- Bag of Words : 단어의 등장 순서를 고려하지 않는 빈도수 기반의 단어 표현 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sTA9BYCFwaKH"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 2.1 Bag of Words란?\n",
    "\n",
    "- 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법\n",
    "- 갖고 있는 어떤 텍스트 문서에 있는 단어들을 가방에다가 전부 넣는다.\n",
    "- 그러고 나서 이 가방을 흔들어 단어들을 섞는다.\n",
    "- 만약, 해당 문서 내에서 특정 단어가 N번 등장했다면, 이 가방에는 그 특정 단어가 N개 있게 된다.\n",
    "- 또한 가방을 흔들어서 단어를 섞었기 때문에 더 이상 단어의 순서는 중요하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ghhCGf8jw1Jo"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.1.1 BoW를 만드는 2가지 과정\n",
    "\n",
    "1. 우선, 각 단어에 고유한 정수 인덱스를 부여한다.\n",
    "2. 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만듬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pjv9NREtw_B9"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.1.2 한국어 예제를 통한 BoW 이해\n",
    "\n",
    "> 문서1 : 정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\n",
    "\n",
    "- 위의 문서1에 대해서 BoW를 만들어 보자.  \n",
    "  \n",
    "\n",
    "- 아래의 코드는 입력된 문서에 대해서 단어 집합(vocabulary)을 만들어 인덱스를 할당하고, BoW를 만드는 코드이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Vpg0gJRxW9a"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EKcE_d_ixt4Z"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "VzT_zUPKxx5Z",
    "outputId": "fe9fc7ac-93ab-4970-a5f1-014fed0bdcc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다\n"
     ]
    }
   ],
   "source": [
    "# 정규 표현식을 통해 온점을 제거하는 정제 작업\n",
    "token = re.sub(\"(\\.)\", \"\", \"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\")\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "rYLx_4q5yTes",
    "outputId": "028bff14-18b5-4078-ca20-82d86dffdca8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['정부', '가', '발표', '하는', '물가상승률', '과', '소비자', '가', '느끼는', '물가상승률', '은', '다르다']\n"
     ]
    }
   ],
   "source": [
    "# OKT 형태소 분석기를 통해 토큰화 작업을 수행한 뒤에, token에다가 넣는다.\n",
    "token = okt.morphs(token)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Uc86rrc3yXj4",
    "outputId": "d6099b4c-07dc-417b-d202-f05563f685b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n"
     ]
    }
   ],
   "source": [
    "word2index = {}\n",
    "bow = []\n",
    "\n",
    "for voca in token:\n",
    "  # token을 읽으면서, word2index에 없는 단어는 새로 추가하고, 이미 있는 단어는 넘긴다.\n",
    "  if voca not in word2index.keys():\n",
    "    word2index[voca] = len(word2index)\n",
    "\n",
    "    # Bow 전체에 전부 기본값 1을 넣어준다. (단어의 개수는 최소 1개 이상이기 때문)\n",
    "    bow.insert(len(word2index)-1,1)\n",
    "  \n",
    "  else:\n",
    "    # 재등장하는 단어의 인덱스를 받아온다.\n",
    "    index = word2index.get(voca)\n",
    "    \n",
    "    # 재등장한 단어는 해당 인덱스의 위치에 1을 더해준다. (단어의 개수를 세는 것)\n",
    "    bow[index] = bow[index]+1\n",
    "\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-LB2TusqzBQB",
    "outputId": "b224c7c6-cf98-4168-9be1-c90d1b15c774"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 1, 2, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k2sgaJyuzC82"
   },
   "source": [
    "- \"물가상승률\"의 인덱스 = 4\n",
    "- 문서1에서 \"물가상승률\"은 2번 언급되었기 때문에 인덱스 4(5번째 값)에 해당하는 값이 2이다.  \n",
    "  \n",
    "\n",
    "- 원한다면 한국어에서 불용어에 해당되는 조사들 또한 제거하여 더 정제된 BoW를 만들수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R1HdP9bfzdC-"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 2.2 Bag of Words의 다른 예제들\n",
    "\n",
    "- BoW에 있어서 중요한 것 $\\Rightarrow$ 단어의 등장 빈도\n",
    "- 단어의 순서, 즉 인덱스의 순서는 전혀 상관 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iu_JcB9HzoKF"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.2.1 인덱스 임의 변경\n",
    "\n",
    "- 문서1에 대한 인덱스 할당을 임의로 바꾸고 그에 따른 BoW를 만든다고 해보자.\n",
    "\n",
    "```python\n",
    "# ('발표': 0, '가': 1, '정부': 2, '하는': 3, '소비자': 4, '과': 5, '물가상승률': 6, '느끼는': 7, '은': 8, '다르다': 9)  \n",
    "[1, 2, 1, 1, 1, 1, 2, 1, 1, 1]  \n",
    "```\n",
    "\n",
    "- 위의 BoW는 단지 단어들의 인덱스만 바뀌었을 뿐이며, 개념적으로는 여전히 앞서 만든 BoW와 동일한 BoW로 취급할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IcG7w9SczxLE"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.2 2개의 문서의 단어 집합을 합친 다음 각 문서의 BoW 만들기\n",
    "\n",
    "> 문서2 : 소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.\n",
    "\n",
    "- 만약, 위의 코드에 문서2로 입력으로 하여 인덱스 할당과 BoW를 만드는 것을 진행한다면 아래와 같은 결과가 나온다.\n",
    "\n",
    "```python\n",
    "('소비자': 0, '는': 1, '주로': 2, '소비': 3, '하는': 4, '상품': 5, '을': 6, '기준': 7, '으로': 8, '물가상승률': 9, '느낀다': 10)  \n",
    "[1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2suNSjRQ11ZC"
   },
   "source": [
    "- 문서1과 문서2를 합쳐서 BoW를 만들 수도 있다.\n",
    "  - 합친 문서를 문서3이라고 한다.\n",
    "\n",
    "> 문서3: 정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다. 소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.\n",
    "\n",
    "- 위의 코드에 문서3을 입력으로 하여 인덱스 할당과 BoW를 만든다면 아래와 같은 결과가 나온다.\n",
    "\n",
    "```python\n",
    "('정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9, '는': 10, '주로': 11, '소비': 12, '상품': 13, '을': 14, '기준': 15, '으로': 16, '느낀다': 17)  \n",
    "[1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1]  \n",
    "```\n",
    "\n",
    "- 문서3의 단어 집합은 문서1과 문서2의 단어들을 모두 포함하고 있는 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j1EZ8YMT2FeA"
   },
   "source": [
    "- BoW는 종종 여러 문서의 단어 집합을 합친 뒤에, 해당 단어 집합에 대한 각 문서의 BoW를 구하기도 한다.\n",
    "  - 가령, 문서3에 대한 단어 집합을 기준으로 문서1, 문서2의 BoW를 만든다고 한다면 결과는 아래와 같다.\n",
    "\n",
    "```python\n",
    "문서3 단어 집합에 대한 문서1 BoW : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
    "문서3 단어 집합에 대한 문서2 BoW : [0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1]  \n",
    "```\n",
    "\n",
    "- 문서3 단어 집합에서 \"물가상승률\"이라는 단어는 인덱스가 4에 해당된다.\n",
    "- \"물가상승률\"이라는 단어는 문서1에서는 2회 등장하며, 문서2에서는 1회 등장하였기 때문에 두 BoW의 인덱스 4의 값은 각각 2와 1이 되는 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ev30VLX52xTg"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.2.3 BoW의 활용\n",
    "\n",
    "- BoW는 각 단어가 등장한 횟수를 수치화하는 텍스트 표현 방법이다.\n",
    "- 그렇기 때문에 주로 어떤 단어가 얼마나 등장했는 지를 기준으로 문서가 어떤 성격의 문서인지를 판단하는 작업에 쓰인다.\n",
    "- 즉, **분류 문제**나 **여러 문서 간의 유사도를 구하는 문제**에 주로 쓰인다.\n",
    "  - ex) '달리기', '체력', '근력'과 같은 단어가 자주 등장  \n",
    "  $\\rightarrow$ 해당 문서를 \"체육\" 관련 문서로 분류\n",
    "  - ex) '미분', '방정식', '부등식'과 같은 단어가 자주 등장  \n",
    "  $\\rightarrow$ \"수학\" 관련 문서로 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gdEZvQCP3gTP"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 2.3 `CountVectorizer` 클래스로 BoW 만들기\n",
    "\n",
    "- 사이킷런에서는 단어의 빈도를 Count하여 Vector로 만드는 `CountVectorizer` 클래스를 지원한다.\n",
    "- 이를 이용하면 영어에 대해서는 손쉽게 BoW를 만들 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Hhtz7Yx5juQ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.3.1 실습\n",
    "\n",
    "- `CountVectorizer`로 간단하고 빠르게 BoW를 만드는 실습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "7EI_Gc184yb2",
    "outputId": "176db3d0-499e-4134-fcac-eacc131b3c34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 1 2 1]]\n",
      "{'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['you know I want your love. because I love you.']\n",
    "vector = CountVectorizer()\n",
    "\n",
    "# 코퍼스로부터 각 단어의 빈도수를 기록\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "\n",
    "# 각 단어의 인덱스가 어떻게 부여되었는 지 확인\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vtlqR-yk5HB5"
   },
   "source": [
    "- \"you\"와 \"love\"는 두 번씩 언급되었으므로 각각 인덱스 2와 인덱스 4에서 2의 값을 가짐\n",
    "- 그 외의 값에서는 1의 값을 가지는 것을 볼 수 있다.\n",
    "- 또한 알파벳 \"I\"는 BoW를 만드는 과정에서 사라짐\n",
    "  - 이는 `CountVectorizer`가 기본적으로 길이가 2 이상인 문자에 대해서만 토큰으로 인식하기 때문\n",
    "  - 영어에서는 길이가 짧은 문자를 제거하는 것 또한 전처리 작업으로 고려되기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i81oT_yk5gja"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.3.2 주의할 점\n",
    "\n",
    "- `CountVectorizer`는 단지 띄어쓰기만을 기준으로 단어를 자르는 낮은 수준의 토큰화를 진행하고 BoW를 만든다.\n",
    "- 이는 영어의 경우 띄어쓰기만으로 토큰화가 수행되기 때문에 문제가 없음\n",
    "- 하지만 한국어에 `CountVectorizer`를 적용하면, 조사 등의 이유로 제대로 BoW가 만들어지지 않음을 의미한다.\n",
    "- ex)  '정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.' 라는 문장을 BoW로 만듬\n",
    "  - `CountVectorizer`는 띄어쓰기를 기준으로 분리\n",
    "  - 그런 다음 '물가상승률과'와 '물가상승률은' 으로 조사를 포함해서 하나의 단어로 판단한다.\n",
    "  - 따라서 서로 다른 두 단어로 인식한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7XjMZs9M6Hra"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 2.4 불용어를 제거한 BoW 만들기\n",
    "\n",
    "- 불용어는 자연어 처리에서 별로 의미를 갖지 않는 단어들을 의미한다.\n",
    "- BoW를 사용한다는 것은 그 문서에서 각 단어가 얼마나 자주 등장했는 지를 보겠다는 것이다.\n",
    "- 그리고 각 단어에 대한 빈도수를 수치화 하겠다는 것은 결국 텍스트 내에서 어떤 단어들이 중요한지를 보고싶다는 의미를 함축하고 있다.\n",
    "- 그렇다면 BoW를 만들 때 불용어를 제거하는 일은 자연어 처리의 정확도를 높이기 위해선 선택할 수 있는 전처리 기법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tEqpscYm6gPZ"
   },
   "source": [
    "- 영어의 BoW를 만들기 위해 사용하는 `CountVectorizer`는 `stop_words` 파라미터를 통해 불용어를 지정하면, 불용어는 제외하고 BoW를 만들 수 있도록 불용어 제거 기능을 지원하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pQk7ROIi6uzX"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.4.1 사용자가 직접 정의한 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "XUE8tYa16y0_",
    "outputId": "4e6abfe9-c970-4549-e41a-32d7371f3fbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text=[\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=[\"the\", \"a\", \"an\", \"is\", \"not\"])\n",
    "\n",
    "print(vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MyFmvDMw7Fk3"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.4.2 `CountVectorizer`에서 제공하는 자체 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "AvYUUVEi7Sdz",
    "outputId": "7d9482db-c0fb-4191-dabb-fe3019c15c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]]\n",
      "{'family': 0, 'important': 1, 'thing': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text=[\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "\n",
    "print(vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ubly46me7gsl"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.4.3 `NLTK`에서 지원하는 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "__lwlk6v73Dw",
    "outputId": "1a46a326-3e17-48d1-8108-14d643f82a5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "RhsLSBrE7k26",
    "outputId": "39f00419-b859-496b-f123-3c7361f15b59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text=[\"Family is not an important thing. It's everything.\"]\n",
    "sw = stopwords.words('english')\n",
    "vect = CountVectorizer(stop_words=sw)\n",
    "\n",
    "print(vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch04_v02_Bag-of-Words.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
