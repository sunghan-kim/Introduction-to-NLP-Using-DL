{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N78UnOpsjoq6"
   },
   "source": [
    "# Ch07. 머신 러닝(Machine Learning) 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xMzbIZk2juWE"
   },
   "source": [
    "# v03. 선형 회귀 (Linear Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJtKq13Wjw4r"
   },
   "source": [
    "- 이번 챕터에서는 머신 러닝에서 쓰이는 다음과 같은 용어들에 대한 개념과 선형 회귀에 대해서 이해한다.\n",
    "  - 가설(Hypothesis)\n",
    "  - 손실 함수(Loss Function)\n",
    "  - 경사 하강법(Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QZ_27P65j94I"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 3.1 선형 회귀 (Linear Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sOrZVWSOkBxQ"
   },
   "source": [
    "- 어떤 요인의 수치에 따라서 특정 요인의 수치가 영향을 받고있다.\n",
    "- 어떤 변수의 값에 따라서 특정 변수의 값이 영향을 받고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UEhRSLbslLKp"
   },
   "source": [
    "- 다른 변수의 값을 변하게 하는 변수 : $x$\n",
    "  - 변수 $x$의 값은 독립적으로 변할 수 있다.\n",
    "  - $x$를 독립 변수라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pzTW-fFqlNeh"
   },
   "source": [
    "- 변수 $x$에 의해서 값이 종속적으로 변하는 변수 : $y$\n",
    "  - 변수 $y$의 값은 계속해서 $x$의 값에 의해서 종속적으로 결정된다.\n",
    "  - $y$를 종속 변수라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ClAyws5YliQt"
   },
   "source": [
    "- 선형 회귀는 한 개 이상의 독립 변수 $x$와 $y$의 선형 관계를 모델링한다.\n",
    "- 만약, 독립 변수 x가 1개라면 단순 선형 회귀라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qOF9cE1rlq_c"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.1.1 단순 선형 회귀 분석 (Simple Linear Regression Analysis)\n",
    "\n",
    "#### 3.1.1.1 단순 선형 회귀 수식\n",
    "\n",
    "$\n",
    "\\quad\n",
    "y = W x + b\n",
    "$\n",
    "\n",
    "<br>\n",
    "\n",
    "- $W$\n",
    "  - 독립 변수 x와 곱해지는 값 \n",
    "  - 가중치(weight)\n",
    "  - 직선의 방정식에서 직선의 기울기\n",
    "- $b$\n",
    "  - 별도로 더해지는 값\n",
    "  - 편향(bias)\n",
    "  - 직선의 방정식에서 절편"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "chXi54izmBzn"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.1.1.2 $W$ 와 $b$가 없이 $y$와 $x$ 수식\n",
    "\n",
    "$\n",
    "\\quad\n",
    "y = x\n",
    "$\n",
    "\n",
    "- $y$는 $x$와 같다는 하나의 식밖에 표현하지 못한다.\n",
    "- 그래프 상으로 말하면, 하나의 직선밖에 표현하지 못한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LresdVXymk4W"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.1.1.3 관계 모델링\n",
    "\n",
    "- 다시 말해 $W$와 $b$의 값을 적절히 찾아내면 $x$와 $y$의 관계를 적절히 모델링한 것이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CFgNlvNnmwVP"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.1.2 다중 선형 회귀 분석(Multiple Linear Regression Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vjGUeYLqm05G"
   },
   "source": [
    "#### 3.1.2.1 다중 선형 회귀 수식\n",
    "\n",
    "$\n",
    "\\quad\n",
    "y = {W_1x_1 + W_2x_2 + \\cdots + W_nx_n + b}\n",
    "$\n",
    "\n",
    "<br>\n",
    "\n",
    "- $y$ 는 여전히 1개이지만 이제 $x$는 1개가 아니라 여러 개가 되었다.\n",
    "- 이를 다중 선형 회귀 분석이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qjx76uoCnAct"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 3.2 가설(Hypothesis) 세우기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Kq_G9lDnWta"
   },
   "source": [
    "### 3.2.1 단순 선형 회귀 문제\n",
    "\n",
    "#### 3.2.1.1 데이터\n",
    "\n",
    "- 어떤 학생의 공부 시간에 따라서 다음과 같은 점수를 얻었다는 데이터가 있다.\n",
    "\n",
    "| hours($x$) | score($y$) |\n",
    "| :--------- | :--------- |\n",
    "| 2          | 25         |\n",
    "| 3          | 50         |\n",
    "| 4          | 42         |\n",
    "| 5          | 61         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l_AaS5uuneJc"
   },
   "source": [
    "- 이를 좌표 평면에 그려보면 다음과 같다.  \n",
    "<img src=\"https://wikidocs.net/images/page/53560/%EA%B7%B8%EB%A6%BC1.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iz7wW48SnpU7"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.2.1.2 가설(Hypothesis)\n",
    "\n",
    "- 알고있는 데이터로부터 $x$와 $y$의 관계를 유추하고, 이 학생이 6시간, 7시간, 8시간을 공부하였을 때의 성적을 예측해보고 싶다.\n",
    "- $x$ 와 $y$의 관계를 유추하기 위해서 수학적으로 식을 세울 수 있다.  \n",
    "$\\rightarrow$ 머신 러닝에서는 이러한 식을 **가설(Hypothesis)**이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mjofGmbXoAIj"
   },
   "source": [
    "- 아래의 $H(x)$에서 $H$는 Hypothesis를 의미한다.\n",
    "- 사실 선형 회귀의 가설은 이미 아래와 같이 널리 알려져 있다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "H(x) = W x + b\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yyBsYi6soo6Z"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.2.1.3 선형 회귀에서 해야할 일\n",
    "\n",
    "-  아래 그림과 같이 $W$와 $b$의 값에 따라서 천차만별로 그려지는 직선의 모습을 보여준다.  \n",
    "<img src=\"https://wikidocs.net/images/page/21670/W%EC%99%80_b%EA%B0%80_%EB%8B%A4%EB%A6%84.PNG\" />\n",
    "- 위의 가설에서 $W$는 직선의 기울기이고 $b$는 절편으로 직선을 표현함을 알 수 있다.\n",
    "- 결국 선형 회귀는 주어진 데이터로부터 $y$와 $x$의 관계를 가장 잘 나타내는 직선을 그리는 일을 말한다.\n",
    "- 어떤 직선인지 결정하는 것은 $W$와 $b$의 값이다.\n",
    "- 그러므로 선형 회귀에서 해야할 일은 결국 **적절한 $W$와 $b$를 찾아내는 일**이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S5UZN7r2pBvc"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 3.3 비용 함수 (Cost function) : 평균 제곱 오차(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C1Je7BbKq0Jo"
   },
   "source": [
    "### 3.3.1 실제값과 예측값의 오차를 계산하는 식\n",
    "\n",
    "- 앞서 주어진 데이터에서 $x$와 $y$의 관계를 $W$와 $b$를 이용하여 식을 세우는 일을 **가설**이라고 했다.\n",
    "- 이제 해야할 일은 문제에 대한 규칙을 가장 잘 표현하는 $W$와 $b$를 찾는 일이다.\n",
    "- 머신 러닝은 $W$와 $b$를 찾기 위해서 실제값과 가설로부터 얻은 예측값의 오차를 계산하는 식을 세운다.\n",
    "- 그리고 이 식의 값을 최소화하는 최적의 $W$와 $b$를 찾아냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YrNMMAt4rTEI"
   },
   "source": [
    "- 이 때 실제값과 예측값에 대한 오차에 대한 식을 다음과 같이 부른다.\n",
    "  - **목적 함수(Objective function)**\n",
    "    - 함수의 값을 최소화하거나, 최대화하거나 하는 목적을 가진 함수\n",
    "  - **비용 함수(Cost function)**\n",
    "    - 값을 최소화하려고 하는 함수\n",
    "  - **손실 함수(Loss function)**\n",
    "    - 값을 최소화하려고 하는 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RfqRmxl9roiq"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.2 평균 제곱 오차(Mean Sqaured Error, MSE)\n",
    "\n",
    "- 비용 함수는 단순히 실제값과 예측값에 대한 오차를 표현하면 되는 것이 아니라, 예측값의 오차를 줄이는 일에 최적화 된 식이어야 한다.\n",
    "- 머신 러닝, 딥 러닝에는 다양한 문제들이 있고, 각 문제들에는 적합한 비용 함수들이 있습니다.\n",
    "- **회귀 문제**의 경우에는 주로 **평균 제곱 오차(Mean Squered Error, MSE)**가 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5qA8B4ngr838"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.3 $y$ 와 $x$의 관계를 가장 잘 나타내는 직선\n",
    "\n",
    "- 아래의 그래프에 임의의 $W$의 값 13과 임의의 $b$의 값 1을 가진 직선을 그렸다.  \n",
    "<img src=\"https://wikidocs.net/images/page/53560/%EA%B7%B8%EB%A6%BC3.PNG\" />\n",
    "- 임의로 그린 직선으로 정답이 아니다.\n",
    "- 이 직선으로부터 서서히 $W$와 $b$의 값을 바꾸면서 정답인 직선을 찾아내야 한다.\n",
    "- $y$ 와 $x$의 관계를 가장 잘 나타내는 직선을 그린다는 것  \n",
    "$\\rightarrow$ 위의 그림에서 모든 점들과 위치적으로 가장 가까운 직선을 그린다는 것과 같ek."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oTZyc2ZRsiNL"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.4 오차(error) 정의\n",
    "\n",
    "- 오차는 주어진 데이터에서 각 $x$에서의 실제값 $y$와 위의 직선에서 예측하고 있는 $H(x)$값의 차이를 말한다.\n",
    "- 즉, 위의 그림에서 ↕는 각 점에서의 오차의 크기를 보여준다.\n",
    "- 오차를 줄여가면서 $W$와 $b$의 값을 찾아내기 위해서는 **전체 오차의 크기**를 구해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B83FCiKxs4M9"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.5 오차의 크기를 측정하기 위한 방법 - 단계 (1) : 모든 오차 더하기\n",
    "\n",
    "- 오차의 크기를 측정하기 위한 가장 기본적인 방법  \n",
    "$\\rightarrow$ 각 오차를 모두 더하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e87DHJBftB8a"
   },
   "source": [
    "- 위의 $y=13x+1$ 직선이 예측한 예측값을 각각 실제값으로부터 오차를 계산하여 표를 만들어보면 아래와 같다.\n",
    "\n",
    "| hours($x$) | 2    | 3    | 4    | 5    |\n",
    "| :--------- | :--- | :--- | :--- | :--- |\n",
    "| 실제값     | 25   | 50   | 42   | 61   |\n",
    "| 예측값     | 27   | 40   | 53   | 66   |\n",
    "| 오차       | -2   | 10   | -7   | -5   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ck7SxO81tLf4"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.6 오차의 크기를 측정하기 위한 방법 - 단계 (2) : 모든 오차의 제곱 더하기\n",
    "\n",
    "- 수식적으로 단순히 **'오차 = 실제값 - 예측값'** 이라고 정의한 후에 모든 오차를 더하면 음수 오차도 있고, 양수 오차도 있으므로 오차의 절대적인 크기를 구할 수가 없다.\n",
    "- 그래서 **모든 오차를 제곱하여 더하는 방법**을 사용한다.\n",
    "- 다시 말해 위의 그림에서의 모든 점과 직선 사이의 ↕ 거리를 제곱하고 모두 더한다.\n",
    "- 이를 수식으로 표현하면 아래와 같다. (여기서 $n$은 갖고 있는 데이터의 개수를 의미)\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^n \\left[ y^{(i)} - H \\left( x^{(i)} \\right) \\right]^2 \n",
    "&= \\left( -2 \\right)^2 + 10^2 + \\left( -7 \\right)^2 + \\left( -5 \\right)^2 \\\\\n",
    "&= 178\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G2U-2j5TuaBT"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.7 오차의 크기를 측정하기 위한 방법 - 단계 (3) : 오차의 제곱합의 평균\n",
    "\n",
    "- 이 때 데이터의 개수인 $n$으로 나누면, 오차의 제곱합에 대한 평균을 구할 수 있다.\n",
    "- 이를 **평균 제곱 오차(Mean Squared Error, MSE)**라고 한다.\n",
    "- 수식은 아래와 같다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "\\begin{align*}\n",
    "{1 \\over n} \\sum_{i=1}^n \\left[ y^{(i)} - H \\left( x^{(i)} \\right) \\right]^2\n",
    "&= 178 / 4 \\\\\n",
    "&= 44.5\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "- $y=13x+1$의 예측값과 실제값의 평균 제곱 오차의 값은 44.5이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LqwRCDvUvEmX"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.8 비용 함수(Cost function) 재정의\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "cost(W, b) = {1 \\over n} \\sum_{i=1}^n \\left[ y^{(i)} - H \\left( x^{(i)} \\right) \\right]^2\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vClir9v3vSMW"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.9 평균 제곱 오차와 $W$와 $b$의 관계\n",
    "\n",
    "- 모든 점들과의 오차가 클 수록 평균 제곱 오차는 커지며, 오차가 작아질 수록 평균 제곱 오차는 작아진다.\n",
    "- 그러므로 이 평균 최곱 오차. 즉, $Cost(W,b)$를 최소가 되게 만드는 $W$와 $b$를 구하면 결과적으로 $y$와 $x$의 관계를 가장 잘 나타내는 직선을 그릴 수 있다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "W, b \\rightarrow minimize \\; cost(W, b)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wtNhpUDrvnyI"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 3.4 옵티마이저(Optimizer) : 경사하강법(Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MBUAWcG9vsVI"
   },
   "source": [
    "### 3.4.1 옵티마이저(Optimizer) : 최적화 알고리즘\n",
    "\n",
    "- 선형 회귀를 포함한 수많은 머신 러닝, 딥 러닝의 학습은 결국 비용 함수를 최소화하는 매개 변수인 $W$와 $b$을 찾기 위한 작업을 수행한다.\n",
    "- 이때 사용되는 알고리즘을 **옵티마이저(Optimizer)** 또는 **최적화 알고리즘**이라고 부릅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RNep75lYzRNB"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.2 학습(Training)\n",
    "\n",
    "- 이 옵티마이저를 통해 적절한 $W$와 $b$를 찾아내는 과정을 머신 러닝에서 **학습(training)**이라고 부른다.\n",
    "- 여기서는 가장 기본적인 옵티마이저 알고리즘인 경사 하강법(Gradient Descent)에 대해서 배운다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XKrx5xtnzdwQ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.3 cost와 기울기 $W$와의 관계\n",
    "\n",
    "- 경사 하강법을 이해하기 위해서 cost와 기울기 W와의 관계를 이해해본다.\n",
    "- $W$는 머신 러닝 용어로는 가중치라고 불리지만, 직선의 방정식 관점에서 보면 직선의 기울기를 의미한다.\n",
    "- 아래의 그래프는 기울기 $W$가 지나치게 높거나, 낮을 때 어떻게 오차가 커지는 보여준다.  \n",
    "<img src=\"https://wikidocs.net/images/page/53560/%EA%B7%B8%EB%A6%BC4.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kLzItP9n0WTj"
   },
   "source": [
    "- ↕ : 각 점에서의 실제값과 두 직선의 예측값과의 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EndQyefOzw9n"
   },
   "source": [
    "- <font color=\"orange\">주황색</font>선\n",
    "  - $y = 20x$에 해당되는 직선\n",
    "  - 기울기 $W$가 20\n",
    "  - 기울기가 지나치게 크면 실제 값과 예측값의 오차가 커진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VOd5wJ6B0IJV"
   },
   "source": [
    "- <font color=\"green\">초록색</font>선\n",
    "  - $y = x$에 해당되는 직선\n",
    "  - 기울기 $W$가 1\n",
    "  - 기울기가 지나치게 작아도 실제 값과 예측값의 오차가 커진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DG418uGo0Mz6"
   },
   "source": [
    "- 사실 $b$ 또한 마찬가지로 지나치게 크거나 작으면 오차가 커진다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fDry2lM00nGk"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.4 $W$와 cost의 관계 그래프\n",
    "\n",
    "- 설명의 편의를 위해 편향 $b$가 없이 단순히 가중치 $W$만을 사용한 $y=Wx$라는 가설 $H(x)$를 가지고, 경사 하강법을 수행한다.\n",
    "- 비용 함수의 값 $cost(W)$는 cost라고 줄여서 표현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "krbaUq_c06Eb"
   },
   "source": [
    "- 이에 따라 $W$와 cost의 관계를 그래프로 표현하면 다음과 같다.  \n",
    "\n",
    "<img src=\"https://wikidocs.net/images/page/21670/%EA%B8%B0%EC%9A%B8%EA%B8%B0%EC%99%80%EC%BD%94%EC%8A%A4%ED%8A%B8.PNG\" />\n",
    "\n",
    "- 기울기 $W$가 무한대로 커지면 커질 수록 cost의 값 또한 무한대로 커진다.\n",
    "- 반대로 기울기 $W$가 무한대로 작아져도 cost의 값은 무한대로 커진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-iYEO5r11laS"
   },
   "source": [
    "### 3.4.5 cost가 가장 작을 때의 $W$ 값\n",
    "\n",
    "- 위의 그래프에서 cost가 가장 작을 때는 볼록한 부분의 맨 아래 부분이다.\n",
    "- 기계가 해야할 일은 cost가 가장 최소값을 가지게 하는 $W$를 찾는 일이다.\n",
    "- 그러므로 볼록한 부분의 맨 아래 부분의 $W$의 값을 찾아야 한다.  \n",
    "\n",
    "<img src=\"https://wikidocs.net/images/page/21670/%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7paTlVDy1f7e"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.6 $W$값이 수정되는 과정\n",
    "\n",
    "- 기계는 임의의 랜덤값 $W$값을 정한 뒤에, 맨 아래의 볼록한 부분을 향해 점차 $W$의 값을 수정해나간다.\n",
    "- 위의 그림은 $W$값이 점차 수정되는 과정을 보여준다.\n",
    "- 그리고 이를 가능하게 하는 것이 경사 하강법(Gradient Descent)이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mUcS1jwv19zw"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.7 접선에서의 기울기의 개념\n",
    "\n",
    "- 경사 하강법을 이해하기 위해서는 미분을 이해해야 한다.\n",
    "- 경사 하강법은 한 점에서의 순간 변화율 또는 접선에서의 기울기의 개념을 사용한다.  \n",
    "<img src=\"https://wikidocs.net/images/page/21670/%EC%A0%91%EC%84%A0%EC%9D%98%EA%B8%B0%EC%9A%B8%EA%B8%B01.PNG\" />\n",
    "- 위의 그림에서 초록색 선은 $W$가 임의의 값을 가지게 되는 네 가지의 경우에 대해서, 그래프 상으로 접선의 기울기를 보여준다.\n",
    "- 맨 아래의 볼록한 부분으로 갈수록 접선의 기울기가 점차 작아진다.\n",
    "- 그리고 맨 아래의 볼록한 부분에서는 결국 접선의 기울기가 0이 된다.\n",
    "- 그래프 상으로는 초록색 화살표가 수평이 되는 지점이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_QSWwcQZ2dEK"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.8 경사 하강법의 아이디어\n",
    "\n",
    "- cost가 최소화가 되는 지점은 접선의 기울기가 0이 되는 지점이며, 또한 미분값이 0이 되는 지점이다.\n",
    "- 경사 하강법의 아이디어는 비용 함수(Cost function)를 미분하여 현재 W에서의 접선의 기울기를 구하고, 접선의 기울기가 낮은 방향으로 $W$의 값을 변경하고 다시 미분하고 이 과정을 접선의 기울기가 0인 곳을 향해 $W$의 값을 변경하는 작업을 반복하는 것에 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D9dCLYcv2reP"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.9 $W$를 업데이트 하는 식\n",
    "\n",
    "- 비용 함수(Cost function)는 아래와 같다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "cost(W, b) = {1 \\over n} \\sum_{i=1}^n \\left[ y^{(i)} - H \\left( x^{(i)} \\right) \\right]^2\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TI4X9NXl211n"
   },
   "source": [
    "- 비용(cost)를 최소화하는 $W$를 구하기 위해 $W$를 업데이트하는 식은 다음과 같다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "W := W - \\alpha  {\\partial \\over \\partial W} cost(W)\n",
    "$\n",
    "\n",
    "- 위의 식은 현재 $W$에서의 접선의 기울기(${\\partial \\over \\partial W} cost(W)$)와 $\\alpha$와 곱한 값을 현재 $W$에서 뺴서 새로운 $W$의 값으로 한다는 것을 의미한다.\n",
    "- $\\alpha$는 여기서 **학습률(learning rate)**이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xYLvwBch4URP"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.10 현재 $W$에서 현재 $W$에서의 접선의 기울기를 빼는 행위의 의미\n",
    "\n",
    "- 우선 $\\alpha$는 생각하지 않고 현재 $W$에서 현재 $W$에서의 접선의 기울기를 빼는 행위가 어떤 의미가 있는 지 알아보자.  \n",
    "<img src=\"https://wikidocs.net/images/page/21670/%EB%AF%B8%EB%B6%84.PNG\" />\n",
    "- 위 그림은 접선의 기울기가 음수일 때, 0일 때, 양수일 때의 경우를 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R2BtruI54wYE"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.10.1 접선의 기울기가 음수일 때\n",
    "\n",
    "- 접선의 기울기가 음수일 때는 위의 수식이 아래와 같이 표현할 수 있다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "W := W - \\alpha \\left(음수기울기\\right) = W + \\alpha \\left(양수기울기\\right)\n",
    "$\n",
    "\n",
    "- 즉, 기울기가 음수면 $W$의 값이 **증가**하게 된다.\n",
    "- 이는 결과적으로 접선의 기울기가 0인 방향으로 $W$의 값이 조정된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lftZY2GW5V-Z"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.10.2 접선의 기울기가 양수일 때\n",
    "\n",
    "- 접선의 기울기가 양수일 때는 위의 수식이 아래와 같이 표현될 수 있다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "W := W - \\alpha \\left(양수기울기\\right)\n",
    "$\n",
    "\n",
    "- 기울기가 양수면 $W$의 값이 **감소**한다.\n",
    "- 이는 결과적으로 기울기가 0인 방향으로 $W$의 값이 조정된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F7gDkdp85taz"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.10.3 종합\n",
    "\n",
    "- 결국, 아래의 수식은 접선의 기울기가 음수이거나, 양수일 때 모두 접선의 기울기가 0인 방향으로 $W$의 값을 조정한다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "W := W - \\alpha  {\\partial \\over \\partial W} cost(W)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pf2fA7xn559z"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.11 학습률(learning rate)\n",
    "\n",
    "- 학습률(learning rate)이라고 말하는 $\\alpha$는 어떤 의미를 가질까?\n",
    "- 학습률 $\\alpha$는 $W$의 값을 변경할 때, 얼마나 크게 변경할 지를 결정한다.\n",
    "- 또는 $W$를 그래프의 한 점으로보고 접선의 기울기가 0일 때까지 경사를 따라 내려간다는 관점에서는 얼마나 큰 폭으로 이동할지를 결정한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dE5VdZvo6NBh"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.11.1 학습률이 지나치게 큰 경우\n",
    "\n",
    "<img src=\"https://wikidocs.net/images/page/21670/%EA%B8%B0%EC%9A%B8%EA%B8%B0%EB%B0%9C%EC%82%B0.PNG\" />\n",
    "\n",
    "- 위의 그림은 학습률 $\\alpha$가 지나치게 높은 값을 가질 때, 접선의 기울기가 0이 되는 $W$를 찾아가는 것이 아니라 $W$의 값이 발산하는 상황을 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s3h-usRR6miZ"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.11.2 학습률이 지나치게 작을 경우\n",
    "\n",
    "- 반대로 학습률 α가 지나치게 낮은 값을 가지면 학습 속도가 느려지므로 적당한 α의 값을 찾아내는 것도 중요합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "88dZGL2Z6tQg"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.11.3 실제 경사 하강법\n",
    "\n",
    "- 지금까지는 $b$는 배제시키고 최적의 $W$를 찾아내는 것에만 초점을 맞추어 경사 하강법의 원리에 대해서 배웠다.\n",
    "- 하지만 실제 경사 하강법은 $W$와 $b$에 대해서 동시에 경사 하강법을 수행하면서 최적의 $W$와 $b$의 값을 찾아간다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oFIPR1L8661f"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.12 정리\n",
    "\n",
    "- 가설, 비용 함수, 옵티마이저는 머신 러닝 분야에서 사용되는 포괄적 개념이다.\n",
    "- 풀고자하는 각 문제에 따라 가설, 비용 함수, 옵티마이저는 전부 다를 수 있다.\n",
    "- 선형 회귀에 가장 적합한 비용 함수와 옵티마이저는 각각 MSE와 경사 하강법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bnF26e4B7HBP"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 3.5 케라스로 구현하는 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kQD9_qy27Kr_"
   },
   "source": [
    "### 3.5.1 케라스로 모델을 만드는 기본적인 형식\n",
    "\n",
    "```python\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(1, input_dim=1))\n",
    "```\n",
    "\n",
    "- `Sequential`로 `model`이라는 이름의 모델을 만듬\n",
    "- `add()`를 통해 필요한 사항들을 추가해간다.\n",
    "  - 첫 번째 인자 - `1` : 출력의 차원을 의미\n",
    "  - 두 번째 인자 - `input_dim` : 입력의 차원을 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIQwtakMWT8M"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.5.2 케라스를 이용한 단순 선형 회귀 구현\n",
    "\n",
    "- 1개의 실수 $x$를 가지고 1개의 실수 $y$를 예측하는 단순 선형 회귀를 구현하는 경우에는 `add()`의 인자로 각각 1의 값을 갖는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PAqcMT5qW1oI"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "iND_VZgBW4-B",
    "outputId": "1481a1a2-f295-48eb-b46a-246915b71f10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.x selected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vNuvFr6VW8pR",
    "outputId": "def9f90b-3e9d-49a2-8886-dc1d47b21a63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9 samples\n",
      "Epoch 1/300\n",
      "9/9 [==============================] - 0s 43ms/sample - loss: 286.0453 - mse: 286.0453\n",
      "Epoch 2/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1138 - mse: 2.1138\n",
      "Epoch 3/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1148 - mse: 2.1148\n",
      "Epoch 4/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1158 - mse: 2.1158\n",
      "Epoch 5/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1167 - mse: 2.1167\n",
      "Epoch 6/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1177 - mse: 2.1177\n",
      "Epoch 7/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1186 - mse: 2.1186\n",
      "Epoch 8/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1195 - mse: 2.1195\n",
      "Epoch 9/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1203 - mse: 2.1203\n",
      "Epoch 10/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1211 - mse: 2.1211\n",
      "Epoch 11/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1219 - mse: 2.1219\n",
      "Epoch 12/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1227 - mse: 2.1227\n",
      "Epoch 13/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1235 - mse: 2.1235\n",
      "Epoch 14/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1242 - mse: 2.1242\n",
      "Epoch 15/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1249 - mse: 2.1249\n",
      "Epoch 16/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1256 - mse: 2.1256\n",
      "Epoch 17/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1262 - mse: 2.1262\n",
      "Epoch 18/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1269 - mse: 2.1269\n",
      "Epoch 19/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1275 - mse: 2.1275\n",
      "Epoch 20/300\n",
      "9/9 [==============================] - 0s 1ms/sample - loss: 2.1281 - mse: 2.1281\n",
      "Epoch 21/300\n",
      "9/9 [==============================] - 0s 1ms/sample - loss: 2.1287 - mse: 2.1287\n",
      "Epoch 22/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1292 - mse: 2.1292\n",
      "Epoch 23/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1298 - mse: 2.1298\n",
      "Epoch 24/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1303 - mse: 2.1303\n",
      "Epoch 25/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1308 - mse: 2.1308\n",
      "Epoch 26/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1313 - mse: 2.1313\n",
      "Epoch 27/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1318 - mse: 2.1318\n",
      "Epoch 28/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1322 - mse: 2.1322\n",
      "Epoch 29/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1327 - mse: 2.1327\n",
      "Epoch 30/300\n",
      "9/9 [==============================] - 0s 1ms/sample - loss: 2.1331 - mse: 2.1331\n",
      "Epoch 31/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1335 - mse: 2.1335\n",
      "Epoch 32/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1339 - mse: 2.1339\n",
      "Epoch 33/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1343 - mse: 2.1343\n",
      "Epoch 34/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1347 - mse: 2.1347\n",
      "Epoch 35/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1351 - mse: 2.1351\n",
      "Epoch 36/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1354 - mse: 2.1354\n",
      "Epoch 37/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1358 - mse: 2.1358\n",
      "Epoch 38/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1361 - mse: 2.1361\n",
      "Epoch 39/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1364 - mse: 2.1364\n",
      "Epoch 40/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1368 - mse: 2.1368\n",
      "Epoch 41/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1371 - mse: 2.1371\n",
      "Epoch 42/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1374 - mse: 2.1374\n",
      "Epoch 43/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1376 - mse: 2.1376\n",
      "Epoch 44/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1379 - mse: 2.1379\n",
      "Epoch 45/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1382 - mse: 2.1382\n",
      "Epoch 46/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1384 - mse: 2.1384\n",
      "Epoch 47/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1387 - mse: 2.1387\n",
      "Epoch 48/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1389 - mse: 2.1389\n",
      "Epoch 49/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1392 - mse: 2.1392\n",
      "Epoch 50/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1394 - mse: 2.1394\n",
      "Epoch 51/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1396 - mse: 2.1396\n",
      "Epoch 52/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1398 - mse: 2.1398\n",
      "Epoch 53/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1400 - mse: 2.1400\n",
      "Epoch 54/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1402 - mse: 2.1402\n",
      "Epoch 55/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1404 - mse: 2.1404\n",
      "Epoch 56/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1406 - mse: 2.1406\n",
      "Epoch 57/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1408 - mse: 2.1408\n",
      "Epoch 58/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1409 - mse: 2.1409\n",
      "Epoch 59/300\n",
      "9/9 [==============================] - 0s 1ms/sample - loss: 2.1411 - mse: 2.1411\n",
      "Epoch 60/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1413 - mse: 2.1413\n",
      "Epoch 61/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1414 - mse: 2.1414\n",
      "Epoch 62/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1416 - mse: 2.1416\n",
      "Epoch 63/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1417 - mse: 2.1417\n",
      "Epoch 64/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1419 - mse: 2.1419\n",
      "Epoch 65/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1420 - mse: 2.1420\n",
      "Epoch 66/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1421 - mse: 2.1421\n",
      "Epoch 67/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1422 - mse: 2.1422\n",
      "Epoch 68/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1424 - mse: 2.1424\n",
      "Epoch 69/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1425 - mse: 2.1425\n",
      "Epoch 70/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1426 - mse: 2.1426\n",
      "Epoch 71/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1427 - mse: 2.1427\n",
      "Epoch 72/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1428 - mse: 2.1428\n",
      "Epoch 73/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1429 - mse: 2.1429\n",
      "Epoch 74/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1430 - mse: 2.1430\n",
      "Epoch 75/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1431 - mse: 2.1431\n",
      "Epoch 76/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1432 - mse: 2.1432\n",
      "Epoch 77/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1433 - mse: 2.1433\n",
      "Epoch 78/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1434 - mse: 2.1434\n",
      "Epoch 79/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1435 - mse: 2.1435\n",
      "Epoch 80/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1436 - mse: 2.1436\n",
      "Epoch 81/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1437 - mse: 2.1437\n",
      "Epoch 82/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1437 - mse: 2.1437\n",
      "Epoch 83/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1438 - mse: 2.1438\n",
      "Epoch 84/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1439 - mse: 2.1439\n",
      "Epoch 85/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1440 - mse: 2.1440\n",
      "Epoch 86/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1440 - mse: 2.1440\n",
      "Epoch 87/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1441 - mse: 2.1441\n",
      "Epoch 88/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1442 - mse: 2.1442\n",
      "Epoch 89/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1442 - mse: 2.1442\n",
      "Epoch 90/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1443 - mse: 2.1443\n",
      "Epoch 91/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1443 - mse: 2.1443\n",
      "Epoch 92/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1444 - mse: 2.1444\n",
      "Epoch 93/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1444 - mse: 2.1444\n",
      "Epoch 94/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1445 - mse: 2.1445\n",
      "Epoch 95/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1445 - mse: 2.1445\n",
      "Epoch 96/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1446 - mse: 2.1446\n",
      "Epoch 97/300\n",
      "9/9 [==============================] - 0s 3ms/sample - loss: 2.1446 - mse: 2.1446\n",
      "Epoch 98/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1447 - mse: 2.1447\n",
      "Epoch 99/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1447 - mse: 2.1447\n",
      "Epoch 100/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1448 - mse: 2.1448\n",
      "Epoch 101/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1448 - mse: 2.1448\n",
      "Epoch 102/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1449 - mse: 2.1449\n",
      "Epoch 103/300\n",
      "9/9 [==============================] - 0s 1ms/sample - loss: 2.1449 - mse: 2.1449\n",
      "Epoch 104/300\n",
      "9/9 [==============================] - 0s 1ms/sample - loss: 2.1449 - mse: 2.1449\n",
      "Epoch 105/300\n",
      "9/9 [==============================] - 0s 1ms/sample - loss: 2.1450 - mse: 2.1450\n",
      "Epoch 106/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1450 - mse: 2.1450\n",
      "Epoch 107/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1450 - mse: 2.1450\n",
      "Epoch 108/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1451 - mse: 2.1451\n",
      "Epoch 109/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1451 - mse: 2.1451\n",
      "Epoch 110/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1451 - mse: 2.1451\n",
      "Epoch 111/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1452 - mse: 2.1452\n",
      "Epoch 112/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1452 - mse: 2.1452\n",
      "Epoch 113/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1452 - mse: 2.1452\n",
      "Epoch 114/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1452 - mse: 2.1452\n",
      "Epoch 115/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1453 - mse: 2.1453\n",
      "Epoch 116/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1453 - mse: 2.1453\n",
      "Epoch 117/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1453 - mse: 2.1453\n",
      "Epoch 118/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1453 - mse: 2.1453\n",
      "Epoch 119/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1454 - mse: 2.1454\n",
      "Epoch 120/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1454 - mse: 2.1454\n",
      "Epoch 121/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1454 - mse: 2.1454\n",
      "Epoch 122/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1454 - mse: 2.1454\n",
      "Epoch 123/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1454 - mse: 2.1454\n",
      "Epoch 124/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1455 - mse: 2.1455\n",
      "Epoch 125/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1455 - mse: 2.1455\n",
      "Epoch 126/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1455 - mse: 2.1455\n",
      "Epoch 127/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1455 - mse: 2.1455\n",
      "Epoch 128/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1455 - mse: 2.1455\n",
      "Epoch 129/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1456 - mse: 2.1456\n",
      "Epoch 130/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1456 - mse: 2.1456\n",
      "Epoch 131/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1456 - mse: 2.1456\n",
      "Epoch 132/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1456 - mse: 2.1456\n",
      "Epoch 133/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1456 - mse: 2.1456\n",
      "Epoch 134/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1456 - mse: 2.1456\n",
      "Epoch 135/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1456 - mse: 2.1456\n",
      "Epoch 136/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1456 - mse: 2.1456\n",
      "Epoch 137/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1457 - mse: 2.1457\n",
      "Epoch 138/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1457 - mse: 2.1457\n",
      "Epoch 139/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1457 - mse: 2.1457\n",
      "Epoch 140/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1457 - mse: 2.1457\n",
      "Epoch 141/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1457 - mse: 2.1457\n",
      "Epoch 142/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1457 - mse: 2.1457\n",
      "Epoch 143/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1457 - mse: 2.1457\n",
      "Epoch 144/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1457 - mse: 2.1457\n",
      "Epoch 145/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1457 - mse: 2.1457\n",
      "Epoch 146/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1458 - mse: 2.1458\n",
      "Epoch 147/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1458 - mse: 2.1458\n",
      "Epoch 148/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1458 - mse: 2.1458\n",
      "Epoch 149/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1458 - mse: 2.1458\n",
      "Epoch 150/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1458 - mse: 2.1458\n",
      "Epoch 151/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1458 - mse: 2.1458\n",
      "Epoch 152/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1458 - mse: 2.1458\n",
      "Epoch 153/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1458 - mse: 2.1458\n",
      "Epoch 154/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1458 - mse: 2.1458\n",
      "Epoch 155/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1458 - mse: 2.1458\n",
      "Epoch 156/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1458 - mse: 2.1458\n",
      "Epoch 157/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1458 - mse: 2.1458\n",
      "Epoch 158/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1458 - mse: 2.1458\n",
      "Epoch 159/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1458 - mse: 2.1458\n",
      "Epoch 160/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 161/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 162/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 163/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 164/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 165/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 166/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 167/300\n",
      "9/9 [==============================] - 0s 1ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 168/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 169/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 170/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 171/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 172/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 173/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 174/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 175/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 176/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 177/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 178/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 179/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 180/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 181/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 182/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 183/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 184/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 185/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 186/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 187/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1459 - mse: 2.1459\n",
      "Epoch 188/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 189/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 190/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 191/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 192/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 193/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 194/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 195/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 196/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 197/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 198/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 199/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 200/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 201/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 202/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 203/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 204/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 205/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 206/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 207/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 208/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 209/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 210/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 211/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 212/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 213/300\n",
      "9/9 [==============================] - 0s 3ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 214/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 215/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 216/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 217/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 218/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 219/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 220/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 221/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 222/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 223/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 224/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 225/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 226/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 227/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 228/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 229/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 230/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 231/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 232/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 233/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 234/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 235/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 236/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 237/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 238/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 239/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 240/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 241/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 242/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 243/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 244/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 245/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 246/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 247/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 248/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 249/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 250/300\n",
      "9/9 [==============================] - 0s 1ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 251/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 252/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 253/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 254/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 255/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 256/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 257/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 258/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 259/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 260/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 261/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 262/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 263/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 264/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 265/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 266/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 267/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 268/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 269/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 270/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 271/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 272/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 273/300\n",
      "9/9 [==============================] - 0s 1ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 274/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 275/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 276/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 277/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 278/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 279/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 280/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 281/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 282/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 283/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 284/300\n",
      "9/9 [==============================] - 0s 1ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 285/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 286/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 287/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 288/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 289/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 290/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 291/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 292/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 293/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 294/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 295/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 296/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 297/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 298/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 299/300\n",
      "9/9 [==============================] - 0s 3ms/sample - loss: 2.1460 - mse: 2.1460\n",
      "Epoch 300/300\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 2.1460 - mse: 2.1460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff2e2d4eac8>"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "import numpy as np\n",
    "\n",
    "# x : 공부하는 시간\n",
    "X = np.array([1,2,3,4,5,6,7,8,9]) \n",
    "\n",
    "# y : 각 공부하는 시간에 맵핑되는 성적\n",
    "y = np.array([11,22,33,44,53,66,77,87,95])\n",
    "\n",
    "model = Sequential()\n",
    "# activation : 어떤 함수를 사용할 것인지? -> 선형 회귀를 사용할 경우에는 linear라고 기재\n",
    "model.add(Dense(1, input_dim=1, activation='linear')) \n",
    "\n",
    "# sgd : 확률적 경사하강법\n",
    "sgd = optimizers.SGD(lr=0.01) # 학습률(learning rate, lr) = 0.01\n",
    "\n",
    "# loss : 손실함수(Loss function) = 평균제곱오차 mse를 사용\n",
    "model.compile(optimizer=sgd, loss='mse', metrics=['mse'])\n",
    "\n",
    "# 주어진 X와 y 데이터에 대해서 오차를 최소화하는 작업을 300번 시도\n",
    "model.fit(X, y, batch_size=1, epochs=300, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YzEo4SUNX37Q"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.5.3 최적화된 가중치\n",
    "\n",
    "- 전체 데이터에 대한 훈련 횟수는 300으로 하였지만, 어느 순간 오차가 더 이상 줄어들지 않는다.\n",
    "- 이는 오차를 최소화하는 가중치 $W$와 $b$를 찾았기 때문으로 추정이 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QG0Y5M7YYU8V"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.5.4 오차를 최소화하는 직선 시각화\n",
    "\n",
    "- 최종적으로 선택된 오차를 최소화하는 직선을 그래프로 그려보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "5wMjemxgYjUF",
    "outputId": "79134de8-50c9-4163-f715-ddbec976f101"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff2dbdf3c88>,\n",
       " <matplotlib.lines.Line2D at 0x7ff2dbdf3dd8>]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuIhITCnARURiSgEuIhJT/w8K4T+k2RSp6AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(X, model.predict(X), 'b', X, y, 'k.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YBLmjQ79Yqeu"
   },
   "source": [
    "- 각 점 : 우리가 실제로 주었던 실제값\n",
    "- 직선 : 실제값으로부터 오차를 최소화하는 $W$와 $b$의 값을 가지는 직선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FL4gNQTYzEQ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.5.5 예측\n",
    "\n",
    "- 이제 이 직선을 통해 9시간 30분을 공부하였을 때의 시험 성적을 예측해보자.\n",
    "`model.predict()`은 학습이 완료된 모델이 입력된 데이터에 대해서 어떤 값을 예측하는 지를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ktd9nfkmY_jo",
    "outputId": "ec18c9e0-c59e-4392-c097-ec37440f3772"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[98.556465]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([9.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXJKLaL3ZBgv"
   },
   "source": [
    "- 9시간 30분을 공부하면 약 98.5점을 얻는다고 예측하고 있다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch07_v03_Linear-Regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
