{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N78UnOpsjoq6"
   },
   "source": [
    "# Ch07. 머신 러닝(Machine Learning) 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xMzbIZk2juWE"
   },
   "source": [
    "# v03. 선형 회귀 (Linear Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJtKq13Wjw4r"
   },
   "source": [
    "- 이번 챕터에서는 머신 러닝에서 쓰이는 다음과 같은 용어들에 대한 개념과 선형 회귀에 대해서 이해한다.\n",
    "  - 가설(Hypothesis)\n",
    "  - 손실 함수(Loss Function)\n",
    "  - 경사 하강법(Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QZ_27P65j94I"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 3.1 선형 회귀 (Linear Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sOrZVWSOkBxQ"
   },
   "source": [
    "- 어떤 요인의 수치에 따라서 특정 요인의 수치가 영향을 받고있다.\n",
    "- 어떤 변수의 값에 따라서 특정 변수의 값이 영향을 받고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UEhRSLbslLKp"
   },
   "source": [
    "- 다른 변수의 값을 변하게 하는 변수 : $x$\n",
    "  - 변수 $x$의 값은 독립적으로 변할 수 있다.\n",
    "  - $x$를 독립 변수라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pzTW-fFqlNeh"
   },
   "source": [
    "- 변수 $x$에 의해서 값이 종속적으로 변하는 변수 : $y$\n",
    "  - 변수 $y$의 값은 계속해서 $x$의 값에 의해서 종속적으로 결정된다.\n",
    "  - $y$를 종속 변수라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ClAyws5YliQt"
   },
   "source": [
    "- 선형 회귀는 한 개 이상의 독립 변수 $x$와 $y$의 선형 관계를 모델링한다.\n",
    "- 만약, 독립 변수 x가 1개라면 단순 선형 회귀라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qOF9cE1rlq_c"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.1.1 단순 선형 회귀 분석 (Simple Linear Regression Analysis)\n",
    "\n",
    "#### 3.1.1.1 단순 선형 회귀 수식\n",
    "\n",
    "$\n",
    "\\quad\n",
    "y = W x + b\n",
    "$\n",
    "\n",
    "<br>\n",
    "\n",
    "- $W$\n",
    "  - 독립 변수 x와 곱해지는 값 \n",
    "  - 가중치(weight)\n",
    "  - 직선의 방정식에서 직선의 기울기\n",
    "- $b$\n",
    "  - 별도로 더해지는 값\n",
    "  - 편향(bias)\n",
    "  - 직선의 방정식에서 절편"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "chXi54izmBzn"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.1.1.2 $W$ 와 $b$가 없이 $y$와 $x$ 수식\n",
    "\n",
    "$\n",
    "\\quad\n",
    "y = x\n",
    "$\n",
    "\n",
    "- $y$는 $x$와 같다는 하나의 식밖에 표현하지 못한다.\n",
    "- 그래프 상으로 말하면, 하나의 직선밖에 표현하지 못한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LresdVXymk4W"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.1.1.3 관계 모델링\n",
    "\n",
    "- 다시 말해 $W$와 $b$의 값을 적절히 찾아내면 $x$와 $y$의 관계를 적절히 모델링한 것이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CFgNlvNnmwVP"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.1.2 다중 선형 회귀 분석(Multiple Linear Regression Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vjGUeYLqm05G"
   },
   "source": [
    "#### 3.1.2.1 다중 선형 회귀 수식\n",
    "\n",
    "$\n",
    "\\quad\n",
    "y = {W_1x_1 + W_2x_2 + \\cdots + W_nx_n + b}\n",
    "$\n",
    "\n",
    "<br>\n",
    "\n",
    "- $y$ 는 여전히 1개이지만 이제 $x$는 1개가 아니라 여러 개가 되었다.\n",
    "- 이를 다중 선형 회귀 분석이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qjx76uoCnAct"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 3.2 가설(Hypothesis) 세우기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Kq_G9lDnWta"
   },
   "source": [
    "### 3.2.1 단순 선형 회귀 문제\n",
    "\n",
    "#### 3.2.1.1 데이터\n",
    "\n",
    "- 어떤 학생의 공부 시간에 따라서 다음과 같은 점수를 얻었다는 데이터가 있다.\n",
    "\n",
    "| hours($x$) | score($y$) |\n",
    "| :--------- | :--------- |\n",
    "| 2          | 25         |\n",
    "| 3          | 50         |\n",
    "| 4          | 42         |\n",
    "| 5          | 61         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l_AaS5uuneJc"
   },
   "source": [
    "- 이를 좌표 평면에 그려보면 다음과 같다.  \n",
    "<img src=\"https://wikidocs.net/images/page/53560/%EA%B7%B8%EB%A6%BC1.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iz7wW48SnpU7"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.2.1.2 가설(Hypothesis)\n",
    "\n",
    "- 알고있는 데이터로부터 $x$와 $y$의 관계를 유추하고, 이 학생이 6시간, 7시간, 8시간을 공부하였을 때의 성적을 예측해보고 싶다.\n",
    "- $x$ 와 $y$의 관계를 유추하기 위해서 수학적으로 식을 세울 수 있다.  \n",
    "$\\rightarrow$ 머신 러닝에서는 이러한 식을 **가설(Hypothesis)**이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mjofGmbXoAIj"
   },
   "source": [
    "- 아래의 $H(x)$에서 $H$는 Hypothesis를 의미한다.\n",
    "- 사실 선형 회귀의 가설은 이미 아래와 같이 널리 알려져 있다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "H(x) = W x + b\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yyBsYi6soo6Z"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.2.1.3 선형 회귀에서 해야할 일\n",
    "\n",
    "-  아래 그림과 같이 $W$와 $b$의 값에 따라서 천차만별로 그려지는 직선의 모습을 보여준다.  \n",
    "<img src=\"https://wikidocs.net/images/page/21670/W%EC%99%80_b%EA%B0%80_%EB%8B%A4%EB%A6%84.PNG\" />\n",
    "- 위의 가설에서 $W$는 직선의 기울기이고 $b$는 절편으로 직선을 표현함을 알 수 있다.\n",
    "- 결국 선형 회귀는 주어진 데이터로부터 $y$와 $x$의 관계를 가장 잘 나타내는 직선을 그리는 일을 말한다.\n",
    "- 어떤 직선인지 결정하는 것은 $W$와 $b$의 값이다.\n",
    "- 그러므로 선형 회귀에서 해야할 일은 결국 **적절한 $W$와 $b$를 찾아내는 일**이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S5UZN7r2pBvc"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 3.3 비용 함수 (Cost function) : 평균 제곱 오차(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C1Je7BbKq0Jo"
   },
   "source": [
    "### 3.3.1 실제값과 예측값의 오차를 계산하는 식\n",
    "\n",
    "- 앞서 주어진 데이터에서 $x$와 $y$의 관계를 $W$와 $b$를 이용하여 식을 세우는 일을 **가설**이라고 했다.\n",
    "- 이제 해야할 일은 문제에 대한 규칙을 가장 잘 표현하는 $W$와 $b$를 찾는 일이다.\n",
    "- 머신 러닝은 $W$와 $b$를 찾기 위해서 실제값과 가설로부터 얻은 예측값의 오차를 계산하는 식을 세운다.\n",
    "- 그리고 이 식의 값을 최소화하는 최적의 $W$와 $b$를 찾아냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YrNMMAt4rTEI"
   },
   "source": [
    "- 이 때 실제값과 예측값에 대한 오차에 대한 식을 다음과 같이 부른다.\n",
    "  - **목적 함수(Objective function)**\n",
    "    - 함수의 값을 최소화하거나, 최대화하거나 하는 목적을 가진 함수\n",
    "  - **비용 함수(Cost function)**\n",
    "    - 값을 최소화하려고 하는 함수\n",
    "  - **손실 함수(Loss function)**\n",
    "    - 값을 최소화하려고 하는 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RfqRmxl9roiq"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.2 평균 제곱 오차(Mean Sqaured Error, MSE)\n",
    "\n",
    "- 비용 함수는 단순히 실제값과 예측값에 대한 오차를 표현하면 되는 것이 아니라, 예측값의 오차를 줄이는 일에 최적화 된 식이어야 한다.\n",
    "- 머신 러닝, 딥 러닝에는 다양한 문제들이 있고, 각 문제들에는 적합한 비용 함수들이 있습니다.\n",
    "- **회귀 문제**의 경우에는 주로 **평균 제곱 오차(Mean Squered Error, MSE)**가 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5qA8B4ngr838"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.3 $y$ 와 $x$의 관계를 가장 잘 나타내는 직선\n",
    "\n",
    "- 아래의 그래프에 임의의 $W$의 값 13과 임의의 $b$의 값 1을 가진 직선을 그렸다.  \n",
    "<img src=\"https://wikidocs.net/images/page/53560/%EA%B7%B8%EB%A6%BC3.PNG\" />\n",
    "- 임의로 그린 직선으로 정답이 아니다.\n",
    "- 이 직선으로부터 서서히 $W$와 $b$의 값을 바꾸면서 정답인 직선을 찾아내야 한다.\n",
    "- $y$ 와 $x$의 관계를 가장 잘 나타내는 직선을 그린다는 것  \n",
    "$\\rightarrow$ 위의 그림에서 모든 점들과 위치적으로 가장 가까운 직선을 그린다는 것과 같ek."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oTZyc2ZRsiNL"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.4 오차(error) 정의\n",
    "\n",
    "- 오차는 주어진 데이터에서 각 $x$에서의 실제값 $y$와 위의 직선에서 예측하고 있는 $H(x)$값의 차이를 말한다.\n",
    "- 즉, 위의 그림에서 ↕는 각 점에서의 오차의 크기를 보여준다.\n",
    "- 오차를 줄여가면서 $W$와 $b$의 값을 찾아내기 위해서는 **전체 오차의 크기**를 구해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B83FCiKxs4M9"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.5 오차의 크기를 측정하기 위한 방법 - 단계 (1) : 모든 오차 더하기\n",
    "\n",
    "- 오차의 크기를 측정하기 위한 가장 기본적인 방법  \n",
    "$\\rightarrow$ 각 오차를 모두 더하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e87DHJBftB8a"
   },
   "source": [
    "- 위의 $y=13x+1$ 직선이 예측한 예측값을 각각 실제값으로부터 오차를 계산하여 표를 만들어보면 아래와 같다.\n",
    "\n",
    "| hours($x$) | 2    | 3    | 4    | 5    |\n",
    "| :--------- | :--- | :--- | :--- | :--- |\n",
    "| 실제값     | 25   | 50   | 42   | 61   |\n",
    "| 예측값     | 27   | 40   | 53   | 66   |\n",
    "| 오차       | -2   | 10   | -7   | -5   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ck7SxO81tLf4"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.6 오차의 크기를 측정하기 위한 방법 - 단계 (2) : 모든 오차의 제곱 더하기\n",
    "\n",
    "- 수식적으로 단순히 **'오차 = 실제값 - 예측값'** 이라고 정의한 후에 모든 오차를 더하면 음수 오차도 있고, 양수 오차도 있으므로 오차의 절대적인 크기를 구할 수가 없다.\n",
    "- 그래서 **모든 오차를 제곱하여 더하는 방법**을 사용한다.\n",
    "- 다시 말해 위의 그림에서의 모든 점과 직선 사이의 ↕ 거리를 제곱하고 모두 더한다.\n",
    "- 이를 수식으로 표현하면 아래와 같다. (여기서 $n$은 갖고 있는 데이터의 개수를 의미)\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^n \\left[ y^{(i)} - H \\left( x^{(i)} \\right) \\right]^2 \n",
    "&= \\left( -2 \\right)^2 + 10^2 + \\left( -7 \\right)^2 + \\left( -5 \\right)^2 \\\\\n",
    "&= 178\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G2U-2j5TuaBT"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.7 오차의 크기를 측정하기 위한 방법 - 단계 (3) : 오차의 제곱합의 평균\n",
    "\n",
    "- 이 때 데이터의 개수인 $n$으로 나누면, 오차의 제곱합에 대한 평균을 구할 수 있다.\n",
    "- 이를 **평균 제곱 오차(Mean Squared Error, MSE)**라고 한다.\n",
    "- 수식은 아래와 같다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "\\begin{align*}\n",
    "{1 \\over n} \\sum_{i=1}^n \\left[ y^{(i)} - H \\left( x^{(i)} \\right) \\right]^2\n",
    "&= 178 / 4 \\\\\n",
    "&= 44.5\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "- $y=13x+1$의 예측값과 실제값의 평균 제곱 오차의 값은 44.5이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LqwRCDvUvEmX"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.8 비용 함수(Cost function) 재정의\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "cost(W, b) = {1 \\over n} \\sum_{i=1}^n \\left[ y^{(i)} - H \\left( x^{(i)} \\right) \\right]^2\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vClir9v3vSMW"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.9 평균 제곱 오차와 $W$와 $b$의 관계\n",
    "\n",
    "- 모든 점들과의 오차가 클 수록 평균 제곱 오차는 커지며, 오차가 작아질 수록 평균 제곱 오차는 작아진다.\n",
    "- 그러므로 이 평균 최곱 오차. 즉, $Cost(W,b)$를 최소가 되게 만드는 $W$와 $b$를 구하면 결과적으로 $y$와 $x$의 관계를 가장 잘 나타내는 직선을 그릴 수 있다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "W, b \\rightarrow minimize \\; cost(W, b)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wtNhpUDrvnyI"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 3.4 옵티마이저(Optimizer) : 경사하강법(Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MBUAWcG9vsVI"
   },
   "source": [
    "### 3.4.1 옵티마이저(Optimizer) : 최적화 알고리즘\n",
    "\n",
    "- 선형 회귀를 포함한 수많은 머신 러닝, 딥 러닝의 학습은 결국 비용 함수를 최소화하는 매개 변수인 $W$와 $b$을 찾기 위한 작업을 수행한다.\n",
    "- 이때 사용되는 알고리즘을 **옵티마이저(Optimizer)** 또는 **최적화 알고리즘**이라고 부릅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RNep75lYzRNB"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.2 학습(Training)\n",
    "\n",
    "- 이 옵티마이저를 통해 적절한 $W$와 $b$를 찾아내는 과정을 머신 러닝에서 **학습(training)**이라고 부른다.\n",
    "- 여기서는 가장 기본적인 옵티마이저 알고리즘인 경사 하강법(Gradient Descent)에 대해서 배운다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XKrx5xtnzdwQ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.3 cost와 기울기 $W$와의 관계\n",
    "\n",
    "- 경사 하강법을 이해하기 위해서 cost와 기울기 W와의 관계를 이해해본다.\n",
    "- $W$는 머신 러닝 용어로는 가중치라고 불리지만, 직선의 방정식 관점에서 보면 직선의 기울기를 의미한다.\n",
    "- 아래의 그래프는 기울기 $W$가 지나치게 높거나, 낮을 때 어떻게 오차가 커지는 보여준다.  \n",
    "<img src=\"https://wikidocs.net/images/page/53560/%EA%B7%B8%EB%A6%BC4.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kLzItP9n0WTj"
   },
   "source": [
    "- ↕ : 각 점에서의 실제값과 두 직선의 예측값과의 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EndQyefOzw9n"
   },
   "source": [
    "- <font color=\"orange\">주황색</font>선\n",
    "  - $y = 20x$에 해당되는 직선\n",
    "  - 기울기 $W$가 20\n",
    "  - 기울기가 지나치게 크면 실제 값과 예측값의 오차가 커진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VOd5wJ6B0IJV"
   },
   "source": [
    "- <font color=\"green\">초록색</font>선\n",
    "  - $y = x$에 해당되는 직선\n",
    "  - 기울기 $W$가 1\n",
    "  - 기울기가 지나치게 작아도 실제 값과 예측값의 오차가 커진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DG418uGo0Mz6"
   },
   "source": [
    "- 사실 $b$ 또한 마찬가지로 지나치게 크거나 작으면 오차가 커진다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fDry2lM00nGk"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.4 $W$와 cost의 관계 그래프\n",
    "\n",
    "- 설명의 편의를 위해 편향 $b$가 없이 단순히 가중치 $W$만을 사용한 $y=Wx$라는 가설 $H(x)$를 가지고, 경사 하강법을 수행한다.\n",
    "- 비용 함수의 값 $cost(W)$는 cost라고 줄여서 표현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "krbaUq_c06Eb"
   },
   "source": [
    "- 이에 따라 $W$와 cost의 관계를 그래프로 표현하면 다음과 같다.  \n",
    "<img src=\"https://wikidocs.net/images/page/21670/%EA%B8%B0%EC%9A%B8%EA%B8%B0%EC%99%80%EC%BD%94%EC%8A%A4%ED%8A%B8.PNG\" />\n",
    "- 기울기 $W$가 무한대로 커지면 커질 수록 cost의 값 또한 무한대로 커진다.\n",
    "- 반대로 기울기 $W$가 무한대로 작아져도 cost의 값은 무한대로 커진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-iYEO5r11laS"
   },
   "source": [
    "### 3.4.5 cost가 가장 작을 때의 $W$ 값\n",
    "\n",
    "- 위의 그래프에서 cost가 가장 작을 때는 볼록한 부분의 맨 아래 부분이다.\n",
    "- 기계가 해야할 일은 cost가 가장 최소값을 가지게 하는 $W$를 찾는 일이다.\n",
    "- 그러므로 볼록한 부분의 맨 아래 부분의 $W$의 값을 찾아야 한다.  \n",
    "<img src=\"https://wikidocs.net/images/page/21670/%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7paTlVDy1f7e"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.6 $W$값이 수정되는 과정\n",
    "\n",
    "- 기계는 임의의 랜덤값 $W$값을 정한 뒤에, 맨 아래의 볼록한 부분을 향해 점차 $W$의 값을 수정해나간다.\n",
    "- 위의 그림은 $W$값이 점차 수정되는 과정을 보여준다.\n",
    "- 그리고 이를 가능하게 하는 것이 경사 하강법(Gradient Descent)이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mUcS1jwv19zw"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.7 접선에서의 기울기의 개념\n",
    "\n",
    "- 경사 하강법을 이해하기 위해서는 미분을 이해해야 한다.\n",
    "- 경사 하강법은 한 점에서의 순간 변화율 또는 접선에서의 기울기의 개념을 사용한다.  \n",
    "<img src=\"https://wikidocs.net/images/page/21670/%EC%A0%91%EC%84%A0%EC%9D%98%EA%B8%B0%EC%9A%B8%EA%B8%B01.PNG\" />\n",
    "- 위의 그림에서 초록색 선은 $W$가 임의의 값을 가지게 되는 네 가지의 경우에 대해서, 그래프 상으로 접선의 기울기를 보여준다.\n",
    "- 맨 아래의 볼록한 부분으로 갈수록 접선의 기울기가 점차 작아진다.\n",
    "- 그리고 맨 아래의 볼록한 부분에서는 결국 접선의 기울기가 0이 된다.\n",
    "- 그래프 상으로는 초록색 화살표가 수평이 되는 지점이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_QSWwcQZ2dEK"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.8 경사 하강법의 아이디어\n",
    "\n",
    "- cost가 최소화가 되는 지점은 접선의 기울기가 0이 되는 지점이며, 또한 미분값이 0이 되는 지점이다.\n",
    "- 경사 하강법의 아이디어는 비용 함수(Cost function)를 미분하여 현재 W에서의 접선의 기울기를 구하고, 접선의 기울기가 낮은 방향으로 $W$의 값을 변경하고 다시 미분하고 이 과정을 접선의 기울기가 0인 곳을 향해 $W$의 값을 변경하는 작업을 반복하는 것에 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D9dCLYcv2reP"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.9 $W$를 업데이트 하는 식\n",
    "\n",
    "- 비용 함수(Cost function)는 아래와 같다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "cost(W, b) = {1 \\over n} \\sum_{i=1}^n \\left[ y^{(i)} - H \\left( x^{(i)} \\right) \\right]^2\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TI4X9NXl211n"
   },
   "source": [
    "- 비용(cost)를 최소화하는 $W$를 구하기 위해 $W$를 업데이트하는 식은 다음과 같다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "W := W - \\alpha  {\\partial \\over \\partial W} cost(W)\n",
    "$\n",
    "\n",
    "- 위의 식은 현재 $W$에서의 접선의 기울기(${\\partial \\over \\partial W} cost(W)$)와 $\\alpha$와 곱한 값을 현재 $W$에서 뺴서 새로운 $W$의 값으로 한다는 것을 의미한다.\n",
    "- $\\alpha$는 여기서 **학습률(learning rate)**이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xYLvwBch4URP"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.10 현재 $W$에서 현재 $W$에서의 접선의 기울기를 빼는 행위의 의미\n",
    "\n",
    "- 우선 $\\alpha$는 생각하지 않고 현재 $W$에서 현재 $W$에서의 접선의 기울기를 빼는 행위가 어떤 의미가 있는 지 알아보자.  \n",
    "<img src=\"https://wikidocs.net/images/page/21670/%EB%AF%B8%EB%B6%84.PNG\" />\n",
    "- 위 그림은 접선의 기울기가 음수일 때, 0일 때, 양수일 때의 경우를 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R2BtruI54wYE"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.10.1 접선의 기울기가 음수일 때\n",
    "\n",
    "- 접선의 기울기가 음수일 때는 위의 수식이 아래와 같이 표현할 수 있다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "W := W - \\alpha \\left(음수기울기\\right) = W + \\alpha \\left(양수기울기\\right)\n",
    "$\n",
    "\n",
    "- 즉, 기울기가 음수면 $W$의 값이 **증가**하게 된다.\n",
    "- 이는 결과적으로 접선의 기울기가 0인 방향으로 $W$의 값이 조정된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lftZY2GW5V-Z"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.10.2 접선의 기울기가 양수일 때\n",
    "\n",
    "- 접선의 기울기가 양수일 때는 위의 수식이 아래와 같이 표현될 수 있다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "W := W - \\alpha \\left(양수기울기\\right)\n",
    "$\n",
    "\n",
    "- 기울기가 양수면 $W$의 값이 **감소**한다.\n",
    "- 이는 결과적으로 기울기가 0인 방향으로 $W$의 값이 조정된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F7gDkdp85taz"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.10.3 종합\n",
    "\n",
    "- 결국, 아래의 수식은 접선의 기울기가 음수이거나, 양수일 때 모두 접선의 기울기가 0인 방향으로 $W$의 값을 조정한다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "W := W - \\alpha  {\\partial \\over \\partial W} cost(W)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pf2fA7xn559z"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.11 학습률(learning rate)\n",
    "\n",
    "- 학습률(learning rate)이라고 말하는 $\\alpha$는 어떤 의미를 가질까?\n",
    "- 학습률 $\\alpha$는 $W$의 값을 변경할 때, 얼마나 크게 변경할 지를 결정한다.\n",
    "- 또는 $W$를 그래프의 한 점으로보고 접선의 기울기가 0일 때까지 경사를 따라 내려간다는 관점에서는 얼마나 큰 폭으로 이동할지를 결정한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dE5VdZvo6NBh"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.11.1 학습률이 지나치게 큰 경우\n",
    "\n",
    "<img src=\"https://wikidocs.net/images/page/21670/%EA%B8%B0%EC%9A%B8%EA%B8%B0%EB%B0%9C%EC%82%B0.PNG\" />\n",
    "\n",
    "- 위의 그림은 학습률 $\\alpha$가 지나치게 높은 값을 가질 때, 접선의 기울기가 0이 되는 $W$를 찾아가는 것이 아니라 $W$의 값이 발산하는 상황을 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s3h-usRR6miZ"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.11.2 학습률이 지나치게 작을 경우\n",
    "\n",
    "- 반대로 학습률 α가 지나치게 낮은 값을 가지면 학습 속도가 느려지므로 적당한 α의 값을 찾아내는 것도 중요합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "88dZGL2Z6tQg"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.11.3 실제 경사 하강법\n",
    "\n",
    "- 지금까지는 $b$는 배제시키고 최적의 $W$를 찾아내는 것에만 초점을 맞추어 경사 하강법의 원리에 대해서 배웠다.\n",
    "- 하지만 실제 경사 하강법은 $W$와 $b$에 대해서 동시에 경사 하강법을 수행하면서 최적의 $W$와 $b$의 값을 찾아간다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oFIPR1L8661f"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.12 정리\n",
    "\n",
    "- 가설, 비용 함수, 옵티마이저는 머신 러닝 분야에서 사용되는 포괄적 개념이다.\n",
    "- 풀고자하는 각 문제에 따라 가설, 비용 함수, 옵티마이저는 전부 다를 수 있다.\n",
    "- 선형 회귀에 가장 적합한 비용 함수와 옵티마이저는 각각 MSE와 경사 하강법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bnF26e4B7HBP"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 3.5 케라스로 구현하는 선형 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kQD9_qy27Kr_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch07_v03_Linear-Regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
