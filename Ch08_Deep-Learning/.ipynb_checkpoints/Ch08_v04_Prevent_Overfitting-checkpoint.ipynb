{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NoqKB3XwQEXO"
   },
   "source": [
    "# Ch08. 딥 러닝(Deep Learning) 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "boVahz-BQOW2"
   },
   "source": [
    "# v04. 과적합(Overfitting)을 막는 방법들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uz4UWWxPQSU2"
   },
   "source": [
    "- 학습 데이터에 모델이 과적합되는 현상은 모델의 성능을 떨어트리는 주요 이슈이다.\n",
    "- 모델이 과적합되면 훈련 데이터에 대한 정확도는 높을지라도, 새로운 데이터. 즉, 검증 데이터나 테스트 데이터에 대해서는 제대로 동작하지 않는다.\n",
    "- 이는 모델이 학습 데이터를 불필요할정도로 과하게 암기하여 훈련 데이터에 포함된 노이즈까지 학습한 상태라고 해석할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kP3yBHvTREw_"
   },
   "source": [
    "- 이번 챕터에서는 인공 신경망 모델의 과적합을 막는 방법에 대해 알아본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6elkguTCRJAW"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 4.1 데이터의 양을 늘리기\n",
    "\n",
    "- 모델은 데이터의 양이 적을 경우, 해당 데이터의 특정 패턴이나 노이즈까지 쉽게 암기하므로 과적합 현상이 발생할 확률이 늘어난다.\n",
    "- 그렇기 때문에 데이터의 양을 늘릴 수록 모델은 데이터의 일반적인 패턴을 학습하여 과적합을 방지할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q2khBJFsRk13"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 4.1.1 데이터 증식 or 데이터 증강 (Data Augmentation)\n",
    "\n",
    "- 만약 데이터의 양이 적을 경우에는 의도적으로 기존의 데이터를 조금씩 변형하고 추가하여 데이터의 양을 늘리기도 한다.\n",
    "- 이를 **데이터 증식** 또는 **데이터 증강(Data Augmentation)**이라고 한다.\n",
    "- 이미지의 경우에는 데이터 증식이 많이 사용된다.\n",
    "  - 이미지를 돌리거나 노이즈를 추가, 일부분을 수정하는 등으로 데이터를 증식시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nnOvNH4WR1bn"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 4.2 모델의 복잡도 줄이기\n",
    "\n",
    "- 인공 신경망의 복잡도는 **은닉층(hidden layer)의 수**나 **매개 변수의 수** 등으로 결정된다.\n",
    "- 과적합 현상이 포착되었을 때, 인공 신경망 모델에 대해서 할 수 있는 한 가지 조치는 인공 신경망의 복잡도를 줄이는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KRdqbJedSFg6"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 4.2.1 모델의 수용력(capacity)\n",
    "\n",
    "- 인공 신경망에서는 모델에 있는 **매개 변수들의 수**를 모델의 **수용력(capacity)**이라고 하기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c_RZUecQSTp5"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 4.3 가중치 규제(Regularization) 적용하기\n",
    "\n",
    "- 복잡한 모델이 간단한 모델보다 과적합될 가능성이 높다.\n",
    "- 그리고 간단한 모델은 적은 수의 매개 변수를 가진 모델을 말한다.\n",
    "- 복잡한 모델은 좀 더 간단하게 하는 방법으로 가중치 규제(Regularization)가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kztTnYPmSWtb"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 4.3.1 가중치 규제의 종류\n",
    "\n",
    "- L1 규제\n",
    "  - 가중치 w들의 **절대값 합계**를 비용 함수에 추가한다.\n",
    "  - L1 노름이라고도 한다.\n",
    "  - L1 규제는 기존의 비용 함수에 모든 가중치에 대해서 $\\lambda  |w|$를 더 한 값을 비용 함수로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LnUiCnJPSvlO"
   },
   "source": [
    "- L2 규제\n",
    "  - 모든 가중치 w들의 **제곱합**을 비용 함수에 추가한다.\n",
    "  - L2 노름이라고도 한다.\n",
    "  - L2 규제는 기존의 비용 함수에 모든 가중치에 대해서 ${1 \\over 2} \\lambda w^2$를 더 한 값을 비용 함수로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rs73C3OVTafh"
   },
   "source": [
    "- $\\lambda$\n",
    "  - 규제의 강도를 정하는 하이퍼파라미터\n",
    "  - $\\lambda$ 가 크다  \n",
    "  $\\rightarrow$ 모델이 훈련 데이터에 대해서 적합한 매개 변수를 찾는 것보다 규제를 위해 추가된 항들을 작게 유지하는 것을 우선한다는 의미가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dYLuEz_dS1DW"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 4.3.2 가중치 규제의 특징\n",
    "\n",
    "- 이 두 식 모두 비용 함수를 최소화하기 위해서는 가중치 w들의 값들이 작아져야 한다는 특징이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3y8qdDh3T6xZ"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.3.2.1 L1 규제의 가중치값 최소화\n",
    "\n",
    "- L1 규제를 사용하면 비용 함수가 최소가 되게 하는 가중치와 편향을 찾는 동시에 가중치들의 절대값의 합도 최소가 되어야 한다.\n",
    "- 이렇게 되면, 가중치 w의 값들은 0 또는 0에 가까이 작아져야 하므로 어떤 특성들은 모델을 만들 때 거의 사용되지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ynBYT7gFUMoI"
   },
   "source": [
    "- ex) $H(x) = w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_5$\n",
    "  - 위 수식에 L2 규제를 적용했더니, $w_3$의 값이 0이 되었다고 하자.\n",
    "  - 이는 $x_3$ 특성은 사실 모델의 결과에 별 영향을 주지 못하는 특성임을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K-v8HsSTUtju"
   },
   "source": [
    "- L1 규제는 어떤 특성들이 모델에 영향을 주고 있는 지를 정확히 판단하고자 할 때 유용하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uFjXGrrTUb62"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.3.2.2 L2 규제의 가중치값 최소화\n",
    "\n",
    "- L2 규제는 L1 규제와는 달리 가중치들의 제곱을 최소화한다.\n",
    "- 그러므로 w의 값이 완전히 0이 되기보다는 0에 가까워지는 경향을 띈다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P6AyL022UhdB"
   },
   "source": [
    "- 경험적으로는 L1 규제보다 L2 규제가 더 잘 동작하므로 L2 규제를 더 권장한다.\n",
    "- 인공 신경망에서 L2 규제는 **가중치 감쇠(weight decay)**라고도 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pRUEbXqeVDI_"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 4.3.3 Regularization vs Normalization\n",
    "\n",
    "- 책에 따라서는 Regularization를 정규화로 번혁하기도 한다.\n",
    "- 하지만, 이는 정규화(Normalization)와 혼동될 수 있다.\n",
    "- 그러므로 Regularization은 **규제** 또는 **정형화**라는 번역이 바람직할 수 있다.\n",
    "- 정규화에 대한 설명은 [링크](http://blog.naver.com/angryking/221330145300)를 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "trKfiXPjVbPc"
   },
   "source": [
    "- 인공 신경망에서 정규화(Normalization)라는 용어가 쓰이는 기법\n",
    "  - 배치 정규화\n",
    "  - 층 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2VZNAbf0Vmaa"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 4.4 드롭아웃(Dropout)\n",
    "\n",
    "- 드롭아웃은 학습 과정에서 신경망의 일부를 사용하지 않는 방법이다.\n",
    "- ex) 드롭아웃의 비율을 0.5로 설정\n",
    "  - 학습 과정마다 랜덤으로 절반의 뉴런을 사용하지 않고, 절반의 뉴런만을 사용한다.\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/60751/%EB%93%9C%EB%A1%AD%EC%95%84%EC%9B%83.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFIIT3R8V5xx"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 4.4.1 드롭아웃의 사용\n",
    "\n",
    "- 드롭아웃은 신경망 학습 시에만 사용하고, 예측 시에는 사용하지 않는 것이 일반적이다.\n",
    "- 학습 시에 인공 신경망이 특정 뉴런 또는 특정 조합에 너무 의존적이게 되는 것을 방지한다.\n",
    "- 또한 매번 랜덤 선택으로 뉴런들을 사용하지 않으므로 서로 다른 신경망들을 앙상블하여 사용하는 것 같은 효과를 내어 과적합을 방지한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yiejQITVWRDn"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 4.4.2 케라스에서의 드롭아웃 사용\n",
    "\n",
    "- 케라스에서는 다음과 같은 방법으로 드롭아웃을 모델에 추가할 수 있다.\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(max_words, ), activation='relu'))\n",
    "model.add(Dropout(0.5)) # 드롭아웃 추가. 비율은 50%\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5)) # 드롭아웃 추가. 비율은 50%\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch08_v04_Prevent-Overfitting.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
