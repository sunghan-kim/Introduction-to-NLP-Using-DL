{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OSIsEdUhU3iZ"
   },
   "source": [
    "# Ch08. 딥 러닝(Deep Learning) 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yT-ZpwrBVFVK"
   },
   "source": [
    "# v05. 기울기 소실(Gradient Vanishing)과 폭주(Exploding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g3sBgfjzVLNv"
   },
   "source": [
    "**기울기 소실(Gradient Vanishing)**\n",
    "\n",
    "- 깊은 인공 신경망을 학습하다 보면 역전파 과정에서 입력층으로 갈수록 기울기(Gradient)가 점차적으로 작아지는 현상이 발생할 수 있다.\n",
    "- 입력층에 가까운 층들에서 가중치들이 업데이트가 제대로 되지 않으면 결국 최적의 모델을 찾을 수 없게 된다.\n",
    "- 이를 **기울기 소실(Gradient Vanishing)**이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OkTiVrMbVkpn"
   },
   "source": [
    "**기울기 폭주(Gradient Exploding)**\n",
    "\n",
    "- 반대의 경우도 있다.\n",
    "- 기울기가 점차 커지더니 가중치들이 비정상적으로 큰 값이 되면서 결국 발산되기도 한다.\n",
    "- 이를 **기울기 폭주(Gradient Exploding)**라고 한다.\n",
    "- 순환 신경망(Recurrent Neural Network, RNN)에서 발생할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqv0OkgTV0AX"
   },
   "source": [
    "- 이번 챕터에서는 기울기 소실 또는 기울기 폭주를 막는 방법들에 대해서 다룬다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5pyShdpnV3hv"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 5.1 ReLU와 ReLU의 변형들\n",
    "\n",
    "- 시그모이드 함수를 사용하면 입력의 절대값이 클 경우에 시그모이드 함수의 출력값이 0 또는 1에 수렴하면서 기울기가 0에 가까워진다.\n",
    "- 그래서 역전파 과정에서 전파 시킬 기울기가 점차 사라져서 입력층 방향으로 갈수록 제대로 역전파가 되지 않는 기울기 소실 문제가 발생할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "82OPcqGEWJfv"
   },
   "source": [
    "- 기울기 소실을 완화하는 가장 간단한 방법은 은닉층의 활성화 함수로 시그모이드나 하이퍼볼릭탄젠트 함수 대신에 ReLU나 ReLU의 변형 함수와 같은 Leaky ReLU를 사용하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kepzbsTGWWIC"
   },
   "source": [
    "**정리**\n",
    "\n",
    "- 은닉층에서는 시그모이드 함수를 사용하지 않는다.\n",
    "- Leaky ReLU를 사용하면 모든 입력값에 대해서 기울기가 0에 수렴하지 않아 죽은 ReLU 문제를 해결한다.\n",
    "- 은닉층에서는 ReLU나 Leaky ReLU와 같은 ReLU 함수의 변형들을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QE2yb8dYWnhH"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 5.2 그래디언트 클리핑(Gradient Clipping)\n",
    "\n",
    "- 그래디언트 클리핑은 말 그대로 기울기 값을 자르는 것을 말한다.\n",
    "- 기울기 폭주를 막기 위해 임계값을 넘지 않도록 값을 자른다.\n",
    "- 다시 말해서 임계치 만큼 크기를 감소시킨다.\n",
    "- 이는 RNN에서 유용하다.\n",
    "  - RNN은 BPTT에서 시점을 역행하면서 기울기를 구한다.\n",
    "  - 이 때 기울기가 너무 커질 수 있기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "687YI0ILXAlH"
   },
   "source": [
    "- 케라스에서는 다음과 같은 방법으로 그래디언트 클리핑을 수행한다.\n",
    "\n",
    "```\n",
    "from tensorflow.keras import optimizers\n",
    "Adam = optimizers.Adam(lr=0.0001, clipnorm=1.)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KdELSd3bXMhu"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 5.3 가중치 초기화(Weight Initialization)\n",
    "\n",
    "- 같은 모델을 훈련시키더라도 가중치가 초기에 어떤 값을 가졌느냐에 따라서 모델의 훈련 결과가 달라지기도 한다.\n",
    "- 다시 말해 가중치 초기화만 적절히 해줘도 기울기 소실 문제와 같은 문제를 완화시킬 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jdAIutZZXdfo"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 5.3.1 세이비어 초기화 (Xavier Initialization)\n",
    "\n",
    "- [논문 링크](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lXJJvWmJXlK2"
   },
   "source": [
    "- 2010년 세이비어 그로럿과 요슈아 벤지오는 가중치 초기화가 모델에 미치는 영향을 분석하여 새로운 초기화 방법을 제안했다.\n",
    "- 이 초기화 방법은 제안한 사람의 이름을 따서 **셰이비어(Xavier Initialization) 초기화** 또는 글로럿 초기화(Glorot Initialization)라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ESKx7gK9YDXP"
   },
   "source": [
    "- 이 방법은 균등 분포(Uniform Distribution) 또는 정규 분포(Normal Distribution)로 초기화할 때 두 가지 경우로 나뉜다.\n",
    "- 이전 층의 뉴런 개수와 다음 층의 뉴런 개수를 가지고 식을 세운다.\n",
    "  - $n_{in}$ : 이전 층의 뉴런의 개수\n",
    "  - $n_{out}$ : 다음 층의 뉴런의 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5T9SKc9RYgwR"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 5.3.1.1 균등 분포를 사용할 경우\n",
    "\n",
    "- 글로럿과 벤지오의 논문에서는 균등 분포를 사용하여 가중치를 초기화할 경우 다음과 같은 균등 분포 범위를 사용하라고 한다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "W \\sim Uniform \\left( - \\sqrt{\\frac{6}{n_{in} + n_{out}}},  + \\sqrt{\\frac{6}{n_{in} + n_{out}}} \\right)\n",
    "$\n",
    "\n",
    "- 다시 말해, $m = \\sqrt{\\frac{6}{n_{in} + n_{out}}}$ 일 때, $-m$과 $+m$ 사이의 균등 분포를 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bdC9639sZKSV"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 5.3.1.2 정규 분포를 사용할 경우\n",
    "\n",
    "- 정규 분포로 초기화할 경우에는 평균이 0이고, 표준 편차 $\\sigma$가 다음을 만족하도록 한다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "\\sigma = \\sqrt{\\frac{2}{n_{in} + n_{out}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qaug7yPLZY_f"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 5.3.1.3 세이비어 초기화와 활성화 함수\n",
    "\n",
    "- 세이비어 초기화는 여러 층의 기울기 분산 사이에 균형을 맞춰서 특정 층이 너무 주목받거나 다른 층이 뒤쳐지는 것을 막는다.\n",
    "- 세이비어 초기화는 시그모이드 함수나 하이퍼볼릭 탄젠트 함수와 같은 S자 형태인 활성화 함수와 함께 사용할 경우에는 좋은 성능을 보인다.\n",
    "- 하지만, ReLU와 함께 사용할 경우에는 성능이 좋지 않다.\n",
    "- ReLU 함수 또는 ReLU의 변형 함수들을 활성화 함수로 사용할 경우에는 다른 초기화 방법을 사용하는 것이 좋은데 이를 **He 초기화(He Initialization)**라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6AREQygzZ-Ax"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 5.3.2 He 초기화 (He Initialization)\n",
    "\n",
    "- [논문 링크](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XcquIUTXaG3P"
   },
   "source": [
    "- He 초기화(He initialization)는 세이비어 초기화와 유사하게 정규 분포와 균등 분포 두 가지 경우로 나뉜다.\n",
    "- 다만, He 초기화는 세이비어 초기화와 다르게 **다음 층의 뉴런의 수를 반영하지 않는다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqkfh2aAaa1e"
   },
   "source": [
    "- $n_{in}$ : 이전 층의 뉴런의 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FCcpCW06afCi"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 5.3.2.1 균등 분포를 사용할 경우\n",
    "\n",
    "- He 초기화는 균등 분포로 초기화할 경우에는 다음과 같은 균등 분포 범위를 가지도록 한다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "W \\sim Uniform \\left( - \\sqrt{\\frac{6}{n_{in}}},  + \\sqrt{\\frac{6}{n_{in}}} \\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eDqB_-5ZavM3"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 5.3.2.2 정규 분포를 사용할 경우\n",
    "\n",
    "- 정규 분포로 초기화할 경우에는 표준 편차 $\\sigma$가 다음을 만족하도록 한다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "\\sigma = \\sqrt{\\frac{2}{n_{in}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NBtL1rtja8-N"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 5.3.2.3 He 초기화와 활성화 함수\n",
    "\n",
    "- 시그모이드 함수나 하이퍼볼릭 탄젠트 함수를 사용할 경우에는 세이비어 초기화 방법이 효율적이다.\n",
    "- ReLU 계열 함수를 사용할 경우에는 He 초기화 방법이 효율적이다.\n",
    "- ReLU + He 초기화 방법이 좀 더 보편적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dmkYUscsbCP4"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 5.4 배치 정규화 (Batch Normalization)\n",
    "\n",
    "- 기울기 소실이나 폭주를 예방하는 또 다른 방법은 **배치 정규화(Batch Normalization)**이다.\n",
    "- 배치 정규화는 인공 신경망의 각 층에 들어가는 입력을 평균과 분산으로 정규화하여 학습을 효율적으로 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_08RmTk0bWv0"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 5.4.1 내부 공변량 변화 (Internal Covariate Shift)\n",
    "\n",
    "- 배치 정규화를 이해하기 위해서는 **내부 공변량 변화(Internal Covariate Shift)**를 이해할 필요가 있다.\n",
    "- 내부 공변량 변화란 학습 과정에서 **층별로 입력 데이터 분포가 달라지는 현상**을 말한다.\n",
    "- 이전 층들의 학습에 의해 이전 층의 가중치 값이 바뀐다.  \n",
    "$\\rightarrow$ 현재 층에 전달되는 입력 데이터의 분포가 현재 층이 학습했던 시점의 분포와 차이가 발생한다.\n",
    "- 배치 정규화를 제안한 논문에서는 기울기 소실/폭주 등의 딥러닝 모델의 불안정성이 층마다 입력의 분포가 달라지기 때문이라고 주장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FRqHcYUgZ_Ci"
   },
   "source": [
    "**내부 공변량 변화 요약**\n",
    "\n",
    "- 공변량 변화는 훈련 데이터의 분포와 테스트 데이터의 분포가 다른 경우를 의미한다.\n",
    "- 내부 공변량 변화는 신경망 층 사이에서 발생하는 입력 데이터의 분포 변화를 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z06ZhGMpUipl"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 5.4.2 배치 정규화 (Batch Normalization)\n",
    "\n",
    "- 배치 정규화(Batch Normalization)는 표현 그대로 한 번에 들어오는 배치 단위로 정규화하는 것을 말한다.\n",
    "- 배치 정규화는 **각 층에서 활성화 함수를 통과하기 전에 수행**된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "40r8H-rjUzW_"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 5.4.2.1 배치 정규화 요약\n",
    "\n",
    "- 입력에 대해 평균을 0으로 만들고, 정규화를 한다.\n",
    "- 그리고 정규화된 데이터에 대해서 스케일과 시프트를 수행한다.\n",
    "- 이 때 두 개의 매개 변수를 사용한다.\n",
    "  - $\\gamma$ : 스케일을 위해 사용되는 매개 변수\n",
    "  - $\\beta$ : 시프트를 위해 사용되는 매개 변수\n",
    "- 이 2개의 매개 변수를 이용하여 다음 레이어에 일정한 범위의 값들만 전달되게 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "731bX1iRVQR4"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 5.4.2.2 배치 정규화의 수식\n",
    "\n",
    "- 배치 정규화의 수식은 다음과 같다. ($BN$ : 배치 정규화를 의미)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OrcLJjgmVWuu"
   },
   "source": [
    "**Input**\n",
    "\n",
    "- $m$ : 미니 배치에 있는 샘플의 수\n",
    "\n",
    "$$\n",
    "\\text{미니 배치 } B = \\{ x^{(1)}, x^{(2)}, \\dots, x^{(m)} \\}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3xgat3ZSVuZC"
   },
   "source": [
    "<br>\n",
    "\n",
    "**Output**\n",
    "\n",
    "$$\n",
    "y^{(i)} = BN_{\\gamma, \\beta} (x^{(i)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0h9FUYIVx4h"
   },
   "source": [
    "<br>\n",
    "\n",
    "**미니 배치에 대한 평균 계산**\n",
    "\n",
    "- $\\mu_B$ : 미니 배치 $B$에 대한 평균\n",
    "\n",
    "$$\n",
    "\\mu_B \\; \\leftarrow \\; {1 \\over m} \\sum_{i=1}^m x^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5nUEYnelWHak"
   },
   "source": [
    "<br>\n",
    "\n",
    "**미니 배치에 대한 분산 계산**\n",
    "\n",
    "- $\\sigma_{B}$ : 미니 배치 $B$에 대한 표준편차\n",
    "\n",
    "$$\n",
    "\\sigma_{B}^2 \\; \\leftarrow \\; {1 \\over m} \\sum_{i=1}^m ( x^{(i)} - \\mu_B )^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fJ9oikS3W5zG"
   },
   "source": [
    "<br>\n",
    "\n",
    "**정규화**\n",
    "\n",
    "- $\\hat{x}^{(i)}$ : 평균이 0이고 정규화된 입력 데이터\n",
    "- $\\varepsilon$ : $\\sigma^2$가 0일 때, 분모가 0이 되는 것을 막는 작은 양수 (보편적으로 $10^{-5}$)\n",
    "\n",
    "$$\n",
    "\\hat{x}^{(i)} ← \\frac{x^{(i)} - μ_{B}}{\\sqrt{σ^{2}_{B}+ε}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e4i3--OiXGHM"
   },
   "source": [
    "<br>\n",
    "\n",
    "**스케일 조정($\\gamma$)과 시프트($\\beta$)를 통한 선형 연산**\n",
    "\n",
    "- $\\gamma$ : 정규화된 데이터에 대한 스케일 매개 변수 (학습 대상)\n",
    "- $\\beta$ : 정규화된 데이터에 대한 시프트 매개 변수 (학습 대상)\n",
    "- $y^{(i)}$ : 스케일과 스프트를 통해 조정한 $BN$의 최종 결과 \n",
    "\n",
    "$$\n",
    "y^{(i)} ← \\gamma \\, \\hat{x}^{(i)} + \\beta = BN_{γ, β} \\left(x^{(i)} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lHNq4aFtXcyq"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 5.4.2.3 배치 정규화의 평균과 분산 사용\n",
    "\n",
    "- 배치 정규화는 학습 시 배치 단위의 평균과 분산들을 차례대로 받아 이동 평균과 이동 분산을 저장해 놓는다.\n",
    "- 테스트 할 때는 해당 배치의 평균과 분산을 구하지 않고 구해놓았던 평균과 분산으로 정규화를 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZzJ93537ZoOl"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 5.4.2.4 배치 정규화의 장점\n",
    "\n",
    "- 배치 정규화를 사용하면 시그모이드 함수나 하이퍼볼릭 탄젠트 함수를 사용하더라도 기울기 소실 문제가 크게 개선된다.\n",
    "- 가중치 초기화에 훨씬 덜 민감해진다.\n",
    "- 훨씬 큰 학습률을 사용할 수 있어 학습 속도를 개선시킨다.\n",
    "- 미니 배치마다 평균과 표준편차를 계산하여 사용하므로 훈련 데이터에 일종의 잡음 주입의 부수 효과로 과적합을 방지하는 효과도 낸다. (드롭아웃과 비슷한 효과)\n",
    "- 드롭아웃과 함께 사용하는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VVXRKXdlaDj4"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 5.4.2.5 배치 정규화의 단점\n",
    "\n",
    "- 배치 정규화는 모델을 복잡하게 한다.\n",
    "- 추가 계산을 하는 것이므로 테스트 데이터에 대한 예측 시에 실행 시간이 느려진다.\n",
    "- 그래서 서비스 속도를 고려하는 관점에서는 배치 정규화가 꼭 필요한 지 고민이 필요하다.\n",
    "- 배치 정규화의 효과는 굉장하지만 내부 공변량 변화 때문은 아니라는 논문도 있다. ([관련 논문 링크](https://arxiv.org/pdf/1805.11604.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5iVSHm28ad0Y"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 5.4.3 배치 정규화의 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kx1VzyLpag1-"
   },
   "source": [
    "#### 5.4.3.1 미니 배치 크기에 의존적이다.\n",
    "\n",
    "- 배치 정규화는 너무 작은 배치 크기에서는 잘 동작하지 않을 수 있다.\n",
    "- 단적으로, 배치 크기를 1로 하게되면 분산은 0이 된다.\n",
    "- 작은 미니 배치에서는 배치 정규화의 효과가 극단적으로 작용되어 훈련에 악영향을 줄 수 있다.\n",
    "- 배치 정규화를 적용할 때는 작은 미니 배치보다는 크기가 어느 정도 되는 미니 배치에서 하는 것이 좋다.\n",
    "- 이처럼 배치 정규화는 배치 크기에 의존적인 면이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vLb0UQEJa4Yv"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 5.4.3.2 RNN에 적용하기 어렵다.\n",
    "\n",
    "- RNN은 각 시점(time step)마다 다른 통계치를 가진다.\n",
    "- 이는 RNN에 배치 정규화를 적용하는 것을 어렵게 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UDMBEkyebFxj"
   },
   "source": [
    "- RNN에서는 배치 크기에도 의존적이지 않으며, 적용도 수월한 **층 정규화(layer normalization)**라는 방법이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4FbBnMB8bOIy"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 5.5 층 정규화 (Layer Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1sWYmNPbSOT"
   },
   "source": [
    "### 5.5.1 배치 정규화 시각화\n",
    "\n",
    "- 층 정규화를 이해하기에 앞서 배치 정규화를 시각화해보자.\n",
    "- 다음은 $m$이 3이고, 특성의 수가 4일 때의 배치 정규화를 보여준다.  \n",
    "(미니 배치 : 동일한 특성(feature) 개수들을 가진 다수의 샘플들)\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/61375/%EB%B0%B0%EC%B9%98%EC%A0%95%EA%B7%9C%ED%99%94.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h8ch_7OLbmJ8"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 5.5.2 층 정규화 시각화\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/61375/%EC%B8%B5%EC%A0%95%EA%B7%9C%ED%99%94.PNG)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch08_v05_Gradient-Vanishing-Exploding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
