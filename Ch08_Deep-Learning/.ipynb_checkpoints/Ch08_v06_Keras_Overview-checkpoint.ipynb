{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vyZKFF4jdl0H"
   },
   "source": [
    "# Ch08. 딥 러닝(Deep Learning) 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "51F4dyGTduxY"
   },
   "source": [
    "# v06. 케라스(Keras) 훑어보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1OP4kMYndxof"
   },
   "source": [
    "- 이 책에서는 딥 러닝을 쉽게 할 수 있는 파이썬 라이브러리인 케라스(Keras)를 사용한다.\n",
    "- 케라스는 유저가 손쉽게 딥 러닝을 구현할 수 있도록 도와주는 상위 레벨의 인터페이스이다.\n",
    "- 케라스를 사용하면 딥 러닝을 쉽게 구현할 수 있다.\n",
    "- [케라스 공식 문서](https://keras.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N1mAj9g7eDU-"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 6.1 전처리 (Preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TrXApLVdeHTk"
   },
   "source": [
    "### 6.1.1 `Tokenizer()`\n",
    "\n",
    "- 토큰화와 정수 인코딩(단어에 대한 인덱싱)을 위해 사용된다.\n",
    "- [정수 인코딩 챕터](https://github.com/sunghan-kim/Introduction-to-NLP-Using-DL/blob/master/Ch02_Text-preprocessing/Ch02_v06_Integer_Encoding.ipynb) 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "XhH1BChweV8x",
    "outputId": "bc171572-cd1f-4bb4-bcde-fb4f8d504240"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "Zb3vnZN7eaV9",
    "outputId": "3ab3da78-b1c0-4af5-8b0f-2d5bad8fd370"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences :  [1, 2, 3, 4, 6, 7]\n",
      "word_index :  {'the': 1, 'earth': 2, 'is': 3, 'an': 4, 'awesome': 5, 'place': 6, 'live': 7}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "fit_text = \"The earth is an awesome place live\"\n",
    "\n",
    "t.fit_on_texts([fit_text])\n",
    "\n",
    "test_text = \"The earth is an great place live\"\n",
    "\n",
    "sequences = t.texts_to_sequences([test_text])[0]\n",
    "\n",
    "print(\"sequences : \" , sequences) # great는 단어 집합(vocabulary)에 없으므로 출력되지 않는다.\n",
    "print(\"word_index : \", t.word_index) # 단어 집합(vocabulary) 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kH61aYnRe-Q6"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 6.1.2 `pad_sequences()`\n",
    "\n",
    "- 전체 훈련 데이터에서 각 샘플의 길이는 서로 다를 수 있다.\n",
    "- 또는 각 문서, 각 문장은 단어의 수가 제각각이다.\n",
    "- 모델의 입력으로 사용하려면 모든 샘플의 길이를 동일하게 맞춰야 할 때가 있다.\n",
    "- 이를 자연어 처리에서는 **패딩(padding) 작업**이라고 한다.\n",
    "- 보통 숫자 0을 넣어서 길이가 다른 샘플들의 길이를 맞춰준다.\n",
    "- 케라스에서는 `pad_sequences()`를 사용한다.\n",
    "- `pad_sequences()`는 정해준 길이보다 길이가 긴 샘플은 값을 일부 자르고, 정해준 길이보다 길이가 짧은 샘플은 값을 0으로 채운다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "NIoMdCqcf7hr",
    "outputId": "13c758a4-b266-4010-ab5d-4cb660ad4d48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [0, 7, 8]], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 전처리가 끝나서 각 단어에 대한 정수 인코딩이 끝났다고 가정하고, 3개의 데이터를 입력으로 한다.\n",
    "pad_sequences([[1, 2, 3],\n",
    "               [3, 4, 5, 6],\n",
    "               [7, 8]],\n",
    "              maxlen=3,\n",
    "              padding='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVmX7QU0gSUO"
   },
   "source": [
    "- `첫 번째 인자` : 패딩을 진행할 데이터\n",
    "- `maxlen` : 모든 데이터에 대해서 정규화할 길이\n",
    "- `padding`\n",
    "  - `padding='pre'` : 앞에 0을 채움\n",
    "  - `padding='post'` : 뒤에 0을 채움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2u60KYdhgl9_"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 6.2 워드 임베딩 (Word Embedding)\n",
    "\n",
    "- 워드 임베딩이란 텍스트 내의 단어들을 밀집 벡터(dense vector)로 만드는 것을 말한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H9qIRrBxmVOK"
   },
   "source": [
    "### 6.2.1 희소 벡터 (sparse vector)\n",
    "\n",
    "- 원-핫 벡터는 대부분이 0의 값을 가지고, 단 하나의 1의 값을 가지는 벡터이다.\n",
    "- 또한 벡터의 차원이 대체적으로 크다는 성질을 가진다.\n",
    "- 원-핫 벡터의 예 : `[0 1 0 0 0 0 0 ... 0 0 0 0 0 0 0]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yEYsxLPzm4Ag"
   },
   "source": [
    "- 대부분의 값이 0인 이러한 벡터를 **희소 벡터(sparse vector)**라고 한다.\n",
    "- 원-핫 벡터는 희소 벡터의 예이다.\n",
    "- 원-핫 벡터는 단어의 수 만큼 벡터의 차원을 가지며 단어 간 유사도가 모두 동일하다는 단점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RMDsyPbcnE3_"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 6.2.2 밀집 벡터 (dense vector)\n",
    "\n",
    "- 반면, 희소 벡터와 표기상으로도 의미상으로도 반대인 벡터가 있다.\n",
    "- 대부분의 값이 실수이고, 상대적으로 저차원인 **밀집 벡터(dense vector)**이다.\n",
    "- 밀집 벡터의 예 : `[0.1 -1.2 0.8 0.2 1.8]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Up-qPQoanZqX"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 6.2.3 희소 벡터 vs 밀집 벡터\n",
    "\n",
    "| -         | 원-핫 벡터               | 임베딩 벡터              |\n",
    "| :-------- | :----------------------- | :----------------------- |\n",
    "| 차원      | 고차원(단어 집합의 크기) | 저차원                   |\n",
    "| 다른 표현 | 희소 벡터의 일종         | 밀집 벡터의 일종         |\n",
    "| 표현 방법 | 수동                     | 훈련 데이터로부터 학습함 |\n",
    "| 값의 타입 | 1과 0                    | 실수                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sh37n1ALnsSl"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 6.2.4 단어를 벡터로 만드는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P1EFoanIoeMO"
   },
   "source": [
    "#### 6.2.4.1 원-핫 인코딩\n",
    "\n",
    "- 단어를 원-핫 벡터로 만드는 과정을 원-핫 인코딩이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBPI351bnyh_"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 6.2.4.2 워드 임베딩 (word embedding)\n",
    "\n",
    "- 단어를 밀집 벡터로 만드는 작업을 **워드 임베딩(word embedding)**이라고 한다.\n",
    "- 밀집 벡터\n",
    "  - 워드 임베딩 과정을 통해 나온 결과\n",
    "  - 임베딩 벡터(embedding vector)라고도 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L640Yylyoh9f"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 6.2.5 벡터의 차원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RmrW_9HSoE_b"
   },
   "source": [
    "#### 6.2.5.1 원-핫 벡터의 차원\n",
    "\n",
    "- 원-핫 벡터의 차원은 주로 20,000 이상을 넘어간다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dKB9pBjfoZOf"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 6.2.5.2 임베딩 벡터의 차원\n",
    "\n",
    "- 임베딩 벡터는 주로 256, 512, 1024 등의 차원을 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "llBl6pVuovb1"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 6.2.6 임베딩 벡터의 값 결정\n",
    "\n",
    "- 임베딩 벡터는 초기에는 랜덤값을 가진다.\n",
    "- 인공 신경망의 가중치가 학습되는 방법과 같은 방식으로 값이 학습되며 변경된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y7q3QGDko4Lw"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 6.2.7 `Embedding()`\n",
    "\n",
    "- `Embedding()`은 단어를 밀집 벡터로 만드는 역할을 한다.\n",
    "- 인공 신경망 용어로 **임베딩 층(embedding layer)**을 만드는 역할을 한다.\n",
    "- `Embedding()`은 정수 인코딩이 된 단어들을 입력으로 받아서 임베딩을 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8AnUQyEusAfx"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 6.2.7.1 `Embedding()`의 입력값\n",
    "\n",
    "- `Embedding()`은 `(number of samples, input_length)`인 2D 정수 텐서를 입력받는다.\n",
    "- 이 때 각 `sample`은 정수 인코딩된 결과이다. (정수 시퀀스)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jJxjPFoqsXNR"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 6.2.7.2 `Embedding()`의 출력값\n",
    "\n",
    "- `Embedding()`은 워드 임베딩 작업을 수행하고 `(number of samples, input_length, embedding word dimentionality)`인 3D 텐서를 리턴한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mbr03TnCsjsS"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 6.2.7.3 임베딩 의사 코드(pseudo-code)\n",
    "\n",
    "```python\n",
    "# 문장 토큰화와 단어 토큰화\n",
    "text = [['Hope', 'to', 'see', 'you', 'soon'],\n",
    "        ['Nice', 'to', 'see', 'you', 'again']]\n",
    "\n",
    "# 각 단어에 대한 정수 인코딩\n",
    "text = [[0, 1, 2, 3, 4],\n",
    "        [5, 1, 2, 3, 6]]\n",
    "\n",
    "# 위 데이터가 아래의 임베딩 층의 입력이 된다.\n",
    "Embedding(7, 2, input_length=5)\n",
    "# 7 : 단어의 개수 (즉, 단어 집합(vocabulary)의 크기)\n",
    "# 2 : 임베딩한 후의 벡터의 크기\n",
    "# 5 : 각 입력 시퀀스의 길이 (즉, input_length)\n",
    "\n",
    "# 각 정수는 아래의 테이블의 인덱스로 사용되며 Embedding()은 각 단어에 대해 임베딩 벡터를 리턴한다.\n",
    "+------------+------------+\n",
    "|   index    | embedding  |\n",
    "+------------+------------+\n",
    "|     0      | [1.2, 3.1] |\n",
    "|     1      | [0.1, 4.2] |\n",
    "|     2      | [1.0, 3.1] |\n",
    "|     3      | [0.3, 2.1] |\n",
    "|     4      | [2.2, 1.4] |\n",
    "|     5      | [0.7, 1.7] |\n",
    "|     6      | [4.1, 2.0] |\n",
    "+------------+------------+\n",
    "# 위의 표는 임베딩 벡터가 된 결과를 예로서 정리한 것이고 Embedding()의 출력인 3D 텐서를 보여주는 것이 아님.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fjgd1UHMtmZS"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 6.2.7.4 `Embedding()`의 인자\n",
    "\n",
    "- `첫 번째 인자` : 단어 집합의 크기 (즉, 총 단어의 개수)\n",
    "- `두 번째 인자` : 임베딩 벡터의 출력 차원 (결과로서 나오는 임베딩 벡터의 크기)\n",
    "- `input_length` : 입력 시퀀스의 길이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gDTIKteQt4tU"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 6.3 모델링 (Modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dAPI3_Kwt7z9"
   },
   "source": [
    "### 6.3.1 `Sequential()`\n",
    "\n",
    "- 케라스에서는 입력층, 은닉층, 출력층과 같은 층을 구성하기 위해 `Sequential()`을 사용한다.\n",
    "- `Sequential()`을 `model`로 선언한 뒤에 `model.add()`라는 코드를 통해 층을 단계적으로 추가한다.\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(...)\n",
    "model.add(...)\n",
    "model.add(...)\n",
    "```\n",
    "\n",
    "- `Embedding()`을 통해 생성하는 임베딩 층(embedding layer) 또한 인공 신경망의 층의 하나이므로 `model.add()`로 추가해야 한다.\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary, output_dim, input_length))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WU8wS-jPxigv"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 6.3.2 `Dense()`\n",
    "\n",
    "- 전결합층(fully-connected layer)을 추가한다.\n",
    "- `model.add()`를 통해 추가할 수 있다.\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=3, activation='relu'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F2CF8bcBx9zn"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 6.3.2.1 `Dense()`의 대표적인 인자\n",
    "\n",
    "- `첫 번째 인자` : 출력 뉴런의 수\n",
    "- `input_dim` : 입력 뉴런의 수 (입력의 차원)\n",
    "- `activation` : 활성화 함수\n",
    "  - `linear` : 디폴트 값, 별도의 활성화 함수 없이 입력 뉴런과 가중치의 계산 결과를 그대로 출력 (ex. 선형 회귀)\n",
    "  - `sigmoid` : 시그모이드 함수, **이진 분류 문제**에서 **출력층**에 주로 사용하는 활성화 함수\n",
    "  - `softmax` : 소프트맥스 함수, 셋 이상을 분류하는 **다중 클래스 분류 문제**에서 **출력층**에 주로 사용되는 활성화 함수\n",
    "  - `relu` : 렐루 함수, **은닉층**에 주로 사용되는 활성화 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o2smhjw_yk5-"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 6.3.2.2 `Dense()`의 의미\n",
    "\n",
    "- 첫 번째 인자값 = 1 $\\rightarrow$ 총 1개의 출력 뉴런을 의미한다.\n",
    "- 두 번째 인자 `input_dim` : 입력층의 뉴런의 수 의미 (위 경우에는 3)\n",
    "- 이를 통해 3개의 입력층 뉴런과 1개의 출력층 뉴런을 만들었다.\n",
    "- 이를 시각화하면 다음과 같다.\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/32105/neural_network1_final.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJLpU4jpy7ou"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 6.3.2.3 2개의 전결합층 사용\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid')) # 출력층\n",
    "```\n",
    "\n",
    "- 이번에는 `Dense()`가 두 번 사용되었다.\n",
    "- `Dense()`가 처음 사용되었을 때와 추가로 사용되었을 때의 인자는 조금 다르다.\n",
    "- 이제 첫 번째 사용된 `Dense()`의 8이라는 값은 더 이상 출력층의 뉴런이 아니라 은닉층의 뉴런이다.\n",
    "- 뒤에 층이 하나 더 생겼기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mjNNuhxCzVWw"
   },
   "source": [
    "- 두 번째 `Dense()`는 `input_dim` 인자가 없다.\n",
    "- 이는 이미 이전층의 뉴런의 수가 8개라는 사실을 알고 있기 때문이다.\n",
    "- 위의 코드에서 두 번째 `Dense()`는 마지막 층이므로, 첫 번째 인자 1은 결국 출력층의 뉴런의 개수가 된다.\n",
    "- 이를 시각화하면 다음과 같다.\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/32105/neural_network2_final.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UnT5C-XkznLQ"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 6.3.2.4 `Dense()` 이외의 다양한 층들\n",
    "\n",
    "- `LSTM`\n",
    "- `GRU`\n",
    "- `Convolution2D`\n",
    "- `BatchNormalization`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "msiZ_bE4zyyv"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 6.3.3 `summary()`\n",
    "\n",
    "- 모델의 정보를 요약해서 보여준다.\n",
    "\n",
    "```python\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "```python\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_1 (Dense)              (None, 8)                 40        \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 1)                 9         \n",
    "=================================================================\n",
    "Total params: 49\n",
    "Trainable params: 49\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3yR4Bi9Fz_uZ"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 6.4 컴파일(Compile)과 훈련(Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nwZ7soNg0OOL"
   },
   "source": [
    "### 6.4.1 `compile()`\n",
    "\n",
    "- 모델을 기계가 이해할 수 있도록 컴파일한다.\n",
    "- 오차 함수와 최적화 방법, 메트릭 함수를 선택할 수 있다.\n",
    "\n",
    "```python\n",
    "# 이 코드는 뒤의 텍스트 분류 챕터의 스팸 메일 분류하기 실습 코드를 갖고온 것임\n",
    "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "max_features = 10000\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "```\n",
    "\n",
    "- 위의 코드는 임베딩 층, 은닉층, 출력층을 추가하여 모델을 설계한 후에 마지막으로 컴파일하는 과정을 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a9gmar_o077l"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 6.4.1.1 `compile()`의 주요 인자\n",
    "\n",
    "- `optimizer` : 훈련 과정에서 설정하는 옵티마이저를 설정, `'adam'`, `'sgd'`와 같이 문자열로 지정할 수도 있다.\n",
    "- `loss` : 훈련 과정에서 사용할 손실 함수(loss function)를 설정\n",
    "- `metrics` : 훈련을 모니터링하기 위한 지표를 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ySt3_eWQ1Yvx"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 6.4.2 대표적을로 사용되는 손실 함수와 활성화 함수의 조합\n",
    "\n",
    "- 더 많은 함수는 케라스 공식 문서에서 확인 가능하다.\n",
    "\n",
    "| 문제유형         | 손실 함수                                              | 출력층의 활성화 함수 | 참고 설명                                                    |\n",
    "| ---------------- | ------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n",
    "| 회귀 문제        | `mean_squared_error`<br />(평균 제곱 오차)             | -                    | -                                                            |\n",
    "| 다중 클래스 분류 | `categorical_crossentropy`<br />(범주형 교차 엔트로피) | 소프트맥스           | Ch10 로이터 뉴스 분류하기 실습 참고                          |\n",
    "| 다중 클래스 분류 | `sparse_categorical_crossentropy`                      | 소프트맥스           | 범주형 교차 크로스엔트로피와 동일<br />하지만 이 경우 원-핫 인코딩이 된 상태일 필요 없이 정수 인코딩된 상태에서 수행 가능 |\n",
    "| 이진 분류        | `binary_crossentropy`<br />(이항 교차 엔트로피)        | 시그모이드           | Ch10 스팸 메일 분류하기, IMDB 리뷰 감성 분류하기 실습 참고   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OODPZ5w42Vgv"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 6.4.3 `fit()`\n",
    "\n",
    "- 진행중..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x82HyK_l2jDt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch08_v06_Keras-Overview.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
