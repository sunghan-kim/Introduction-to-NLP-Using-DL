{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Op-hO-blWtU"
   },
   "source": [
    "# Ch08. 딥 러닝(Deep Learning) 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hZeF_xEtlkuB"
   },
   "source": [
    "# v08. 피드 포워드 신경망 언어 모델 (Neural Network Language Model, NNLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eS_-X7qUlp18"
   },
   "source": [
    "- 파이썬 등과 같은 프로그래밍 언어를 사용할 때는 명세되어져 있는 튜플, 클래스 등과 같은 용어와 작성할 때 지켜야 하는 문법을 바탕으로 코드를 작성한다.\n",
    "- 문법에 맞지 않으면 에러가 발생하므로 명세된 규칙을 지키는 것은 필수적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-yQ2_w3l0xr"
   },
   "source": [
    "- 자연어는 어떨까?\n",
    "- 자연어에도 문법이라는 규칙이 있기는 하지만, 많은 예외 사항, 시간에 따른 언어의 변화, 중의성과 모호성 문제 등을 전부 명세하기란 어렵다.\n",
    "- 기계가 자연어를 표현하도록 규칙으로 명세하기가 어려운 상황에서 대안은 규칙 기반 접근이 아닌 **기계가 주어진 자연어 데이터를 학습하게 하는 것**이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1kKW9m3Hl-RD"
   },
   "source": [
    "- 과거에는 기계가 자연어를 학습하게 하는 방법으로 통계적인 접근을 사용했다.\n",
    "- 그러나 최근에는 인공 신경망을 사용하는 방법이 자연어 처리에서 더 좋은 성능을 얻고 있다.\n",
    "- 번역기, 음성 인식 같이 자연어 생성(Natural Language Generation, NLG)의 기반으로 사용되는 언어 모델도 마찬가지이다.\n",
    "- 통계적 언어 모델(Statistical Language Model, SLM)에서 다양한 구조의 인공 신경망을 사용한 언어 모델들로 대체되기 시작했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8T7UQE3XmKnj"
   },
   "source": [
    "- 여기서는 신경망 언어 모델의 시초인 피드 포워드 신경망 언어 모델(Feed Forward Neural Network Language Model)에 대해서 학습한다.\n",
    "- 여기서는 간단히 줄여 NNLM이라고 하자.\n",
    "- 뒤의 챕터에서 RNNLM, BiLM 등의 보다 발전된 신경망 언어 모델들을 배운다.\n",
    "- cf) 이 모델이 제안 되었을 때는 NPLM(Neural Probabilistic Language Model)이라는 이름을 갖고 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NP6JS4NfmXaa"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 8.1 기존 N-gram 언어 모델의 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7T74opNCn9Sp"
   },
   "source": [
    "### 8.1.1 언어 모델링(Language Modeling)\n",
    "\n",
    "- 언어 모델은 문장에 확률을 할당하는 모델이다.\n",
    "- 주어진 문맥으로부터 아직 모르는 단어를 예측하는 것을 언어 모델링이라고 한다.\n",
    "- 다음은 이전 단어들로부터 다음 단어를 예측하는 **언어 모델링(Language Modeling)**의 예를 보여준다.\n",
    "\n",
    "```\n",
    "# 다음 단어 예측하기\n",
    "An aborable little boy is spreading _____\n",
    "```\n",
    "\n",
    "- 위 문장을 가지고 앞서 배운 n-gram 언어 모델이 언어 모델링을 하는 방법을 복습해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Evs2HbC4qsZT"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.1.2 n-gram 언어 모델\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/21692/n-gram.PNG)\n",
    "\n",
    "- n-gram 언어 모델은 언어 모델링에 바로 앞의 n-1개의 단어만 참고한다.\n",
    "- 4-gram 언어 모델이라고 가정하자.\n",
    "- 모델은 바로 앞의 3개의 단어만 참고하며 더 앞의 단어들은 무시한다.\n",
    "- 위 예제에서 다음 단어 예측에 사용되는 단어는 다음과 같다.\n",
    "  - boy\n",
    "  - is\n",
    "  - spreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "diV8KOj4rhOT"
   },
   "source": [
    "$\n",
    "\\qquad\n",
    "P(w|\\text{boy is spreading}) = \\frac{\\text{count(boy is spreading } w \\text{)}}{\\text{count(boy is spreading)}}\n",
    "$\n",
    "\n",
    "- 그 후에는 훈련 코퍼스에서 분모와 분자를 다음과 같이 하여 다음 단어가 등장할 확률을 예측했다.\n",
    "  - 분모 : (n-1)-gram을 카운트한 것\n",
    "  - 분자 : n-gram을 카운트한 것\n",
    "- 예를 들어 갖고 있는 코퍼스에서 다음과 같이 문장들이 등장했다고 하자.\n",
    "   - \"boy is spreading\" : 1,000번\n",
    "   - \"boy is spreading insults\" : 500번\n",
    "   - \"boy is spreading smiles\" : 200번\n",
    "- 이 때 각 확률은 아래와 같다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "P(\\text{insults} | \\text{boy is spreading}) = 0.500\n",
    "$\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "P(\\text{smiles} | \\text{boy is spreading}) = 0.200\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oVCHK6YDrsjh"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.1.3 n-gram 언어 모델의 희소 문제(sparsity problem)\n",
    "\n",
    "- 하지만 이러한 n-gram 언어 모델은 충분한 데이터를 관측하지 못하면 언어를 정확히 모델링하지 못하는 **희소 문제(sparsity problem)**가 있다.\n",
    "- 예를 들어 훈련 코퍼스에 \"boy is spreading smile\"라는 단어 시퀀스가 존재하지 않으면 n-gram 언어 모델에서 해당 단어 시퀀스의 확률 $P(\\text{smiles} | \\text{boy is spreading})$는 0이 되버린다.\n",
    "- 이는 언어 모델이 예측하기에 \"boy is spreading\" 다음에 \"smiles\"이란 단어가 나올 수 없다는 의미이다.\n",
    "- 하지만 해당 단어 시퀀스는 현실에서 실제로는 많이 사용되므로 제대로된 모델링이 아니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0QT95SwZwQjR"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 8.2 단어의 의미적 유사성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IC1EQaRDw6-B"
   },
   "source": [
    "### 8.2.1 단어의 유사도를 통한 희소 문제 해결\n",
    "\n",
    "- 희소 문제는 기계가 단어 각 유사도를 알 수 있다면 해결할 수 있는 문제이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBeo0IiRxwar"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.2.2 실제 사례\n",
    "\n",
    "- \"톺아보다\"라는 생소한 단어를 새로 배웠고, \"톺아보다\"가 \"샅샅이 살펴보다\"와 유사한 의미임을 학습했다.\n",
    "- 그리고 \"발표 자료를 살펴보다\"라는 표현 대신 \"발표 자료를 톺아보다\"라는 표현을 써봤다.\n",
    "- \"발표 자료를 톺아보다\"라는 예문을 어디서 읽은 적은 없지만 두 단어가 유사함을 학습하였으므로 단어를 대신 선택하여 자연어 생성을 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lMT-w0A2w9S4"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.2.3 실제 사례를 기계에 적용\n",
    "\n",
    "- \"보도 자료를 살펴보다\"라는 단어 시퀀스는 존재하지만, \"발표 자료를 톺아보다\"라는 단어 시퀀스는 존재하지 않는 코퍼스를 학습한 언어 모델이 있다고 가정해보자.\n",
    "- 언어 모델은 아래 선택지에서 다음 단어를 예측해야 한다.\n",
    "\n",
    "$\\qquad$ P(톺아보다 | 보도 자료를)\n",
    "\n",
    "$\\qquad$ P(냠냠하다 | 보도 자료를)\n",
    "\n",
    "- \"살펴보다\"와 \"톺아보다\"의 유사성을 학습하였고 이를 근거로 두 선택지 중에서 \"톺아보다\"가 더 맞는 선택이라고 판단할 수 있다.\n",
    "- 하지만 n-gram 언어 모델은 \"보도 자료를\" 다음에 \"톺아보다\"가 나올 확률 P(톺아보다 | 보도 자료를) 를 0으로 연산한다.\n",
    "- n-gram 언어 모델은 \"살펴보다\"와 \"톺아보다\"의 단어 유사도를 학습한 적이 없으며, 예측에 고려할 수 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o-mVlPTYxoQo"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.2.4 신경망 언어 모델(NNLM)과 워드 임베딩(word embedding)의 아이디어\n",
    "\n",
    "- 만약 이 언어 모델 또한 단어의 유사도를 학습할 수 있도록 설계한다면, 훈련 코퍼스에 없는 단어 시퀀스에 대한 예측이라도 유사한 단어가 사용된 단어 시퀀스를 참고하여 보다 정확한 예측을 할 수 있을 것이다.\n",
    "- 그리고 이런 아이디어를 가지고 탄생한 언어 모델이 **신경망 언어 모델(NNLM)**이다.\n",
    "- 그리고 이 아이디어는 단어 간 유사도를 반영한 벡터를 만드는 **워드 임베딩(word embedding)**의 아이디어이기도 한다.\n",
    "- 이제 NNL이 어떻게 훈련 과정에서 단어의 유사도를 학습할 수 있는 지 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L2W3jzQhyYvn"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 8.3 피드 포워드 신경망 언어 모델 (NNLM)\n",
    "\n",
    "- NNLM이 언어 모델링을 학습하는 과정을 살펴보자.\n",
    "- 이해를 위해 매우 간소화된 형태로 설명한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JIbDd_tFylNV"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.3.1 훈련 코퍼스\n",
    "\n",
    "**예문 : \"what will the fat cat sit on\"**\n",
    "\n",
    "- 예를 들어 훈련 코퍼스에 위와 같은 문장이 있다고 해보자.\n",
    "- 언어 모델은 주어진 단어 시퀀스로부터 다음 단어를 예측하는 모델이다.\n",
    "- 훈련 과정에서는 \"what will the fat cat\"이라는 단어 시퀀스가 입력으로 주어지면, 다음 단어 \"sit\"을 예측하는 방식으로 훈련된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yDVHaEauy5Fs"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.3.2 단어 숫자 인코딩\n",
    "\n",
    "- 훈련 코퍼스가 준비된 상태에서 가장 먼저 해야 할 일은 기계가 단어를 인식할 수 있도록 모든 단어를 숫자로 인코딩하는 것이다.\n",
    "- 훈련 코퍼스에 7개의 단어만 존재한다고 가정했을 때 위 단어들에 대해서 다음과 같이 원-핫 인코딩을 할 수 있다.\n",
    "\n",
    "```python\n",
    "what = [1, 0, 0, 0, 0, 0, 0]\n",
    "will = [0, 1, 0, 0, 0, 0, 0]\n",
    "the = [0, 0, 1, 0, 0, 0, 0]\n",
    "fat = [0, 0, 0, 1, 0, 0, 0]\n",
    "cat = [0, 0, 0, 0, 1, 0, 0]\n",
    "sit = [0, 0, 0, 0, 0, 1, 0]\n",
    "on = [0, 0, 0, 0, 0, 0, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZZ40PwlYzKqS"
   },
   "source": [
    "- 모든 단어가 단어 집합(vocabulary)의 크기인 7의 차원을 가지는 원-핫 벡터가 됐다.\n",
    "- 이제 이 원-핫 벡터들이 훈련을 위한 NNLM의 입력이면서 예측을 위한 레이블이기도 한다.\n",
    "- 'what will the fat cat'를 입력을 받아서 'sit'을 예측하는 일은 기계에게 실제로는 what, will, the, fat, cat의 원-핫 벡터를 입력받아 sit의 원-핫 벡터를 예측하는 문제가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gechQgLm295F"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.3.3 윈도우(window)\n",
    "\n",
    "- NNLM은 n-gram 언어 모델과 유사하게 다음 단어를 예측할 때, 앞의 모든 단어를 참고하는 것이 아니라 정해진 n개의 단어만을 참고한다.\n",
    "- 예를 들어 n을 4라고 해보자.\n",
    "- 이 때, 언어 모델은 \"what will the fat cat\"라는 단어 시퀀스가 주어졌을 때, 다음 단어를 예측하기 위해 앞의 4개 단어 \"will the fat cat\"까지만 참고하고 그 앞의 단어인 \"what\"은 무시한다.\n",
    "- 이 범위를 윈도우(window)라고 하기도 한다.\n",
    "- 여기서 윈도우의 크기 n은 4이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kb4v8ki83kff"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.3.4 NNLM의 구조\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/45609/nnlm1.PNG)\n",
    "\n",
    "- NNLM은 위의 그림과 같이 총 4개의 층(layer)으로 이루어진 인공 신경망이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-Yf2iCo3v6u"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 8.3.4.1 입력층(input layer)\n",
    "\n",
    "- 앞에서 윈도우의 크기는 4로 정하였으므로 입력은 4개의 단어 \"will, the, fat, cat\"의 원-핫 벡터이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JmPNwIDY3-ga"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 8.3.4.2 출력층(output layer)\n",
    "\n",
    "- 모델이 예측해야 하는 정답에 해당되는 단어 \"sit\"의 원-핫 벡터는 출력층에서 모델이 예측한 값의 오차를 구하기 위해 사용될 예정이다.\n",
    "- 그리고 이 오차로부터 손실 함수를 사용해 인공 신경망이 학습을 하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vk7LxuHS4UhS"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.3.5 NNLM 내부 메커니즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JOyTYwg_5B-K"
   },
   "source": [
    "#### 8.3.5.1 투사층(projection layer)\n",
    "\n",
    "- 4개의 원-핫 벡터를 입력 받은 NNLM은 다음층인 **투사층(projection layer)**를 지나게 된다.\n",
    "- 인공 신경망에서 입력층과 출력층 사이의 층은 보통 은닉층이라고 부른다.\n",
    "- 하지만 여기서 투사층이 일반 은닉층과 구별되는 특징은 가중치 행렬과의 연산은 이루어지지만 **활성화 함수가 존재하지 않는다는 것**이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1q6vLh1K4s5A"
   },
   "source": [
    "<br> \n",
    "\n",
    "#### 8.3.5.2 투사층의 가중치 행렬의 크기\n",
    "\n",
    "- 투사층의 크기를 M으로 설정하면, 각 입력 단어들은 투사층에서 V x M 크기의 가중치 행렬과 곱해진다.\n",
    "- 여기서 V는 단어 집합의 크기를 의미한다.\n",
    "- 만약 원-핫 벡터의 차원이 7이고, M이 5라면 가중치 행렬 W는 7 x 5 행렬이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9G2glCBt4r90"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 8.3.5.3 룩업 테이블 (lookup table)\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/45609/nnlm2_renew.PNG)\n",
    "\n",
    "- 각 단어의 원-핫 벡터와 가중치 행렬 W 행렬의 곱이 어떻게 이루어지는 지 살펴보자.\n",
    "- 위 그림에서는 각 원-핫 벡터를 $x$로 표기했다.\n",
    "- 원-핫 벡터의 특성으로 인해 i번째 인덱스에 1이라는 값을 가지고 그 외의 0의 값을 가지는 원-핫 벡터와 가중치 W 행렬의 곱은 사실 **W 행렬의 i번째 행을 그대로 읽어오는 것(lookup)과 동일**하다.\n",
    "- 그래서 이 작업을 **룩업 테이블(lookup table)**이라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y31-7MKP5wzv"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 8.3.5.4 임베딩 벡터 (embedding vector)\n",
    "\n",
    "- 이 룩업 테이블 작업을 거치면 V의 차원을 가지는 **원-핫 벡터**는 이보다 **더 차원이 작은** M 차원의 단어 벡터로 맵핑된다.\n",
    "- 위 그림에서는 \n",
    "  - 단어 'fat'을 의미하는 원-핫 벡터를 $x_{\\text{fat}}$으로 표현\n",
    "  - 테이블 룩업 과정을 거친 후의 단어 벡터는 $e_{\\text{fat}}$으로 표현\n",
    "- 이 벡터들은 초기에는 랜덤한 값을 가지지만 학습 과정에서 값이 계속 변경된다.\n",
    "- 이 단어 벡터를 **임베딩 벡터(embedding vector)**라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "70nXbEmb6WbA"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 8.3.5.5 투사층의 식 표현\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/45609/nnlm3_renew.PNG)\n",
    "\n",
    "- 각 단어가 테이블 룩업을 통해 임베딩 벡터로 변경됨\n",
    "- 그런 다음 투사층(projection layer)에서 모든 임베딩 벡터들의 값은 **연결(concatenation)**된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gLJFyudkUwtN"
   },
   "source": [
    "- 투사층을 식으로 표현하면 아래와 같다.\n",
    "  - $x$ : 각 단어의 원-핫 벡터\n",
    "  - 문장에서 $t$번 째 단어 : NNLM이 예측하고자 하는 단어\n",
    "  - $n$ : 윈도우의 크기\n",
    "  - $lookup$ : 룩업 테이블을 의미하는 함수\n",
    "  - 세미콜론(;) : 연결 기호\n",
    "\n",
    "<br>\n",
    "\n",
    "- 투사층 : \n",
    "$\n",
    "\\quad\n",
    "p^{layer} = \\left( lookup(x_{t-n}); \\; \\cdots ; \\; lookup(x_{t-2}); \\; lookup(x_{t-1}); \\right)\n",
    "= (e_{t-n}; \\; \\cdots; \\; e_{t-2}; \\; e_{t-1})\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCByXl7aV7DH"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 8.3.5.6 투사층의 특징 : 선형층(linear layer)\n",
    "\n",
    "- 일반적인 은닉층은 활성화 함수를 사용하는 비선형층(nonlinear layer)이다.\n",
    "- 이와 달리 투사층은 활성화 함수가 존재하지 않는 선형층(linear layer)이다.\n",
    "- 이 다음부터는 다시 은닉층을 사용하므로 일반적인 피드 포워드 신경망과 동일하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5esv5hWzV4zF"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 8.3.5.7 은닉층\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/45609/nnlm4.PNG)\n",
    "\n",
    "- 투사층의 결과는 h의 크기를 가지는 은닉층을 지난다.\n",
    "- 일반적인 피드 포워드 신경망에서 은닉층을 지난다는 것은 은닉층의 입력은 가중치와 곱해진 후 편향이 더해져 활성화 함수의 입력이 된다는 의미이다.\n",
    "- 이 때의 가중치와 편향을 각각 $W_h$와 $b_h$라고 하자.\n",
    "- 은닉층의 활성화 함수를 하이퍼볼릭 탄젠트 함수라고 했을 때, 은닉층을 식으로 표현하면 아래와 같다.\n",
    "\n",
    "<br>\n",
    "\n",
    "- 은닉층 : \n",
    "$\n",
    "\\quad\n",
    "h^{\\text{layer}} = tanh \\left( W_h \\; p^{layer} + b_h \\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0qlNlT_XDAM"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 8.3.5.8 출력층\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/45609/nnlm5_final.PNG)\n",
    "\n",
    "- 은닉층의 출력은 이제 V의 크기를 가지는 출력층으로 향한다.\n",
    "- 이 과정에서 또 다시 다른 가중치와 곱해지고 편향이 더해지면, 입력이었던 원-핫 벡터들과 동일하게 V차원의 벡터를 얻는다.\n",
    "- 만약 입력 벡터의 차원이 7이었다면 여기서 나오는 벡터도 마찬가지이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Swso8ucvXC8J"
   },
   "source": [
    "- 출력층에서는 **활성화 함수**로 **소프트맥스(softmax) 함수**를 사용한다.\n",
    "- V차원의 벡터는 소프트맥스 함수를 지나면서 각 원소는 0과 1 사이의 실수값을 가지며 총합은 1이 되는 상태로 바뀐다.\n",
    "- 이렇게 나온 벡터를 NNLM의 예측값이라는 의미에서 $\\hat{y}$라고 하자.\n",
    "- 이를 식으로 표현하면 아래와 같다.\n",
    "\n",
    "<br>\n",
    "\n",
    "- 출력층 : \n",
    "$\n",
    "\\quad\n",
    "\\hat{y} = softmax \\left( W_y \\; h^{layer} + b_y \\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KbWliB6VYxKb"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.3.6 벡터 $\\hat{y}$의 각 차원 안에서의 값의 의미\n",
    "\n",
    "- $\\hat{y}$의 j번 째 인덱스가 가진 0과 1 사이의 값은 j번 째 단어가 다음 단어일 확률을 나타낸다.\n",
    "- 그리고 $\\hat{y}$는 실제값, 즉 실제 정답에 해당되는 단어의 원-핫 벡터의 값에 가까워져야 한다.\n",
    "- 실제값에 해당되는 다음 단어를 $y$라고 했을 때, 이 두 벡터가 가까워지게 하기 위해서 NNLM은 **손실 함수**로 **cross-entropy** 함수를 사용한다.\n",
    "- 그리고 역전파가 이루어지면서 가중치 행렬들이 학습된다.\n",
    "- 이 과정에서 임베딩 벡터값들도 학습된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8RswWdk1ZWtU"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.3.7 NNLM이 얻을 수 있는 이점\n",
    "\n",
    "- 이번 예제에서는 7개의 단어만 사용했다.\n",
    "- 만약 충분한 훈련 데이터가 있다는 가정 하에 NNLM이 얻을 수 있는 이점은 무엇일까?\n",
    "- NNLM의 핵심은 충분한 양의 훈련 코퍼스를 위와 같은 과정으로 학습한다면 결과적으로 수많은 문장에서 유사한 목적으로 사용되는 단어들은 결국 유사한 임베딩 벡터값을 얻게 되는 것이다.\n",
    "- 이렇게 되면 훈련이 끝난 후 다음 단어를 예측하는 과정에서 훈련 코퍼스에서 없던 단어 시퀀스라고 하더라도 다음 단어를 선택할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xHQUFLv9Z63h"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.3.8 임베딩 벡터의 아이디어 활용\n",
    "\n",
    "- 단어 간 유사도를 구할 수 있는 임베딩 벡터의 아이디어는 Word2Vec, FastText, GloVe 등으로 발전되어서 딥 러닝 모델에서는 필수적으로 사용되는 방법이 되었다.\n",
    "- 임베딩 벡터에 대해서는 워드 임베딩 챕터에서 자세히 다룬다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mgHmGbI0aMgl"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 8.4 NNLM의 이점과 한계\n",
    "\n",
    "- NNLM은 기존 n-gram 언어 모델의 한계를 몇 가지 개선했지만 또한 여전히 가지는 문제점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rSklRD5NaUYH"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.4.1 기존 모델에서의 개선점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h_txnc4aaYuQ"
   },
   "source": [
    "- NNLM은 단어를 표현하기 위해 **밀집 벡터(dense vector)**를 사용하므로서 단어의 유사도를 표현할 수 있었다.\n",
    "- 그리고 이를 통해 희소 문제(sparsity problem)를 해결했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qcTSzvZsceMz"
   },
   "source": [
    "- 밀집 벡터(dense vector) : 벡터의 원소들이 실수값을 가지면서, 원-핫 벡터보다 저차원을 가지는 벡터\n",
    "- 원-핫 벡터 : 벡터의 원소값이 대부분이 0이란 의미에서 **희소 벡터(sparse vector)**라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k1p6lkn2cug7"
   },
   "source": [
    "- 또한 더 이상 모든 n-gram을 저장하지 않아도 된다는 점에서 n-gram 언어 모델보다 **저장 공간의 이점**을 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jj_klMj2c2GC"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.4.2 고정된 길이의 입력 (Fixed-length input)\n",
    "\n",
    "- NNLM이 극복하지 못한 한계 또한 존재한다.\n",
    "- NNLM은 n-gram 언어 모델과 마찬가지로 **다음 단어를 예측하기 위해 모든 이전 단어를 참고하는 것이 아니라, 정해진 n개의 단어만을 참고**한다.\n",
    "- 이는 버려지는 단어들이 가진 문맥 정보는 참고할 수 없음을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EEqe2QoPdLM9"
   },
   "source": [
    "- 훈련 코퍼스에 있는 각 문장의 길이는 전부 다를 수 있다.\n",
    "- 이를 개선하기 위해서는 모델이 매번 다른 길이의 입력 시퀀스에 대해서도 처리할 수 있는 능력이 필요하다.\n",
    "- 피드 포워드 신경망으로 만든 언어 모델이 이를 할 수 없다면, 피드 포워드 신경망이 아닌 다른 신경망을 사용하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxTxtERxdbzE"
   },
   "source": [
    "- 다음 챕터에서 RNN(Recurrent Neural Network)을 학습하고, 그 이후에 RNN을 이용하여 만든 언어 모델인 RNN 언어 모델(Recurrent Neural Network Language Model, RNNLM)에 대해 학습한다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch08_v08_Neural-Network-Language-Model-NNLM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
