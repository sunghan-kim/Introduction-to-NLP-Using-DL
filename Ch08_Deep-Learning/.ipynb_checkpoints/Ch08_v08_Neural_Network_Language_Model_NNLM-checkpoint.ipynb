{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Op-hO-blWtU"
   },
   "source": [
    "# Ch08. 딥 러닝(Deep Learning) 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hZeF_xEtlkuB"
   },
   "source": [
    "# v08. 피드 포워드 신경망 언어 모델 (Neural Network Language Model, NNLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eS_-X7qUlp18"
   },
   "source": [
    "- 파이썬 등과 같은 프로그래밍 언어를 사용할 때는 명세되어져 있는 튜플, 클래스 등과 같은 용어와 작성할 때 지켜야 하는 문법을 바탕으로 코드를 작성한다.\n",
    "- 문법에 맞지 않으면 에러가 발생하므로 명세된 규칙을 지키는 것은 필수적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-yQ2_w3l0xr"
   },
   "source": [
    "- 자연어는 어떨까?\n",
    "- 자연어에도 문법이라는 규칙이 있기는 하지만, 많은 예외 사항, 시간에 따른 언어의 변화, 중의성과 모호성 문제 등을 전부 명세하기란 어렵다.\n",
    "- 기계가 자연어를 표현하도록 규칙으로 명세하기가 어려운 상황에서 대안은 규칙 기반 접근이 아닌 **기계가 주어진 자연어 데이터를 학습하게 하는 것**이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1kKW9m3Hl-RD"
   },
   "source": [
    "- 과거에는 기계가 자연어를 학습하게 하는 방법으로 통계적인 접근을 사용했다.\n",
    "- 그러나 최근에는 인공 신경망을 사용하는 방법이 자연어 처리에서 더 좋은 성능을 얻고 있다.\n",
    "- 번역기, 음성 인식 같이 자연어 생성(Natural Language Generation, NLG)의 기반으로 사용되는 언어 모델도 마찬가지이다.\n",
    "- 통계적 언어 모델(Statistical Language Model, SLM)에서 다양한 구조의 인공 신경망을 사용한 언어 모델들로 대체되기 시작했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8T7UQE3XmKnj"
   },
   "source": [
    "- 여기서는 신경망 언어 모델의 시초인 피드 포워드 신경망 언어 모델(Feed Forward Neural Network Language Model)에 대해서 학습한다.\n",
    "- 여기서는 간단히 줄여 NNLM이라고 하자.\n",
    "- 뒤의 챕터에서 RNNLM, BiLM 등의 보다 발전된 신경망 언어 모델들을 배운다.\n",
    "- cf) 이 모델이 제안 되었을 때는 NPLM(Neural Probabilistic Language Model)이라는 이름을 갖고 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NP6JS4NfmXaa"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 8.1 기존 N-gram 언어 모델의 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7T74opNCn9Sp"
   },
   "source": [
    "### 8.1.1 언어 모델링(Language Modeling)\n",
    "\n",
    "- 언어 모델은 문장에 확률을 할당하는 모델이다.\n",
    "- 주어진 문맥으로부터 아직 모르는 단어를 예측하는 것을 언어 모델링이라고 한다.\n",
    "- 다음은 이전 단어들로부터 다음 단어를 예측하는 **언어 모델링(Language Modeling)**의 예를 보여준다.\n",
    "\n",
    "```\n",
    "# 다음 단어 예측하기\n",
    "An aborable little boy is spreading _____\n",
    "```\n",
    "\n",
    "- 위 문장을 가지고 앞서 배운 n-gram 언어 모델이 언어 모델링을 하는 방법을 복습해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Evs2HbC4qsZT"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.1.2 n-gram 언어 모델\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/21692/n-gram.PNG)\n",
    "\n",
    "- n-gram 언어 모델은 언어 모델링에 바로 앞의 n-1개의 단어만 참고한다.\n",
    "- 4-gram 언어 모델이라고 가정하자.\n",
    "- 모델은 바로 앞의 3개의 단어만 참고하며 더 앞의 단어들은 무시한다.\n",
    "- 위 예제에서 다음 단어 예측에 사용되는 단어는 다음과 같다.\n",
    "  - boy\n",
    "  - is\n",
    "  - spreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "diV8KOj4rhOT"
   },
   "source": [
    "$\n",
    "\\qquad\n",
    "P(w|\\text{boy is spreading}) = \\frac{\\text{count(boy is spreading } w \\text{)}}{\\text{count(boy is spreading)}}\n",
    "$\n",
    "\n",
    "- 그 후에는 훈련 코퍼스에서 분모와 분자를 다음과 같이 하여 다음 단어가 등장할 확률을 예측했다.\n",
    "  - 분모 : (n-1)-gram을 카운트한 것\n",
    "  - 분자 : n-gram을 카운트한 것\n",
    "- 예를 들어 갖고 있는 코퍼스에서 다음과 같이 문장들이 등장했다고 하자.\n",
    "   - \"boy is spreading\" : 1,000번\n",
    "   - \"boy is spreading insults\" : 500번\n",
    "   - \"boy is spreading smiles\" : 200번\n",
    "- 이 때 각 확률은 아래와 같다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "P(\\text{insults} | \\text{boy is spreading}) = 0.500\n",
    "$\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "P(\\text{smiles} | \\text{boy is spreading}) = 0.200\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oVCHK6YDrsjh"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.1.3 n-gram 언어 모델의 희소 문제(sparsity problem)\n",
    "\n",
    "- 하지만 이러한 n-gram 언어 모델은 충분한 데이터를 관측하지 못하면 언어를 정확히 모델링하지 못하는 **희소 문제(sparsity problem)**가 있다.\n",
    "- 예를 들어 훈련 코퍼스에 \"boy is spreading smile\"라는 단어 시퀀스가 존재하지 않으면 n-gram 언어 모델에서 해당 단어 시퀀스의 확률 $P(\\text{smiles} | \\text{boy is spreading})$는 0이 되버린다.\n",
    "- 이는 언어 모델이 예측하기에 \"boy is spreading\" 다음에 \"smiles\"이란 단어가 나올 수 없다는 의미이다.\n",
    "- 하지만 해당 단어 시퀀스는 현실에서 실제로는 많이 사용되므로 제대로된 모델링이 아니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0QT95SwZwQjR"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 8.2 단어의 의미적 유사성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IC1EQaRDw6-B"
   },
   "source": [
    "### 8.2.1 단어의 유사도를 통한 희소 문제 해결\n",
    "\n",
    "- 희소 문제는 기계가 단어 각 유사도를 알 수 있다면 해결할 수 있는 문제이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBeo0IiRxwar"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.2.2 실제 사례\n",
    "\n",
    "- \"톺아보다\"라는 생소한 단어를 새로 배웠고, \"톺아보다\"가 \"샅샅이 살펴보다\"와 유사한 의미임을 학습했다.\n",
    "- 그리고 \"발표 자료를 살펴보다\"라는 표현 대신 \"발표 자료를 톺아보다\"라는 표현을 써봤다.\n",
    "- \"발표 자료를 톺아보다\"라는 예문을 어디서 읽은 적은 없지만 두 단어가 유사함을 학습하였으므로 단어를 대신 선택하여 자연어 생성을 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lMT-w0A2w9S4"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.2.3 실제 사례를 기계에 적용\n",
    "\n",
    "- \"보도 자료를 살펴보다\"라는 단어 시퀀스는 존재하지만, \"발표 자료를 톺아보다\"라는 단어 시퀀스는 존재하지 않는 코퍼스를 학습한 언어 모델이 있다고 가정해보자.\n",
    "- 언어 모델은 아래 선택지에서 다음 단어를 예측해야 한다.\n",
    "\n",
    "$\\qquad$ P(톺아보다 | 보도 자료를)\n",
    "\n",
    "$\\qquad$ P(냠냠하다 | 보도 자료를)\n",
    "\n",
    "- \"살펴보다\"와 \"톺아보다\"의 유사성을 학습하였고 이를 근거로 두 선택지 중에서 \"톺아보다\"가 더 맞는 선택이라고 판단할 수 있다.\n",
    "- 하지만 n-gram 언어 모델은 \"보도 자료를\" 다음에 \"톺아보다\"가 나올 확률 P(톺아보다 | 보도 자료를) 를 0으로 연산한다.\n",
    "- n-gram 언어 모델은 \"살펴보다\"와 \"톺아보다\"의 단어 유사도를 학습한 적이 없으며, 예측에 고려할 수 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o-mVlPTYxoQo"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.2.4 신경망 언어 모델(NNLM)과 워드 임베딩(word embedding)의 아이디어\n",
    "\n",
    "- 만약 이 언어 모델 또한 단어의 유사도를 학습할 수 있도록 설계한다면, 훈련 코퍼스에 없는 단어 시퀀스에 대한 예측이라도 유사한 단어가 사용된 단어 시퀀스를 참고하여 보다 정확한 예측을 할 수 있을 것이다.\n",
    "- 그리고 이런 아이디어를 가지고 탄생한 언어 모델이 **신경망 언어 모델(NNLM)**이다.\n",
    "- 그리고 이 아이디어는 단어 간 유사도를 반영한 벡터를 만드는 **워드 임베딩(word embedding)**의 아이디어이기도 한다.\n",
    "- 이제 NNL이 어떻게 훈련 과정에서 단어의 유사도를 학습할 수 있는 지 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L2W3jzQhyYvn"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 8.3 피드 포워드 신경망 언어 모델 (NNLM)\n",
    "\n",
    "- NNLM이 언어 모델링을 학습하는 과정을 살펴보자.\n",
    "- 이해를 위해 매우 간소화된 형태로 설명한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JIbDd_tFylNV"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.3.1 훈련 코퍼스\n",
    "\n",
    "**예문 : \"what will the fat cat sit on\"**\n",
    "\n",
    "- 예를 들어 훈련 코퍼스에 위와 같은 문장이 있다고 해보자.\n",
    "- 언어 모델은 주어진 단어 시퀀스로부터 다음 단어를 예측하는 모델이다.\n",
    "- 훈련 과정에서는 \"what will the fat cat\"이라는 단어 시퀀스가 입력으로 주어지면, 다음 단어 \"sit\"을 예측하는 방식으로 훈련된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yDVHaEauy5Fs"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.3.2 단어 숫자 인코딩\n",
    "\n",
    "- 훈련 코퍼스가 준비된 상태에서 가장 먼저 해야 할 일은 기계가 단어를 인식할 수 있도록 모든 단어를 숫자로 인코딩하는 것이다.\n",
    "- 훈련 코퍼스에 7개의 단어만 존재한다고 가정했을 때 위 단어들에 대해서 다음과 같이 원-핫 인코딩을 할 수 있다.\n",
    "\n",
    "```python\n",
    "what = [1, 0, 0, 0, 0, 0, 0]\n",
    "will = [0, 1, 0, 0, 0, 0, 0]\n",
    "the = [0, 0, 1, 0, 0, 0, 0]\n",
    "fat = [0, 0, 0, 1, 0, 0, 0]\n",
    "cat = [0, 0, 0, 0, 1, 0, 0]\n",
    "sit = [0, 0, 0, 0, 0, 1, 0]\n",
    "on = [0, 0, 0, 0, 0, 0, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZZ40PwlYzKqS"
   },
   "source": [
    "- 모든 단어가 단어 집합(vocabulary)의 크기인 7의 차원을 가지는 원-핫 벡터가 됐다.\n",
    "- 이제 이 원-핫 벡터들이 훈련을 위한 NNLM의 입력이면서 예측을 위한 레이블이기도 한다.\n",
    "- 'what will the fat cat'를 입력을 받아서 'sit'을 예측하는 일은 기계에게 실제로는 what, will, the, fat, cat의 원-핫 벡터를 입력받아 sit의 원-핫 벡터를 예측하는 문제가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gechQgLm295F"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.3.3 윈도우(window)\n",
    "\n",
    "- NNLM은 n-gram 언어 모델과 유사하게 다음 단어를 예측할 때, 앞의 모든 단어를 참고하는 것이 아니라 정해진 n개의 단어만을 참고한다.\n",
    "- 예를 들어 n을 4라고 해보자.\n",
    "- 이 때, 언어 모델은 \"what will the fat cat\"라는 단어 시퀀스가 주어졌을 때, 다음 단어를 예측하기 위해 앞의 4개 단어 \"will the fat cat\"까지만 참고하고 그 앞의 단어인 \"what\"은 무시한다.\n",
    "- 이 범위를 윈도우(window)라고 하기도 한다.\n",
    "- 여기서 윈도우의 크기 n은 4이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kb4v8ki83kff"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.3.4 NNLM의 구조\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/45609/nnlm1.PNG)\n",
    "\n",
    "- NNLM은 위의 그림과 같이 총 4개의 층(layer)으로 이루어진 인공 신경망이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-Yf2iCo3v6u"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 8.3.4.1 입력층(input layer)\n",
    "\n",
    "- 앞에서 윈도우의 크기는 4로 정하였으므로 입력은 4개의 단어 \"will, the, fat, cat\"의 원-핫 벡터이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JmPNwIDY3-ga"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 8.3.4.2 출력층(output layer)\n",
    "\n",
    "- 모델이 예측해야 하는 정답에 해당되는 단어 \"sit\"의 원-핫 벡터는 출력층에서 모델이 예측한 값의 오차를 구하기 위해 사용될 예정이다.\n",
    "- 그리고 이 오차로부터 손실 함수를 사용해 인공 신경망이 학습을 하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vk7LxuHS4UhS"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 8.3.5 NNLM 내부 메커니즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JOyTYwg_5B-K"
   },
   "source": [
    "#### 8.3.5.1 투사층(projection layer)\n",
    "\n",
    "- 4개의 원-핫 벡터를 입력 받은 NNLM은 다음층인 **투사층(projection layer)**를 지나게 된다.\n",
    "- 인공 신경망에서 입력층과 출력층 사이의 층은 보통 은닉층이라고 부른다.\n",
    "- 하지만 여기서 투사층이 일반 은닉층과 구별되는 특징은 가중치 행렬과의 연산은 이루어지지만 **활성화 함수가 존재하지 않는다는 것**이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1q6vLh1K4s5A"
   },
   "source": [
    "<br> \n",
    "\n",
    "#### 8.3.5.2 투사층의 가중치 행렬의 크기\n",
    "\n",
    "- 투사층의 크기를 M으로 설정하면, 각 입력 단어들은 투사층에서 V x M 크기의 가중치 행렬과 곱해진다.\n",
    "- 여기서 V는 단어 집합의 크기를 의미한다.\n",
    "- 만약 원-핫 벡터의 차원이 7이고, M이 5라면 가중치 행렬 W는 7 x 5 행렬이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9G2glCBt4r90"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 8.3.5.3 룩업 테이블 (lookup table)\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/45609/nnlm2_renew.PNG)\n",
    "\n",
    "- 각 단어의 원-핫 벡터와 가중치 행렬 W 행렬의 곱이 어떻게 이루어지는 지 살펴보자.\n",
    "- 위 글미에서는 각 원-핫 벡터를 $x$로 표기했다.\n",
    "- 원-핫 벡터의 특성으로 인해 i번째 인덱스에 1이라는 값을 가지고 그 외의 0의 값을 가지는 원-핫 벡터와 가중치 W 행렬의 곱은 사실 **W 행렬의 i번째 행을 그대로 읽어오는 것(lookup)과 동일**하다.\n",
    "- 그래서 이 작업을 **룩업 테이블(lookup table)**이라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y31-7MKP5wzv"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 8.3.5.4 임베딩 벡터 (embedding vector)\n",
    "\n",
    "- 이 룩업 테이블 작업을 거치면 V의 차원을 가지는 **원-핫 벡터**는 이보다 **더 차원이 작은** M 차원의 단어 벡터로 맵핑된다.\n",
    "- 위 그림에서는 \n",
    "  - 단어 'fat'을 의미하는 원-핫 벡터를 $x_{\\text{fat}}$으로 표현\n",
    "  - 테이블 룩업 과정을 거친 후의 단어 벡터는 $e_{\\text{fat}}$으로 표현\n",
    "- 이 벡터들은 초기에는 랜덤한 값을 가지지만 학습 과정에서 값이 계속 변경된다.\n",
    "- 이 단어 벡터를 **임베딩 벡터(embedding vector)**라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "70nXbEmb6WbA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch08_v08_Neural-Network-Language-Model-NNLM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
