{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tExVLlNMtbCq"
   },
   "source": [
    "# Ch08. 딥 러닝(Deep Learning) 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iAJ3FOlvtl-_"
   },
   "source": [
    "# v03. 딥 러닝의 학습 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P1ILR-ALtoUf"
   },
   "source": [
    "## 3.4 역전파 (BackPropagation) 이해하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PAlcAPaMtqmf"
   },
   "source": [
    "- 인공 신경망이 순전파 과정을 진행하여 예측값과 실제값의 오차를 계산하였을 때 어떻게 역전파 과정에서 경사 하강법을 사용하여 가중치를 업데이트하는 지 직접 계산을 통해 이해해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ye925qkVSbkX"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.1 인공 신경망의 이해 (Neural Network Overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IuX4GgdbSfa3"
   },
   "source": [
    "- 예제를 위해 사용될 인공 신경망\n",
    "  - 3개의 층을 가짐 (입력층, 은닉층, 출력층)\n",
    "  - 각 층의 뉴런의 개수는 각각 2개씩이다.\n",
    "  - 은닉층과 출력층의 모든 뉴런은 활성화 함수로 시그모이드 함수를 사용한다.\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/37406/nn1_final.PNG)\n",
    "\n",
    "- 위의 그림은 여기서 사용할 인공 신경망의 모습을 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L5AO0djCS6BW"
   },
   "source": [
    "- 은닉층과 출력층의 모든 뉴런에서 변수 $z$가 존재\n",
    "- 변수 $z$\n",
    "  - 이전 층의 모든 입력이 각각의 가중치와 곱해진 값들이 모두 더해진 가중합을 의미\n",
    "  - 이 값은 뉴런에서 아직 시그모이드 함수를 거치지 않은 상태이다.\n",
    "  - 즉, 활성화 함수의 입력을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0M0TriHNY55z"
   },
   "source": [
    "- $z$의 우측의 $|$를 지나서 존재하는 변수 $h$ 또는 $o$\n",
    "  - $z$가 시그모이드 함수를 지난 후의 값\n",
    "  - 각 뉴런의 출력값을 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G8BcL6s2ZGzI"
   },
   "source": [
    "- 이번 역전파 예제에서는 인공 신경망에 존재하는 모든 가중치 $W$에 대해서 역전파를 통해 업데이트하는 것을 목표로 한다.\n",
    "- 해당 인공 신경망은 편향 $b$는 고려하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dpgBtQagZQ_D"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.2 순전파 (Forward Propagation)\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/37406/nn2_final_final.PNG)\n",
    "\n",
    "- 주어진 값이 위의 그림과 같을 때 순전파를 진행해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OQ5lPp_Y4WC5"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.2.1 조건\n",
    "\n",
    "- 위의 그림에서 소수점 앞의 0은 생략했다. (ex. .25 == 0.25)\n",
    "- <font color=\"blue\">파란색 숫자</font> : 입력값을 의미\n",
    "- <font color=\"red\">빨간색 숫자</font> : 각 가중치의 값을 의미\n",
    "- 앞으로 진행하는 계산의 결과값은 소수점 아래 여덟번째 자리까지 반올림하여 표기한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cxQFF31hZ2IL"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.2.2 $z_1$, $z_2$\n",
    "\n",
    "- 각 입력은 입력층에서 은닉층 방향으로 향하면서 각 입력에 해당하는 가중치와 곱해진다.\n",
    "- 그리고 결과적으로 가중합으로 계산되어 은닉층 뉴런의 시그모이드 함수의 입력값이 된다.\n",
    "- $z_1$과 $z_2$는 시그모이드 함수의 입력으로 사용되는 각각의 값에 해당된다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "z_1 = W_1 x_1 + W_2 x_2 = 0.3 \\times 0.1 + 0.25 \\times 0.2 = 0.08\n",
    "$\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "z_2 = W_3 x_1 + W_4 x_2 = 0.4 \\times 0.1 + 0.35 \\times 0.2 = 0.11\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7F_yoiKi0M_t"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.2.3 $h_1$, $h_2$\n",
    "\n",
    "- $z_1$과 $z_2$는 각각의 은닉층 뉴런에서 시그모이드 함수를 지난다.\n",
    "- 시그모이드 함수가 리턴하는 결과값은 은닉층 뉴런의 최종 출력값이다.\n",
    "- 식에서는 각각 $h_1$과 $h_2$에 해당되며, 아래의 결과와 같다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "h_1 = sigmoid(z_1) = 0.51998934\n",
    "$\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "h_2 = sigmoid(z_2) = 0.52747230\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bRU8GAcx3STJ"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.2.4 $z_3$, $z_4$\n",
    "\n",
    "- $h_1$과 $h_2$, 이 두 값은 다시 출력층의 뉴런으로 향하게 된다.\n",
    "- 이 때 다시 각각의 값에 해당되는 가중치와 곱해진다.\n",
    "- 그리고 다시 가중합되어 출력층 뉴런의 시그모이드 함수의 입력값이 된다.\n",
    "- 식에서는 각각 $z_3$과 $z_4$에 해당된다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "z_3 = W_5 h_1 + W_6 h_2 = 0.45 \\times h_1 + 0.4 \\times h_2 = 0.44498412\n",
    "$\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "z_4 = W_7 h_1 + W_8 h_2 = 0.7 \\times h_1 + 0.6 \\times h_2 = 0.68047592\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fn_eKGK-3inE"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.2.5 $o_1$, $o_2$\n",
    "\n",
    "- $z_3$과 $z_4$이 출력층 뉴런에서 시그모이드 함수를 지난 값은 이 인공 신경망이 최종적으로 계산한 출력값이다.\n",
    "- 실제값을 예측하기 위한 값으로서 예측값이라고도 부른다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "o_1 = sigmoid(z_3) = 0.60944600\n",
    "$\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "o_2 = sigmoid(z_4) = 0.66384491\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RJJFFQQA4SLN"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.2.6 오차 계산\n",
    "\n",
    "- 이제 해야 할 일은 예측값과 실제값의 오차를 계산하기 위한 오차 함수를 선택하는 것이다.\n",
    "- 오차(Error)를 계산하기 위한 손실 함수(Loss function)로는 **평균 제곱 오차 MSE**를 사용한다.\n",
    "- $target$ : 실제값\n",
    "- $output$ : 순전파를 통해 나온 예측값\n",
    "- $E_{total}$ : 각 오차를 모두 더한 전체 오차\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "E_{o_1} = {1 \\over 2} \\left( target_{o_1} - output_{o_1} \\right)^2 = 0.02193381\n",
    "$\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "E_{o_2} = {1 \\over 2} \\left( target_{o_2} - output_{o_2} \\right)^2 = 0.00203809\n",
    "$\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "E_{total} = E_{o_1} + E_{o_1} = 0.02397190\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uzxmUvAX5lIJ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.3 역전파의 2가지 단계\n",
    "\n",
    "- 순전파는 입력층에서 출력층으로 향한다.\n",
    "- 역전파는 반대로 출력층에서 입력층 방향으로 계산하면서 가중치를 업데이트해 간다.\n",
    "- 출력층 바로 이전의 은닉층을 N층이라고 하자.\n",
    "- 출력층과 N층 사이의 가중치를 업데이트하는 단계 $\\rightarrow$ 역전파 1단계\n",
    "- N층과 N층 이전층 사이의 가중치를 업데이트 하는 단계 $\\rightarrow$ 역전파 2단계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S0STHC5a8JXe"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.4 역전파 1단계 (BackPropagation Step 1)\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/37406/nn3_final.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kLv_tl2zPgy8"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.4.1 역전파 1단계에서 구해야 하는 값\n",
    "\n",
    "- 역전파 1단계에서 업데이트해야 할 가중치 (4개)  \n",
    ": $W_5$, $W_6$, $W_7$, $W_8$\n",
    "- 원리 자체는 동일하므로 우선 $W_5$에 대해서 먼저 업데이트를 진행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ku-eQMWi11Z"
   },
   "source": [
    "- 경사 하강법을 수행하려면 가중치 $W_5$를 업데이트하기 위해서 $\\frac{\\partial E_{total}}{\\partial W_5}$를 계산해야 한다.\n",
    "- $\\frac{\\partial E_{total}}{\\partial W_5}$를 계산하기 위해 **미분의 연쇄 법칙(Chain rule)**에 따라서 이와 같이 풀어쓸 수 있다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "\\frac{\\partial E_{total}}{\\partial W_5} = \n",
    "\\frac{\\partial E_{total}}{\\partial o_1} \\times \n",
    "\\frac{\\partial o_1}{\\partial z_3} \\times\n",
    "\\frac{\\partial z_3}{\\partial W_5}\n",
    "$\n",
    "\n",
    "- 위의 식에서 우변의 세 개의 각 항에 대해서 순서대로 계산해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O5EuxYV28LLT"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.4.2 $E_{total}$\n",
    "\n",
    "- 미분을 진행하기 전에 $E_{total}$의 값을 상기해보자.\n",
    "- $E_{total}$은 앞서 순전파를 진행하고 계산했던 전체 오차값이다.\n",
    "- 식은 다음과 같다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "E_{total} = \n",
    "\\frac{1}{2} \\left( target_{o_1} - output_{o_1} \\right)^2 + \n",
    "\\frac{1}{2} \\left( target_{o_2} - output_{o_2} \\right)^2\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5MGBvcMTLGbh"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.4.3 우변의 첫 번째 항 : $\\frac{\\partial E_{total}}{\\partial o_1}$\n",
    "\n",
    "- 이에 $\\frac{\\partial E_{total}}{\\partial o_1}$는 다음과 같다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "\\frac{\\partial E_{total}}{\\partial o_1} = \n",
    "2 \\times \\frac{1}{2} \\left( target_{o_1} - output_{o_1} \\right)^{2-1} \\times (-1) + 0\n",
    "$\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "\\frac{\\partial E_{total}}{\\partial o_1} = \n",
    "- \\left( target_{o_1} - output_{o_1} \\right) = -(0.4 - 0.60944600) = 0.20944600\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hiTq3SERL7Mh"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.4.4 시그모이드 함수의 미분값\n",
    "\n",
    "- $o_1$이라는 값은 시그모이드 함수의 출력값이다.\n",
    "- 그런데 시그모이드 함수의 미분은 다음과 같다 : $f(x) \\times (1 - f(x))$\n",
    "- 앞으로의 계산 과정에서도 계속해서 시그모이드 함수를 미분해야 하는 상황이 생기므로 기억두면 좋다.\n",
    "- [시그모이드 함수 미분 참고 링크](https://en.wikipedia.org/wiki/Logistic_function#Derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mR3KhUqLMbPg"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.4.5 우변의 두 번째 항 : $\\frac{\\partial o_1}{\\partial z_3}$\n",
    "\n",
    "- 두 번째 항의 미분 경과는 다음과 같다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "\\frac{\\partial o_1}{\\partial z_3} = o_1 \\times (1 - o_1) = 0.60944600(1 - 0.60944600) = 0.23802157\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pnZZllJDObmC"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.4.6 우변의 세 번째 항 : $\\frac{\\partial z_3}{\\partial W_5}$\n",
    "\n",
    "- 세 번째 항은 $h_1$의 값과 동일하다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "\\frac{\\partial z_3}{\\partial W_5} = h_1 = 0.51998934\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jQQLG79gOtrZ"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.4.7 좌변 : $\\frac{\\partial E_{total}}{\\partial W_5}$\n",
    "\n",
    "- 우변의 모든 항을 계산했다.\n",
    "- 이제 이 값을 모두 곱해주면 된다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "\\frac{\\partial E_{total}}{\\partial W_5} = 0.20944600 \\times 0.23802157 \\times 0.51998934 = 0.02592286\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "InERS0LZPZgo"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.4.8 가중치 업데이트\n",
    "\n",
    "- 이제 경사 하강법을 통해 가중치를 업데이트한다.\n",
    "- 하이퍼 파라미터에 해당되는 학습률(learning rate) $\\alpha$는 0.5라고 가정한다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "{W_5}^{+} = W_5 - \\alpha \\frac{\\partial E_{total}}{\\partial W_5} = 0.45 - 0.5 \\times 0.02592286 = 0.43703857\n",
    "$\n",
    "\n",
    "- 이와 같은 원리로 ${W_6}^{+}$, ${W_7}^{+}$, ${W_8}^{+}$을 계산할 수 있다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "\\frac{\\partial E_{total}}{\\partial W_6} = \n",
    "\\frac{\\partial E_{total}}{\\partial o_1} \\times \n",
    "\\frac{\\partial o_1}{\\partial z_3} \\times\n",
    "\\frac{\\partial z_3}{\\partial W_6}\n",
    "\\; \\rightarrow \\;\n",
    "{W_6}^{+} = 0.38685205\n",
    "$\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "\\frac{\\partial E_{total}}{\\partial W_7} = \n",
    "\\frac{\\partial E_{total}}{\\partial o_2} \\times \n",
    "\\frac{\\partial o_2}{\\partial z_4} \\times\n",
    "\\frac{\\partial z_4}{\\partial W_7}\n",
    "\\; \\rightarrow \\;\n",
    "{W_7}^{+} = 0.69629578\n",
    "$\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "\\frac{\\partial E_{total}}{\\partial W_8} = \n",
    "\\frac{\\partial E_{total}}{\\partial o_2} \\times \n",
    "\\frac{\\partial o_2}{\\partial z_4} \\times\n",
    "\\frac{\\partial z_4}{\\partial W_8}\n",
    "\\; \\rightarrow \\;\n",
    "{W_8}^{+} = 0.59624247\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Axg5qySYhptA"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.5 역전파 2단계(BackPropagation Step 2)\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/37406/nn4.PNG)\n",
    "\n",
    "- 1단계를 완료했다면 이제 입력층 방향으로 이동하며 다시 계산을 이어간다.\n",
    "- 위의 그림에서 빨간색 화살표는 순전파의 정반대 방향인 역전파의 방향을 보여준다.\n",
    "- 현재 인공 신경망은 은닉층이 1개밖에 없으므로 이번 단계가 마지막 단계이다.\n",
    "- 하지만 은닉층이 더 많은 경우라면 입력층 방향으로 한 단계씩 계속해서 계산해가야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "So1MsVJoiOn5"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4.5.1 역전파 2단계에서 구해야 하는 값\n",
    "\n",
    "- 이번 단계에서 계산할 가중치는 $W_1$, $W_2$, $W_3$, $W_4$ 이다.\n",
    "- 원리 자체는 동일하므로 우선 $W_1$에 대해서 먼저 업데이트를 진행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qqSNYYPfilyx"
   },
   "source": [
    "- 경사 하강법ㅇ르 수행하려면 가중치 $W_1$를 업데이트 하기 위해서 $\\frac{\\partial E_{total}}{\\partial W_1}$를 계산해야 한다.\n",
    "- $\\frac{\\partial E_{total}}{\\partial W_1}$를 계산하기 위해 미분의 연쇄 법칙(Chain rule)에 따라서 이와 같이 풀어 쓸 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xGzLgQMrjU9b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch08_v03_04_BackPropagation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
