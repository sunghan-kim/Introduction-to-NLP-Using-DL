{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wpO3QoCzsO0I"
   },
   "source": [
    "# Ch09. 순환 신경망(Recurrent Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pi49TZY5sdmc"
   },
   "source": [
    "# v01. 순환 신경망(Recurrent Neural Network, RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A4PTR7NSsf8r"
   },
   "source": [
    "- RNN(Recurrent Neural Network)은 **시퀀스(Sequence) 모델**이다.\n",
    "- 입력과 출력을 시퀀스 단위로 처리하는 모델이다.\n",
    "- ex) 번역기\n",
    "  - 입력 : 번역하고자 하는 문장 (단어 시퀀스)\n",
    "  - 출력 : 해당되는 번역된 문장 (단어 시퀀스)\n",
    "- 이러한 시퀀스들을 처리하기 위해 고안된 모델들을 시퀀스 모델이라고 한다.\n",
    "- 그 중에서도 RNN은 딥 러닝에 있어 가장 기본적인 시퀀스 모델이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "da-FQn2wvA6I"
   },
   "source": [
    "- 뒤에서 배우는 LSTM이나 GRU 또한 근본적으로 RNN에 속한다.\n",
    "- RNN을 이해하고 이를 통해 11챕터의 텍스트 분류, 12챕터의 태깅 작업, 13챕터의 기계 번역을 이해한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x7ZXhWSkC8Tt"
   },
   "source": [
    "- cf) 용어는 비슷하지만 순환 신경망과 재귀 신경망(Recursive Neural Network)은 전혀 다른 개념이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9BFzyKVDDKC"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 1.1 순환 신경망 (Recurrent Neural Network, RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWfPRoIDGuHW"
   },
   "source": [
    "### 1.1.1 피드 포워드 신경망\n",
    "\n",
    "- 앞서 배운 신경망들은 전부 은닉층에서 활성화 함수를 지난 값은 오직 출력층 방향으로만 향했다.\n",
    "- 이와 같은 신경망들을 피드 포워드 신경망(Feed Forward Neural Network)이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "00y8LNCOEAVf"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.1.2 RNN의 특징\n",
    "\n",
    "- 그런데 그렇지 않은 신경망들이 있다.\n",
    "- RNN(Recurrent Neural Network) 또한 그 중 하나이다.\n",
    "- RNN은 은닉층의 노드에서 활성화 함수를 통해 나온 결과를 출력층 방향으로도 보내면서, 다시 은닉층 노드의 다음 계산의 입력으로 보내는 특징을 갖고 있다.\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/22886/rnn_image1_ver2.PNG)\n",
    "\n",
    "- 이를 그림으로 표현하면 위와 같다.\n",
    "- $x$ : 입력층의 입력 벡터\n",
    "- $y$ : 출력층의 출력 벡터\n",
    "- 실제로는 편향 $b$도 입력으로 존재할 수 있지만 앞으로의 그림에서는 생략한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KccVxwW1FBXP"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.1.3 셀 (Cell)\n",
    "\n",
    "- RNN에서 **은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드**를 **셀(cell)**이라고 한다.\n",
    "- 이 셀은 이전의 값을 기억하려고 하는 일종의 메모리 역할을 수행하므로 이를 **메모리 셀** 또는 **RNN 셀**이라고 표현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SjXxgAGyFRM1"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.1.4 시점 (time step)\n",
    "\n",
    "- 은닉층의 메모리 셀은 각각의 시점(time step)에서 바로 이전 시점에서의 은닉층의 메모리 셀에서 나온 값을 자신의 입력으로 사용하는 재귀적 활동을 하고 있다.\n",
    "- 앞으로는 현재 시점을 변수 t로 표현한다.\n",
    "- 이는 현재 시점 t에서의 메모리 셀이 갖고 있는 값은 과거의 메모리 셀들의 값에 영향을 받은 것임을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E30tpphCI5Ge"
   },
   "source": [
    "- 그렇다면 메모리 셀이 갖고 있는 이 값을 뭐라고 부를까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lG4hP5zNGr21"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.1.5 은닉 상태 (hidden state)\n",
    "\n",
    "- 메모리 셀이 출력층 방향으로 또는 다음 시점 t+1의 자신에게 보내는 값을 **은닉 상태(hidden state)**라고 한다.\n",
    "- 다시 말해 t 시점의 메모리 셀은 t-1 시점의 메모리 셀이 보낸 은닉 상태값을 t 시점의 은닉 상태 계산을 위한 입력값으로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_16lyg8cI3U2"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.1.6 RNN 아키텍처\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/22886/rnn_image2_ver3.PNG)\n",
    "\n",
    "- RNN을 표현할 때는 일반적으로 위의 그림에서 좌측과 같이 화살표로 사이클을 그려서 재귀 형태로 표현하기도 한다.\n",
    "- 또는 우측과 같이 사이클을 그리는 화살표 대신 여러 시점으로 펼쳐서 표현하기도 한다.\n",
    "- 두 그림은 동일한 그림으로 단지 사이클을 그리는 화살표를 사용하여 표현하였느냐, 시점의 흐름에 따라서 표현하였느냐의 차이일 뿐 둘 다 동일한 RNN을 표현하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MMD4crY6Le2u"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.1.7 RNN 층의 표현\n",
    "\n",
    "- 피드 포워드 신경망에서는 뉴런이라는 단위를 사용했다.\n",
    "- RNN에서는 뉴런이라는 단위보다는 각각의 층에서 다음과 같은 표현을 주로 사용한다.\n",
    "  - 입력층 : 입력 벡터\n",
    "  - 출력층 : 출력 벡터\n",
    "  - 은닉층 : 은닉 상태\n",
    "- 그래서 사실 위의 그림에서 회색과 초록색으로 표현한 각 네모들은 기본적으로 벡터 단위를 가정하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yKfh--HJNWJ6"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.1.8 RNN 뉴런 단위 시각화\n",
    "\n",
    "- 피드 포워드 신경망과의 차이를 비교하기 위해서 RNN을 뉴런 단위로 시각화해보자.\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/22886/rnn_image2.5.PNG)\n",
    "\n",
    "- 위의 그림은 각 층의 벡터의 차원(or 은닉 상태)이 아래와 같은 RNN이 시점이 2일 때의 모습을 보여준다.\n",
    "  - 입력층의 입력 벡터의 차원 : 4\n",
    "  - 은닉층의 은닉 상태의 크기 : 2\n",
    "  - 출력층의 출력 벡터의 차원 : 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BVE8E-uRTtDe"
   },
   "source": [
    "- 다시 말해 뉴런 단위로 해석하면 다음과 같다.\n",
    "  - 입력층의 뉴런 수 : 4\n",
    "  - 은닉층의 뉴런 수 : 2\n",
    "  - 출력층의 뉴런 수 : 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-C7PPMdGTwhb"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.1.9 입력과 출력의 길이에 따른 RNN의 다양한 형태\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/22886/rnn_image3_ver2.PNG)\n",
    "\n",
    "- RNN은 입력과 출력의 길이를 다르게 설계할 수 있으므로 다양한 용도로 사용할 수 있다.\n",
    "- 위 그림은 입력과 출력의 길이에 따라서 달라지는 RNN의 다양한 형태를 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2I4HzasVzvC"
   },
   "source": [
    "- 위 구조가 자연어 처리에서 어떻게 사용될 수 있는 지 예를 들어보자.\n",
    "- RNN 셀의 각 시점별 입, 출력의 단위는 사용자가 정의하기 나름이지만 가장 보편적인 단위는 **'단어 벡터'**이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CT6JjQ9_WHJ3"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 1.1.9.1 일대다(one-to-many) 모델\n",
    "\n",
    "- 하나의 입력에 대해서 여러 개의 출력(one-to-many)의 모델\n",
    "- 하나의 이미지 입력에 대해서 사진의 제목을 출력하는 **이미지 캡셔닝(Image Captioning)** 작업에 사용할 수 있다.\n",
    "- 사진의 제목은 단어들의 나열이므로 시퀀스 출력이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZpqPm0iCWgYZ"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 1.1.9.2 다대일(many-to-one) 모델\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/22886/rnn_image3.5.PNG)\n",
    "\n",
    "- 단어 시퀀스에 대해서 하나의 출력(many-to-one)을 하는 모델\n",
    "- 다음과 같은 경우에 사용 가능\n",
    "  - **감성 분류(sentiment classification)** : 입력 문서가 긍정적인 지 부정적인 지를 판별 \n",
    "  - **스팸 메일 분류(spam detection)** : 메일이 정상 메일인지 스팸 메일인 지 판별\n",
    "- 위 그림은 RNN으로 스팸 메일을 분류할 때의 아키텍쳐를 보여준다.\n",
    "- 이러한 예제들은 **11챕터에서 배우는 텍스트 분류**에서 배운다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_x9uVvFvXaUw"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 1.1.9.3 다대다(many-to-many) 모델\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/22886/rnn_image3.7.PNG)\n",
    "\n",
    "- 다대다(many-to-many) 모델은 다음과 같은 경우에 사용\n",
    "  - **챗봇** : 입력 문장으로 부터 대답 문장을 출력\n",
    "  - **번역기** : 입력 문장으로부터 번역된 문장을 출력\n",
    "  - **개체명 인식** (12챕터에서 학습)\n",
    "  - **품사 태깅** (12챕터에서 학습)\n",
    "- 위 그림은 개체명 인식을 수행할 때의 RNN 아키텍처를 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z2W6hD-MYHnt"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.1.10 RNN 수식 정의\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/22886/rnn_image4_ver2.PNG)\n",
    "\n",
    "- $h_t$ : 현재 시점 t에서의 은닉 상태값\n",
    "- 은닉층의 메모리 셀은 $h_t$를 계산하기 위해서 총 두 개의 가중치를 갖게 된다.\n",
    "  1. $W_x$ : 입력층에서 입력값을 위한 가중치\n",
    "  2. $W_h$ : 이전 시점 t-1의 은닉 상태값 $h_{t-1}$을 위한 가중치 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZgVlqoU-ZcmK"
   },
   "source": [
    "- 이를 식으로 표현하면 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LQdzNc72ZfVB"
   },
   "source": [
    "- 은닉층 : \n",
    "$\n",
    "\\quad\n",
    "h_t = tanh \\left( W_x \\; x_t + W_h \\; h_{t-1} + b \\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fNb5H3VkZqya"
   },
   "source": [
    "- 출력층 : \n",
    "$\n",
    "\\quad\n",
    "y_t = f \\left( W_y \\; h_t + b \\right)\n",
    "\\quad\n",
    "$\n",
    "$f$ : 비선형 활성화 함수 중 하나"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gln7-If8Z9yn"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.1.11 RNN의 은닉층 연산\n",
    "\n",
    "- RNN의 은닉층 연산을 벡터와 행렬 연산으로 이해할 수 있다.\n",
    "- 자연어 처리에서 RNN의 입력 $x_t$는 대부분의 경우 단어 벡터로 간주할 수 있다.\n",
    "- $d$와 $D_h$를 다음과 같이 정의했을 때, 각 벡터와 행렬의 크기는 다음과 같다.\n",
    "  - $d$ : 단어 벡터의 차원\n",
    "  - $D_h$ : 은닉 상태의 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4l7D72pnaVT-"
   },
   "source": [
    "- $x_t$ : $\\left( d \\times 1 \\right)$\n",
    "- $W_x$ : $\\left( D_h \\times d \\right)$\n",
    "- $W_h$ : $\\left( D_h \\times D_h \\right)$\n",
    "- $h_{t-1}$ : $\\left( D_h \\times 1 \\right)$\n",
    "- $b$ : $\\left( D_h \\times 1 \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHjJdbySaxtJ"
   },
   "source": [
    "<br>\n",
    "\n",
    "- 배치 크기가 1이고, $d$와 $D_h$ 두 값 모두를 4로 가정했을 때, RNN의 은닉층 연산을 그림으로 표현하면 아래와 같다.\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/22886/rnn_images4-5.PNG)\n",
    "\n",
    "- 이 때 $h_t$를 계산하기 위한 활성화 함수로는 주로 **하이퍼볼릭 탄젠트 함수(tanh)**가 사용된다.  \n",
    "(ReLU로 바꿔 사용하는 시도도 있다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RMi1Rg4FbHqV"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.1.12 가중치의 값\n",
    "\n",
    "- 위의 식에서 각각의 가중치 $W_x$, $W_h$, $W_y$의 값은 모든 시점에서 값을 동일하게 공유한다.\n",
    "- 만약 은닉층이 2개 이상일 경우에는 은닉층 2개의 가중치는 서로 다른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_w9D401ibX5G"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.1.13 출력층 활성화 함수\n",
    "\n",
    "- 출력층은 결과값인 $y_t$를 계산하기 위한 활성화 함수로는 상황에 따라 다르게 사용된다.\n",
    "  - 이진 분류를 해야 하는 경우 : 시그모이드 함수 사용\n",
    "  - 다양한 카테고리 중에서 선택해야 하는 문제 : 소프트맥스 함수 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-JNcycfbllp"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 1.2 케라스(Keras)로 RNN 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nmo6VVE9bqtS"
   },
   "source": [
    "### 1.2.1 케라스로 RNN 층 추가\n",
    "\n",
    "```python\n",
    "# RNN 층을 추가하는 코드\n",
    "model.add(SimpleRNN(hidden_size)) # 가장 간단한 형태\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kZPHaTu4bzRb"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.2.2 인자 사용\n",
    "\n",
    "```python\n",
    "# 추가 인자를 사용할 때\n",
    "model.add(SimpleRNN(hidden_size, input_shape=(timesteps, input_dim))\n",
    "\n",
    "# 다른 표기\n",
    "model.add(SimpleRNN(hidden_size, input_length=M, input_dim=N)) # 단, M과 N은 정수\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GpdoaTlDcHSK"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 1.2.2.1 `hidden_size`\n",
    "\n",
    "- 은닉 상태의 크기를 정의\n",
    "- 메모리 셀이 다음 시점의 메모리 셀과 출력층으로 보내는 값의 크기(`output_dim`)와도 동일\n",
    "- RNN의 용량(capacity)을 늘린다고 보면 된다.\n",
    "- 중소형 모델의 경우 보통 128, 256, 512, 1024 등의 값을 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LEbxWKFMccNy"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 1.2.2.2 `timesteps`\n",
    "\n",
    "- 입력 시퀀스의 길이(`input_length`)라고 표현하기도 한다.\n",
    "- 시점의 수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2EgV1U2cciTP"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 1.2.2.3 `input_dim`\n",
    "\n",
    "- 입력의 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5sY03jyGclh-"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.2.3 RNN 입력 텐서의 크기\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/22886/rnn_image6between7.PNG)\n",
    "\n",
    "- RNN 층은 `(batch_size, timesteps, input_dim)` 크기의 3D 텐서를 입력으로 받는다.\n",
    "- `batch_size`는 한 번에 학습하는 데이터의 개수를 말한다.\n",
    "- 다만, 이러한 표현은 사람이나 문헌에 따라서, 또는 풀고자 하는 문제에 따라서 종종 다르게 기재된다.\n",
    "- 위의 그림은 문제와 상황에 따라서 다르게 표현되는 입력 3D 텐서의 대표적인 표현들을 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MdX_DjcjsXjW"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.2.4 RNN층에 대한 코드\n",
    "\n",
    "- 헷갈리지 말아야 할 점은 위의 코드는 출력층까지 포함한 하나의 완성된 인공 신경망 코드가 아니라 은닉층, 즉 RNN 층에 대한 코드라는 점이다.\n",
    "- 해당 코드가 리턴하는 결과값은 출력층의 결과가 아니라 하나의 은닉 상태 또는 정의하기에 따라 다수의 은닉 상태이다.\n",
    "- 아래의 그림은 앞서 배운 출력층을 포함한 완성된 인공 신경망 그림과 은닉층까지만 표현한 그림의 차이를 보여준다.\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/22886/rnn_image7_ver2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3wGO8dYCtG0o"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.2.5 RNN 층의 2가지 형태의 은닉 상태 출력 방법\n",
    "\n",
    "- RNN 층은 위에서 설명한 입력 3D 텐서를 입력받아서 어떻게 은닉 상태를 출력할까?\n",
    "- RNN 층은 사용자의 설정에 따라 두 가지 종류의 출력을 내보낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNFBZYa-t6Rt"
   },
   "source": [
    "1. 메모리 셀의 최종 시점의 은닉 상태만 리턴  \n",
    "$\\rightarrow$ `(batch_size, output_dim)` 크기의 2D 텐서 리턴\n",
    "2. 메모리 셀의 각 시점(time step)의 은닉 상태값들을 모아서 전체 시퀀스를 리턴  \n",
    "$\\rightarrow$ `(batch_size, timesteps, output_dim)` 크기의 3D 텐서 리턴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "etF8bbYet_Ns"
   },
   "source": [
    "- 이는 RNN 층의 `return_sequences` 매개 변수에 `True`를 설정하여 사용이 가능하다.\n",
    "- `output_dim`은 앞서 코드에서 정의한 `hidden_size`의 값으로 설정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fZzqycWBt_zD"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.2.6 `return_sequences` 매개 변수 설정 차이\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/22886/rnn_image8_ver2.PNG)\n",
    "\n",
    "- 위의 그림은 `time_steps=3`일 때, `return_sequences=True`를 설정했을 때와 그렇지 않았을 때 어떤 차이가 있는 지를 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kdw5JBFEuN-r"
   },
   "source": [
    "- `return_sequences=True` 선택\n",
    "  - 메모리 셀이 모든 시점(time step)에 대해서 은닉 상태값을 출력\n",
    "  - 모든 시점의 은닉 상태를 전달  \n",
    "  $\\rightarrow$ 다음층이 하나 더 있는 경우  \n",
    "  $\\rightarrow$ many-to-many 문제를 풀 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_DK_w7KcvS-p"
   },
   "source": [
    "- `return_sequences=False` 선택 (또는 별도로 기재 x)\n",
    "  - 메모리 셀은 하나의 은닉 상태값만을 출력한다.\n",
    "  - 그리고 이 하나의 값은 마지막 시점(time step)의 메모리 셀의 은닉 상태 값이다.\n",
    "  - 마지막 은닉 상태만 전달  \n",
    "  $\\rightarrow$ many-to-one 문제를 풀 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "21jnr-w3vcAq"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.2.7 RNN 실습\n",
    "\n",
    "- 뒤에서 배우는 LSTM이나 GRU도 내부 메커니즘은 다르지만 `model.add()`를 통해서 층을 추가하는 코드는 사실상 `SimpleRNN` 코드와 같은 형태를 가진다.\n",
    "- 실습을 통해 모델 내부적으로 출력 결과를 어떻게 정의하는 지 보면서 RNN을 이해해보자."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch09_v01_Recurrent-Neural-Network-RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
