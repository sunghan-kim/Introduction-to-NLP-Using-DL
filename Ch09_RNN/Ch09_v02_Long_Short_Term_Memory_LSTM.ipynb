{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pXvwpZnauFO"
   },
   "source": [
    "# Ch09. 순환 신경망 (Recurrent Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HOgYfeSMa2ky"
   },
   "source": [
    "# v02. 장단기 메모리 (Long Short-Term Memory, LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P2yLJJTcbGsq"
   },
   "source": [
    "**바닐라 RNN (Vanila RNN)**\n",
    "\n",
    "- 바닐라 아이스크림이 가장 기본적인 맛을 가진 아이스크림인 것 처럼, 앞서 배운 RNN을 가장 단순한 형태의 RNN이라고 하여 **바닐라 RNN(Vanila RNN)**이라고 한다.\n",
    "- 케라스에서는 `SimpleRNN`\n",
    "- 바닐라 RNN 이후 바닐라 RNN의 한계를 극복하기 위한 다양한 RNN의 변형이 나왔다.\n",
    "- 이번 챕터에서 배우게 될 LSTM도 그 중 하나이다.\n",
    "- LSTM과 비교하여 RNN을 언급하는 것은 전부 바닐라 RNN을 말한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xa93loM9rknm"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 2.1 바닐라 RNN의 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d3fyeQTbrm2s"
   },
   "source": [
    "$\\quad$ ![](https://wikidocs.net/images/page/22888/lstm_image1_ver2.PNG)\n",
    "\n",
    "- 바닐라 RNN은 출력 결과가 이전의 계산 결과에 의존한다.\n",
    "- 하지만 바닐라 RNN은 비교적 **짧은 시퀀스(sequence)에 대해서만 효과를 보이는 단점**이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VhsNiD_40-Bc"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.1.1 장기 의존성 문제 (the problem of Long-Term Dependencies)\n",
    "\n",
    "- 바닐라 RNN의 시점(time step)이 길어질수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상이 발생한다.\n",
    "- 위의 그림은 첫 번째 입력값인 $x_1$의 정보량을 짙은 남색으로 표현했을 때, 색이 점차 얕아지는 것으로 시점이 지날수록 $x_1$의 정보량이 손실되어가는 과정을 표현했다.\n",
    "- 뒤로 갈수록 $x_1$의 정보량은 손실되고, 시점이 충분히 긴 상황에서는 $x_1$의 전체 정보에 대한 영향력은 거의 의미가 없을 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zngTCKGCxdWr"
   },
   "source": [
    "- 어쩌면 가장 중요한 정보가 시점의 앞 쪽에 위치할 수도 있다.\n",
    "- RNN으로 만든 언어 모델이 다음 단어를 예측하는 과정을 생각해보자.\n",
    "- 예를 들어 다음과 같은 문장에서 가장 마지막 단어를 예측한다고 해보자.\n",
    "\n",
    "> \"모스크바에 여행을 왔는데 건물도 예쁘고 먹을 것도 맛있었어. 그런데 글쎄 직장 상사한테 전화가 왔어. 어디냐고 묻더라구 그래서 나는 말했지. 저 여행왔는데요. 여기 ___\"\n",
    "\n",
    "- 다음 단어를 예측하기 위해서는 장소 정보가 필요하다.\n",
    "- 그런데 장소 정보에 해당되는 단어인 '모스크바'는 앞에 위치하고 있고, RNN이 충분한 기억력을 가지고 있지 못한다면 다음 단어를 엉뚱하게 예측한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8j96TXXK09QD"
   },
   "source": [
    "- 이를 **장기 의존성 문제(the problem of Long-Term Dependencies)**라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4tRYYS6-1MlC"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 2.2 바닐라 RNN 내부 열어보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N9uVLGqX1Wyt"
   },
   "source": [
    "$\\quad$ ![](https://wikidocs.net/images/page/22888/vanilla_rnn_ver2.PNG)\n",
    "\n",
    "- LSTM에 대해서 이해해보기 전에 바닐라 RNN의 뚜껑을 열어보자.\n",
    "- 위의 그림은 바닐라 RNN의 내부 구조를 보여준다.\n",
    "- 이 책에서는 RNN 계열의 인공 신경망의 그림에서는 편향 $b$를 생략한다.\n",
    "- 위의 그림에 편향 $b$를 그린다면 $x_t$ 옆에 tanh로 향하는 또 하나의 입력선을 그리면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OGLWQLJv2dzC"
   },
   "source": [
    "$\n",
    "\\qquad\n",
    "h_t = tanh \\left( W_x \\, x_t + W_h \\, h_{t-1} + b \\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n8qVSiOp3bxe"
   },
   "source": [
    "- 바닐라 RNN은 $x_t$와 $h_{t-1}$이라는 두 개의 입력이 각각 가중치와 곱해져서 메모리 셀의 입력이 된다.\n",
    "- 그리고 이를 하이퍼볼릭 탄젠트 함수의 입력으로 사용하고 이 값은 은닉층의 출력인 은닉 상태가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Or6cN2_u3uLe"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 2.3 LSTM (Long Short-Term Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "md8CXpek9UMJ"
   },
   "source": [
    "### 2.3.1 LSTM의 내부 구조\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/22888/vaniila_rnn_and_different_lstm_ver2.PNG)\n",
    "\n",
    "- 위의 그림은 LSTM의 전체적인 내부의 모습을 보여준다.\n",
    "- 전통적인 RNN의 이러한 단점을 보완한 RNN의 일종을 **장단기 메모리(Long Short-Term Memory)**라고 하며, 줄여서 LSTM이라고 한다.\n",
    "- LSTM은 은닉층의 메모리 셀에 다음 3가지 게이트를 추가하여 불필요한 기억을 지우고, 기억해야 할 것을 정한다.\n",
    "  - 입력 게이트\n",
    "  - 망각 게이트\n",
    "  - 출력 게이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D1GbGWfH83hQ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.3.2 셀 상태 (cell state)\n",
    "\n",
    "- 요약하면 LSTM은 은닉 상태(hidden state)를 계산하는 식이 전통적인 RNN보다 조금 더 복잡해졌으며 **셀 상태(cell state)**라는 값을 추가하였다.\n",
    "- 위의 그림에서는 t시점의 셀 상태를 $C_t$로 표현하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zBYyM_259Fn0"
   },
   "source": [
    "- LSTM은 RNN과 비교하여 긴 시퀀스의 입력을 처리하는 데 탁월한 성능을 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYTYw3PN9PUs"
   },
   "source": [
    "$\\quad$ ![](https://wikidocs.net/images/page/22888/cellstate.PNG)\n",
    "\n",
    "- 셀 상태는 위의 그림에서 왼쪽에서 오른쪽으로 가는 굵은 선이다.\n",
    "- 셀 상태 또한 이전에 배운 은닉 상태처럼 이전 시점의 셀 상태가 다음 시점의 셀 상태를 구하기 위한 입력으로서 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XpofMatSIewm"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.3.3 삭제 게이트, 입력 게이트, 출력 게이트\n",
    "\n",
    "- 은닉 상태값과 셀 상태값을 구하기 위해서 새로 추가된 3개의 게이트를 사용한다.\n",
    "- 각 게이트는 다음과 같다.\n",
    "  - 삭제 게이트\n",
    "  - 입력 게이트\n",
    "  - 출력 게이트\n",
    "- 이 3개의 게이트에는 공통적으로 시그모이드 함수가 존재한다.\n",
    "- 시그모이드 함수를 지나면 0과 1 사이의 값이 나오게 된다.\n",
    "- 이 값들을 가지고 게이트를 조절한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ysk4Ni2ZI9zg"
   },
   "source": [
    "- 아래의 내용을 먼저 이해하고 각 게이트에 대해서 알아보도록 한다.\n",
    "  - $\\sigma$ : 시그모이드 함수를 의미\n",
    "  - tanh : 하이퍼볼릭 탄젠트 함수를 의미\n",
    "  - $W_{xi}$, $W_{xg}$, $W_{xf}$, $W_{xo}$ : $x_t$와 함께 각 게이트에서 사용되는 4개의 가중치\n",
    "  - $W_{hi}$, $W_{hg}$, $W_{hf}$, $W_{ho}$ : $h_{t-1}$와 함께 각 게이트에서 사용되는 4개의 가중치\n",
    "  - $b_i$, $b_g$, $b_f$, $b_o$ : 각 게이트에서 사용되는 4개의 편향"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcpmPC5UJxIP"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 2.3.3.1 입력 게이트\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/22888/inputgate.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ty0I6rRGKGe8"
   },
   "source": [
    "$\n",
    "\\qquad\n",
    "i_t = \\sigma \\left( W_{xi} \\, x_t + W_{hi} \\, h_{t-1} + b_i \\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TRDO1IBGLESS"
   },
   "source": [
    "$\n",
    "\\qquad\n",
    "g_t = tanh \\left( W_{xg} \\, x_t + W_{hg} h_{t-1} + b_g \\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iTqzoffMLPJy"
   },
   "source": [
    "- 입력 게이트는 **현재 정보를 기억**하기 위한 게이트이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SA349b8QLcHK"
   },
   "source": [
    "- $i_t$\n",
    "  - 현재 시점 t의 $x$값과 입력 게이트로 이어지는 가중치 $W_{xi}$를 곱한 값\n",
    "  - 이전 시점 t-1의 은닉 상태가 입력 게이트로 이어지는 $W_{hi}$를 곱한 값\n",
    "  - 위의 2개의 값을 더하여 **시그모이드 함수**를 지난다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vEquwzTJLt8R"
   },
   "source": [
    "- $g_t$\n",
    "  - 현재 시점 t의 $x$값과 입력 게이트로 이어지는 가중치 $W_{xg}$를 곱한 값\n",
    "  - 이전 시점 t-1의 은닉 상태가 입력 게이트로 이어지는 가중치 $W_{hg}$를 곱한 값\n",
    "  - 위의 2개의 값을 더하여 **하이퍼볼릭 탄젠트 함수**를 지난다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rUVALlWfMNEv"
   },
   "source": [
    "- 입력 게이트 수행 결과 다음 두 개의 값이 나온다.\n",
    "  - 시그모이드 함수를 지나 0과 1 사이의 값\n",
    "  - 하이퍼볼릭 탄젠트 함수를 지나 -1과 1 사이의 값\n",
    "- 이 두 개의 값을 가지고 이번에 선택된 기억할 정보의 양을 정한다.\n",
    "- 구체적으로 어떻게 결정하는 지는 아래에서 배우게 될 **셀 상태 수식**을 보면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UN1ZqgF4MkRp"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 2.3.3.2 삭제 게이트\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/22888/forgetgate.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_cNVOd38MrkH"
   },
   "source": [
    "$\n",
    "\\qquad\n",
    "f_t = \\sigma \\left( W_{xf} \\, x_t + X_{hf} \\, h_{t-1} + b_f \\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HyPpM240M0r_"
   },
   "source": [
    "- 삭제 게이트는 **기억을 삭제하기 위한 게이트**이다.\n",
    "- 현재 시점 t의 $x$ 값과 이전 시점 t-1의 은닉 상태가 시그모이드 함수를 지나게 된다.\n",
    "- 시그모이드 함수를 지나면 0과 1 사이의 값이 나온다.\n",
    "- 이 값이 곧 **삭제 과정을 거친 정보의 양**이다.\n",
    "   - 0에 가깝다 : 정보가 많이 삭제된 것\n",
    "   - 1에 가깝다 : 정보를 온전히 기억한 것\n",
    "- 이를 가지고 셀 상태를 구하게 되는 데, 구체적으로는 아래의 셀 상태 수식을 보면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sVsOnbG8NRj4"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 2.3.3.3 셀 상태 (장기 상태)\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/22888/cellstate2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hud0_hlCNX1u"
   },
   "source": [
    "$\n",
    "\\qquad\n",
    "C_{t}=f_{t} \\circ C_{t-1} + i_{t} \\circ g_{t}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UFsSBGasNcc-"
   },
   "source": [
    "- 셀 상태 $C_t$를 LSTM에서는 **장기 상태**라고 부르기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iy-cjaxTNlCu"
   },
   "source": [
    "<br>\n",
    "\n",
    "**셀 상태를 구하는 방법 (삭제 게이트에서 일부 기억을 잃은 상태)**\n",
    "\n",
    "- 입력 게이트에서 구한 $i_t$, $g_t$ 두 개의 값에 대해서 **원소별 곱(entrywise product)**을 진행한다.\n",
    "- 다시 말해, 같은 크기의 두 행렬이 있을 때 같은 위치의 성분끼리 곱하는 것을 말한다.\n",
    "- 여기서는 식으로 $\\circ$ 로 표현한다.\n",
    "- 이것이 이번에 선택된 기억할 값이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wonvatgGNpg2"
   },
   "source": [
    "- 입력 게이트에서 선택된 기억을 삭제 게이트의 결과값과 더한다.\n",
    "- 이 값을 현재 시점 t의 셀 상태라고 한다.\n",
    "- 이 값은 다음 t+1 시점의 LSTM 셀로 넘겨진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XZz5sHSiQZ5Q"
   },
   "source": [
    "<br>\n",
    "\n",
    "**삭제 게이트와 입력 게이트의 영향력**\n",
    "\n",
    "- 만약 삭제 게이트의 출력값인 $f_t$가 0이 된다  \n",
    "$\\rightarrow$ 이전 시점의 셀 상태값인 $C_{t-1}$은 현재 시점의 셀 상태값을 결정하기 위한 영향력이 0이 된다.  \n",
    "$\\rightarrow$ 오직 입력 게이트의 결과만이 현재 시점의 셀 상태값 $C_t$을 결정할 수 있다.  \n",
    "$\\rightarrow$ 이는 삭제 게이트가 완전히 닫히고 입력 게이트를 연 상태를 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_vxbVDZwRkb4"
   },
   "source": [
    "- 입력 게이트의 $i_t$ 값이 0이 된다.  \n",
    "$\\rightarrow$ 현재 시점의 셀 상태값 $C_t$는 오직 이전 시점의 셀 상태값 $C_{t-1}$의 값에만 의존한다.  \n",
    "$\\rightarrow$ 이는 입력 게이트를 완전히 닫고 삭제 게이트만을 연 상태를 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6yyiWvV4R0Rw"
   },
   "source": [
    "- 결과적으로\n",
    "  - 삭제 게이트 : 이전 시점의 입력을 얼마나 반영할 지를 결정\n",
    "  - 입력 게이트 : 현재 시점의 입력을 얼마나 반영할 지를 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdVQUjy2R-bx"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 2.3.3.4 출력 게이트와 은닉 상태 (단기 상태)\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/22888/outputgateandhiddenstate.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_WYBC868SIp-"
   },
   "source": [
    "$\n",
    "\\qquad\n",
    "o_t = \\sigma \\left( W_{xo} \\, x_t + W_{ho} \\, h_{t-1} + b_o \\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PFr4_VU3S9Vs"
   },
   "source": [
    "$\n",
    "\\qquad\n",
    "h_t = o_t \\, \\circ \\, tanh(C_t)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_N1LlaKTYn8"
   },
   "source": [
    "- 출력 게이트는 현재 시점 t의 $x$값과 이전 시점 t-1의 은닉 상태가 시그모이드 함수를 지난 값이다.\n",
    "- 해당 값은 현재 시점 t의 은닉 상태를 결정하는 일에 쓰인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-evYW4oTi40"
   },
   "source": [
    "- 은닉 상태를 **단기 상태**라고 하기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z7B-jEU0Tlkk"
   },
   "source": [
    "- 은닉 상태는 장기 상태의 값이 하이퍼볼릭 탄젠트 함수를 지나 -1과 1 사이의 값이다.\n",
    "- 해당 값은 출력 게이트의 값과 연산되면서, 값이 걸러지는 효과가 발생한다.\n",
    "- 단기 상태의 값은 또한 출력층으로 향한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ISBTJUfTxT0"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 2.4 참고 자료\n",
    "\n",
    "- [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [https://www.quora.com/In-LSTM-how-do-you-figure-out-what-size-the-weights-are-supposed-to-be](https://www.quora.com/In-LSTM-how-do-you-figure-out-what-size-the-weights-are-supposed-to-be)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch09_v02_Long-Short-Term-Memory-LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
