{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eiqZm0cM5yZe"
   },
   "source": [
    "# Ch09. 순환 신경망 (Recurrent Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "31Ydhuys553C"
   },
   "source": [
    "# v04. RNN 언어 모델 (Recurrent Neural Network Language Model, RNNLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CyzWGBb26BFe"
   },
   "source": [
    "- 이번 챕터에서는 RNN을 이용하여 언어 모델을 구현한 RNN 언어 모델에 대해서 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OUvxdu6a6FFh"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 4.1 RNN 언어 모델 (Recurrent Neural Network Language Model, RNNLM)\n",
    "\n",
    "- n-gram 언어 모델과 NNLM은 고정된 개수의 단어만을 입력으로 받아야 한다는 단점이 있었다.\n",
    "- 하지만 시점(time step)이라는 개념이 도입된 RNN으로 언어 모델을 만들면 입력의 길이를 고정하지 않을 수 있다.\n",
    "- 이처럼 RNN으로 만든 언어 모델을 RNNLM(Recurrent Neural Network Language Model)이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SXFvke3E-Zu_"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 4.1.1 RNNLM 언어 모델링 학습 과정 (테스트 과정)\n",
    "\n",
    "- RNNLM이 언어 모델링을 학습하는 과정을 보자.\n",
    "- 이해를 위해 매우 간소화된 형태로 설명한다.\n",
    "\n",
    "> \"what will the fat cat sit on\"\n",
    "\n",
    "- 예를 들어 훈련 코퍼스에 위와 같은 문장이 있다고 하자.\n",
    "- 언어 모델은 주어진 단어 시퀀스로부터 다음 단어를 예측하는 모델이다.\n",
    "- 아래의 그림은 RNNLM이 어떻게 이전 시점의 단어들과 현재 시점의 단어로 다음 단어를 예측하는 지를 보여준다.\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/46496/rnnlm1_final_final.PNG)\n",
    "\n",
    "- RNNLM은 기본적으로 예측 과정에서 이전 시점의 출력을 현재 시점의 입력으로 한다.\n",
    "  - \"what\"을 입력 받으면, \"will\"을 예측\n",
    "  - 이 \"will\"은 다음 시점의 입력이 되어 \"the\"를 예측한다.\n",
    "  - 그리고 \"the\"는 또 다시 다음 시점의 입력이 되고 해당 시점에서는 \"fat\"을 예측한다.\n",
    "  - 그리고 이 또한 다시 다음 시점의 입력이 된다.\n",
    "- 결과적으로 세 번째 시점에서 \"fat\"은 앞서 나온 \"what\", \"will\", \"the\" 라는 시퀀스로 인해 결정된 단어이다.\n",
    "- 네 번째 시점의 \"cat\"은 앞서 나온 \"what\", \"will\", \"the\", \"fat\"이라는 시퀀스로 인해 결정된 단어이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mkXn5hU_CSaO"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 4.1.2 훈련 과정에서 발생하는 일\n",
    "\n",
    "- 사실 위 과정은 훈련이 끝난 모델의 **테스트 과정 동안(실제 사용할 때)**의 이야기이다.\n",
    "- 훈련 과정에서는 이전 시점의 예측 결과를 다음 시점의 입력으로 넣으면서 예측하는 것이 아니라\n",
    "  - \"what will the fat cat sit on\"라는 훈련 샘플이 있으면\n",
    "  - \"what will the fat cat sit\" 시퀀스를 모델의 입력으로 넣고\n",
    "  - \"will the fat cat sit on\"을 예측하도록 훈련된다.\n",
    "  - 'will\", \"the\", \"fat\", \"cat\", \"sit\", \"on\"는 각 시점의 레이블이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1sr6E1jNEgSE"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 4.1.3 교사 강요 (teacher forcing)\n",
    "\n",
    "- 이러한 RNN 훈련 기법을 **교사 강요(teacher forcing)**라고 한다.\n",
    "- 교사 강요(teacher forcing)란, 테스트 과정에서 t 시점의 출력이 t+1 시점의 입력으로 사용되는 RNN 모델을 훈련시킬 때 사용하는 훈련 기법이다.\n",
    "- 훈련할 때 교사 강요를 사용할 경우, 모델이 t 시점에서 예측한 값을 t+1 시점에 입력으로 사용하지 않고, **t 시점의 레이블, 즉 실제 알고 있는 정답을 t+1 시점의 입력으로 사용**한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ayl51kpAFKcA"
   },
   "source": [
    "- 물론, 훈련 과정에서도 이전 시점의 출력을 다음 시점의 입력으로 사용하면서 훈련 시킬 수 있다.\n",
    "- 하지만 이는 한 번 잘못 예측하면 뒤에서의 예측까지 영향을 미쳐 훈련 시간이 느려지게 되므로 교사 강요를 사용하여 RNN을 좀 더 빠르고 효과적으로 훈련시킬 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ibdjG5j4x_fU"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 4.1.4 활성화 함수 및 손실 함수\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/46496/rnnlm2_final_final.PNG)\n",
    "\n",
    "- 훈련 과정 동안 출력층에서 사용하는 **활성화 함수**는 **소프트맥스 함수**이다.\n",
    "- 그리고 모델이 예측한 값과 실제 레이블과의 오차를 계산하기 위해서 **손실 함수**로 **크로스 엔트로피 함수**를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HtVQqt9Fyet2"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 4.1.5 RNNLM 구조\n",
    "\n",
    "- 이해를 돕기 위해 앞서 배운 NNLM의 그림과 유사한 형태로 RNNLM을 다시 시각화해보자.\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/46496/rnnlm3_final.PNG)\n",
    "\n",
    "- RNNLM은 위의 그림과 같이 총 4개의 층(layer)으로 이루어진 인공 신경망이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_xirOFMtyzuz"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.1.5.1 입력층 (input layer)\n",
    "\n",
    "- RNNLM의 현 시점(time step)은 4로 가정한다.\n",
    "- 그래서 4번 째 입력 단어인 \"fat\"의 원-핫 벡터가 입력이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "At9rgc4NzNTL"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.1.5.2 임베딩층 (embedding layer)\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/46496/rnnlm4_final.PNG)\n",
    "\n",
    "- 현 시점의 입력 단어의 원-핫 벡터 $x_t$를 입력 받은 RNNLM은 우선 임베딩층(embedding layer)을 지난다.\n",
    "- 이 임베딩층은 기본적으로 NNLM 챕터에서 배운 **투사층(projection layer)**이다.\n",
    "- NNLM 챕터에서는 룩업 테이블을 수행하는 층을 투사층이라고 표현했다.\n",
    "- 하지만 이미 투사층의 결과로 얻는 벡터를 **임베딩 벡터**라고 부른다고 NNLM 챕터에서 학습했으므로, 앞으로는 임베딩 벡터를 얻는 투사층을 **임베딩층(embedding layer)**이라는 표현을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "okV5fbXp8muN"
   },
   "source": [
    "- 단어 집합의 크기가 V일 때, 임베딩 벡터의 크기를 M으로 설정하면, 각 입력 단어들은 임베딩층에서 V x M 크기의 임베딩 행렬과 곱해진다.\n",
    "- 만약 원-핫 벡터의 차원이 7(V= 7)이고, M이 5라면 임베딩 행렬은 7 x 5 행렬이 된다.\n",
    "- 그리고 이 임베딩 행렬은 역전파 과정에서 다른 가중치들과 함께 학습 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xAhR9C6387H5"
   },
   "source": [
    "- 임베딩층 : $\\quad e_t = lookup(x_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uh07pe139DBJ"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.1.5.3 은닉층 (hidden layer)\n",
    "\n",
    "- 여기서부터는 다시 RNN을 복습하는 것과 같다.\n",
    "- 이 임베딩 벡터는 은닉층에서 이전 시점의 은닉 상태인 $h_{t-1}$과 함께 다음의 연산을 하여 현재 시점의 은닉 상태 $h_t$를 계산하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mym8K_mf9phA"
   },
   "source": [
    "- 은닉층 : $\\quad h_t = tanh(W_x \\, e_t + W_h \\, h_{t-1} + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KzGfE2jo9wPw"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.1.5.4 출력층 (output layer)\n",
    "\n",
    "- 모델이 예측해야 하는 정답에 해당되는 단어 \"cat\"의 원-핫 벡터는 출력층에서 모델이 예측한 값의 오차를 구하기 위해 사용될 예정이다.\n",
    "- 그리고 이 오차로부터 손실 함수를 사용해 인공 신경망이 학습을 하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56_Bzkle91IQ"
   },
   "source": [
    "- 출력층에서는 활성화 함수로 소프트맥스(softmax) 함수를 사용한다.\n",
    "- V차원의 벡터는 소프트맥스 함수를 지나면서 각 원소는 0과 1 사이의 실수값을 가지며 총 합은 1이 되는 상태로 바뀐다.\n",
    "- 이렇게 나온 벡터를 RNNLM의 t시점의 예측값이라는 의미에서 $\\hat{y_t}$라고 하자.\n",
    "- 이를 식으로 표현하면 아래와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Mr_fhcY-LFF"
   },
   "source": [
    "- 출력층 : $\\quad \\hat{y_t} = softmax(W_y \\, h_t + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MG8gqmmc-Ypv"
   },
   "source": [
    "<br>\n",
    "\n",
    "**벡터 $\\hat{y_t}$의 각 차원 안에서의 값이 의미하는 것**\n",
    "\n",
    "- $\\hat{y_t}$의 j번째 인덱스가 가진 0과 1 사이의 값은 j번째 단어가 다음 단어일 확률을 나타낸다.\n",
    "- $\\hat{y_t}$는 실제값, 즉 실제 정답에 해당되는 단어인 원-핫 벡터의 값에 가까워져야 한다.\n",
    "- 실제값에 해당되는 다음 단어를 $y$라고 했을 때, 이 두 벡터가 가까워지게 하기 위해서 RNNLM은 손실 함수로 cross-entropy 함수를 사용한다.\n",
    "- 그리고 역전파가 이루어지면서 가중치 행렬들이 학습된다.\n",
    "- 이 과정에서 임베딩 벡터값들도 학습이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yrta6Q05-5MH"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.1.5.5 RNNLM의 가중치 행렬들\n",
    "\n",
    "- 룩업 테이블의 대상이 되는 테이블인 임베딩 행렬을 $E$라고 하자\n",
    "- 이 때 결과적으로 RNNLM에서 학습 과정에서 학습되는 가중치 행렬은 다음 4개 이다.\n",
    "  - $E$\n",
    "  - $W_x$\n",
    "  - $W_h$\n",
    "  - $W_y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K13pq9sA_Qw4"
   },
   "source": [
    "- 뒤의 글자 단위 RNN 챕터에서 RNN 언어 모델을 구현해보면서 훈련 과정과 테스트 과정의 차이를 이해해본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Nxd2HF0_RVN"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 4.2 참고 자료\n",
    "\n",
    "- [https://docs.chainer.org/en/stable/examples/ptb.html](https://docs.chainer.org/en/stable/examples/ptb.html)\n",
    "- [https://www.d2l.ai/chapter_recurrent-neural-networks/rnn.html](https://www.d2l.ai/chapter_recurrent-neural-networks/rnn.html)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch09_v04_Recurrent-Neural-Network-Language-Model-RNNLM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
