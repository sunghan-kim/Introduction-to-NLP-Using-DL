{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cppcMvzM9tLg"
   },
   "source": [
    "# Ch10. 워드 임베딩 (Word Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oDJNAg7M98jC"
   },
   "source": [
    "# v03. 영어/한국어 Word2Vec 실습\n",
    "\n",
    "- 이번 챕터에서는 영어와 한국어 훈련 데이터에 대해서 Word2Vec을 학습해본다.\n",
    "- `gensim` 패키지에서 Word2Vec은 이미 구현되어져 있으므로, 별도로 Word2Vec을 구현할 필요 없이 손쉽게 훈련시킬 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eu6L3Zk__Hco"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 3.1 영어 Word2Vec 만들기\n",
    "\n",
    "- 이번에는 영어 데이터를 다운로드 받아 직접 Word2Vec 작업을 진행해본다.\n",
    "- 파이썬의 `gensim` 패키지에는 Word2Vec을 지원하고 있어, `gensim` 패키지를 이용하면 손쉽게 단어를 임베딩 벡터로 변환시킬 수 있다.\n",
    "- 영어로 된 코퍼스를 다운받아 전처리를 수행하고, 전처리한 데이터를 바탕으로 Word2Vec 작업을 진행해본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-EfZM87U_bu_"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.1.1 훈련 데이터 이해하기\n",
    "\n",
    "- [링크](https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip)\n",
    "- 위 링크에서 훈련 데이터를 다운로드 받을 수 있다.\n",
    "- zip 파일의 압축을 풀면 `ted_en-20160408.xml` 파일이 있는 데, 이를 훈련 데이터로 사용할 예정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l9wfIB6Q_p6v"
   },
   "source": [
    "- 아래는 해당 xml 파일의 형식을 보여준다.\n",
    "\n",
    "```xml\n",
    "<file id=\"1\">\n",
    "  <head>\n",
    "<url>http://www.ted.com/talks/knut_haanaes_two_reasons_companies_fail_and_how_to_avoid_them</url>\n",
    "       <pagesize>72832</pagesize>\n",
    "... xml 문법 중략 ...\n",
    "<content>\n",
    "Here are two reasons companies fail: they only do more of the same, or they only do what's new.\n",
    "To me the real, real solution to quality growth is figuring out the balance between two activities:\n",
    "... content 내용 중략 ...\n",
    "To me, the irony about the Facit story is hearing about the Facit engineers, who had bought cheap, small electronic calculators in Japan that they used to double-check their calculators.\n",
    "(Laughter)\n",
    "... content 내용 중략 ...\n",
    "(Applause)\n",
    "</content>\n",
    "</file>\n",
    "<file id=\"2\">\n",
    "    <head>\n",
    "<url>http://www.ted.com/talks/lisa_nip_how_humans_could_evolve_to_survive_in_space<url>\n",
    "... 이하 중략 ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pyMSkeM7UFGN"
   },
   "source": [
    "- xml 문법으로 작성되어져 있는 데이터이다.\n",
    "- 그러므로 자연어를 얻기 위해서는 전처리가 필요하다.\n",
    "- 얻고자 하는 실질적인 데이터는 영어 문장으로만 구성된 `<content>`와 `</content>` 사이의 내용이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OmdLEgSTVqS0"
   },
   "source": [
    "- 해야할 일\n",
    "  1. 전처리 작업을 통해 xml 문법들을 제거\n",
    "  2. `<content>`와 `</content>` 사이 내용 중 (Laughter)나 (Applause)와 같은 배경음을 나타내는 단어들을 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cJKuE8chV6OB"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.1.2 데이터 로드 및 전처리하기\n",
    "\n",
    "- 데이터를 다운로드 하고, xml 파일로부터 필요한 내용만을 가져와 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2VwXejNIWBCx"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from lxml import etree\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YOUtvuJdWOTi"
   },
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip\", filename=\"ted_en-20160408.zip\")\n",
    "\n",
    "# 데이터 다운로드\n",
    "with zipfile.ZipFile('ted_en-20160408.zip', 'r') as z:\n",
    "    target_text = etree.parse(z.open('ted_en-20160408.xml', 'r'))\n",
    "\n",
    "    # xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\n",
    "    parse_text = '\\n'.join(target_text.xpath('//content/text()'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WV6VOjajWlwo"
   },
   "source": [
    "<br>\n",
    "\n",
    "- 로드한 데이터 중 300개의 글자(character)만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "t2fsUKiGWvAV",
    "outputId": "bf865155-5ed6-4774-be1d-3796f566fee3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here are two reasons companies fail: they only do more of the same, or they only do what's new.\\nTo me the real, real solution to quality growth is figuring out the balance between two activities: exploration and exploitation. Both are necessary, but it can be too much of a good thing.\\nConsider Facit\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_text[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gXsA5M1XWwcA"
   },
   "source": [
    "<br>\n",
    "\n",
    "- 전처리 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SRo0VwUaWzqT",
    "outputId": "d0ee1358-0b70-4e74-d9ed-2d14513a50c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here are two reasons companies fail: they only do more of the same, or they only do what's new.\\nTo me the real, real solution to quality growth is figuring out the balance between two activities: expl\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거\n",
    "# 해당 코드는 괄호로 구성된 내용을 제거\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "\n",
    "content_text[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "T0i7fbutb36Z",
    "outputId": "8cf2e1a5-783e-4b3a-9fa7-79f17a6446e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shkim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "OswN98kIbpd9",
    "outputId": "9e8d008d-c780-4d98-a607-7507ddfb2cf9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Here are two reasons companies fail: they only do more of the same, or they only do what's new.\",\n",
       " 'To me the real, real solution to quality growth is figuring out the balance between two activities: exploration and exploitation.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행\n",
    "sent_text = sent_tokenize(content_text)\n",
    "\n",
    "sent_text[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "KeMQQgICb1mU",
    "outputId": "6a23f8b0-cb63-4c84-9ec9-f6795250764c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here are two reasons companies fail they only do more of the same or they only do what s new ',\n",
       " 'to me the real real solution to quality growth is figuring out the balance between two activities exploration and exploitation ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "    normalized_text.append(tokens)\n",
    "\n",
    "normalized_text[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZvNlGzt1dJb7"
   },
   "outputs": [],
   "source": [
    "# 각 문장에 대해서 NLTK를 이용해 단어 토큰화 수행\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SeIXd7d7dVmx"
   },
   "source": [
    "<br>\n",
    "\n",
    "- 전처리 수행 후의 총 샘플의 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Nu9-5FVvdta0",
    "outputId": "3442e47b-3b24-41ee-b956-8ceb496692da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플의 개수 : 273424\n"
     ]
    }
   ],
   "source": [
    "print('총 샘플의 개수 : {}'.format(len(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cDXL53v1dwaQ"
   },
   "source": [
    "<br>\n",
    "\n",
    "- 샘플 3개만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "Q7gx15Modzzj",
    "outputId": "3c2ba2f8-4196-41eb-a036-20c35d3f864f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
      "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
      "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n"
     ]
    }
   ],
   "source": [
    "for line in result[:3]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KwWupPWId3XY"
   },
   "source": [
    "- 상위 3개 문장 출력 결과 토큰화가 수행되었음을 볼 수 있다.\n",
    "- 이제 Word2Vec 모델에 텍스트 데이터를 훈련시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGEHHAlZeBO3"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.1.3 Word2Vec 훈련시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uxMne_60eGL8"
   },
   "source": [
    "#### 3.1.3.1 Word2Vec의 하이퍼파라미터\n",
    "\n",
    "- `size`\n",
    "  - 워드 벡터의 특징 값\n",
    "  - 즉, 임베딩된 벡터의 차원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jxkbosUFeR67"
   },
   "source": [
    "- `window`\n",
    "  - 컨텍스트 윈도우 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rd6W3w0leVRq"
   },
   "source": [
    "- `min_count`\n",
    "  - 단어 최소 빈도수 제한 (빈도가 적은 단어들은 학습하지 않는다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e4oaItLgea_2"
   },
   "source": [
    "- `workers`\n",
    "  - 학습을 위한 프로세스 수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dG2B50tZefEa"
   },
   "source": [
    "- `sg`\n",
    "  - `sg=0` : CBOW\n",
    "  - `sg=1` : Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J9Qpn0XRevBW"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=result,\n",
    "                 size=100,\n",
    "                 window=5,\n",
    "                 min_count=5,\n",
    "                 workers=4,\n",
    "                 sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6XuofT8e-eM"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.1.3.2 유사한 단어 출력\n",
    "\n",
    "- Word2Vec는 입력한 단어에 대해서 가장 유사한 단어들을 출력하는 `model.wv.most_similar`을 지원한다.\n",
    "- \"man\"과 가장 유사한 단어들을 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "5-nTasg4fMVU",
    "outputId": "7f81306c-d0bd-4445-a27d-db37af20fb35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8301703333854675), ('guy', 0.8020456433296204), ('lady', 0.7743570804595947), ('boy', 0.7733756303787231), ('girl', 0.7600075602531433), ('soldier', 0.7195061445236206), ('gentleman', 0.7169319987297058), ('kid', 0.6965048313140869), ('rabbi', 0.6876524686813354), ('poet', 0.6633204221725464)]\n"
     ]
    }
   ],
   "source": [
    "model_result = model.wv.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZpqGZ128fQ6N"
   },
   "source": [
    "- man과 유사한 단어로 woman, guy, boy, lady, girl, gentleman, soldier, kid 등을 출력하는 것을 볼 수 있다.\n",
    "- 이제 Word2Vec를 통해 단어의 유사도를 계산할 수 있게 됐다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FT6L4gXbfXus"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.1.4 Word2Vec 모델 저장하고 로드하기\n",
    "\n",
    "- 공들여 학습한 모델을 언제든 나중에 다시 사용할 수 있도록 컴퓨터 파일로 저장하고 다시 로드해보자.\n",
    "- 이 모델을 가지고 시각화 챕터에서 시각화를 진행할 예정이므로 꼭 저장해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JgTrkooOgwJW"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.1.4.1 Word2Vec 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "gNdlb0XtgbMy",
    "outputId": "3692c949-a9c9-4477-db89-64885afd0080"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# 모델 저장\n",
    "model.wv.save_word2vec_format('eng_w2v') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rXQ5dmHkgiUh"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.1.4.2 Word2Vec 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "kE4_aMGXg1xU",
    "outputId": "7e20dfb5-3dd2-4585-d93c-7fbe388755b7"
   },
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "loaded_model = KeyedVectors.load_word2vec_format(\"eng_w2v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xi4L81s7g65x"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.1.4.3 로드한 모델 이용 단어 유사도 확인\n",
    "\n",
    "- 로드한 모델에 대해서 다시 \"man\"과 유사한 단어를 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "qgzWNwH8hD5v",
    "outputId": "8d2b51e7-f5dd-421b-c5a0-02feb90cac00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8301703333854675), ('guy', 0.8020456433296204), ('lady', 0.7743570804595947), ('boy', 0.7733756303787231), ('girl', 0.7600075602531433), ('soldier', 0.7195061445236206), ('gentleman', 0.7169319987297058), ('kid', 0.6965048313140869), ('rabbi', 0.6876524686813354), ('poet', 0.6633204221725464)]\n"
     ]
    }
   ],
   "source": [
    "model_result = loaded_model.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XuRDOPlHhKU3"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 3.2 한국어 Word2Vec 만들기 (네이버 영화 리뷰)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tcy7N9yvhPK4"
   },
   "source": [
    "### 3.2.1 필요 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eepC8c0Ehgam"
   },
   "outputs": [],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "--qq2i25hTzk"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2VdjfyNuhdzl"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.2.2 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9pvE2Tt-hoOM",
    "outputId": "5ea5df68-18a6-4005-f2db-5a0bd2fea193"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ratings.txt', <http.client.HTTPMessage at 0x217ec5e7128>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KTjLCLAahsAv"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_table('ratings.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "m2MzcDBFhvX_",
    "outputId": "8e6439ea-5dd3-4290-9bfa-dd343d2e1853"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9251303</td>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
       "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
       "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nbWny_DGiVzA"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.2.3 결측값 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "D2iNhG7Ahw4n",
    "outputId": "6a62f548-a590-4338-c715-2c7f5a7a5592"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "# 리뷰 개수 출력\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-f5QaQ7gh153",
    "outputId": "66a9674d-0485-418f-aaf9-d6f488662fea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# NULL 값 존재 유무\n",
    "print(train_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0bPuhTVvh-TZ"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "WB22K7bqh_Sf",
    "outputId": "d98becfd-25f6-45f4-fedd-754aa4ecf920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# NULL 값이 존재하는 행 제거\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "\n",
    "# NULL 값 제거 후 NULL 값이 존재하는 지 확인\n",
    "print(train_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_qURQ0B3iMjO"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "KHKmX0eRiPo2",
    "outputId": "fad54179-a804-42a7-fa4e-50d005420263"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199992\n"
     ]
    }
   ],
   "source": [
    "# NULL 값 제거 후 리뷰 개수 확인\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nEuwFCK_iT9O"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.2.4 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pSpW6DZniezg"
   },
   "outputs": [],
   "source": [
    "# 정규 표현식을 통한 한글 외 문자 제거\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "3YOMEEoJip7G",
    "outputId": "641947f2-cdc5-490a-e2cf-3e9a01305479"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9251303</td>\n",
       "      <td>와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...      1\n",
       "2   4655635                   폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고      1\n",
       "3   9251303   와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지      1\n",
       "4  10067386                         안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화      1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hg1C3vNZisW9"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.2.5 토큰화 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RLTan5XGi1iL"
   },
   "outputs": [],
   "source": [
    "# 불용어 정의\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "duLQHSQ3i5AF"
   },
   "outputs": [],
   "source": [
    "# 형태소 분석기 OKT를 사용한 토큰화 작업\n",
    "okt = Okt()\n",
    "\n",
    "tokenized_data = []\n",
    "\n",
    "for sentence in train_data['document']:\n",
    "    # 토큰화\n",
    "    temp_X = okt.morphs(sentence, stem=True)\n",
    "\n",
    "    # 불용어 제거\n",
    "    temp_X = [word for word in temp_X if not word in stopwords]\n",
    "    \n",
    "    tokenized_data.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "q17VKaD1jN1P",
    "outputId": "4dedc66d-bf63-4671-fbf7-0d4043fe5142"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['어리다', '때', '보고', '지금', '다시', '보다', '재밌다', 'ㅋㅋ']\n",
      "['디자인', '을', '배우다', '학생', '외국', '디자이너', '그', '일군', '전통', '을', '통해', '발전', '문화', '산업', '부럽다', '사실', '우리나라', '에서도', '그', '어렵다', '시절', '끝', '까지', '열정', '을', '지키다', '노라노', '같다', '전통', '있다', '저', '같다', '사람', '꿈', '을', '꾸다', '이루다', '나가다', '수', '있다', '것', '감사하다']\n"
     ]
    }
   ],
   "source": [
    "for data in tokenized_data[:2]:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VYz_54XZmGD4"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.2.6 리뷰 길이 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "tMDyvWJ_mQZ1",
    "outputId": "ca860901-be04-4c09-b4ff-71d0633bcb0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 72\n",
      "리뷰의 평균 길이 : 10.716703668146726\n"
     ]
    }
   ],
   "source": [
    "print('리뷰의 최대 길이 :',max(len(l) for l in tokenized_data))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, tokenized_data))/len(tokenized_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "XnfSX7WWmW_d",
    "outputId": "adf07299-7226-439a-a7fa-4ca4b4b541b0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(s) for s in tokenized_data], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4qzCaofUmYz1"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.2.7 Word2Vec 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4-hiXK2umexB"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=tokenized_data,\n",
    "                 size=100,\n",
    "                 window=4,\n",
    "                 min_count=5,\n",
    "                 workers=4,\n",
    "                 sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "88UBtFRamr58"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "N4bAWrr5mwUU",
    "outputId": "37a489df-0c61-432b-de3e-ac3e16038f78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16477, 100)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 완성된 임베딩 메트릭스의 크기 확인\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-PncdInfm08s"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.2.8 유사한 단어 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "QuJpBwhrm4tQ",
    "outputId": "dea85c18-bf3f-476e-cb4c-9cba91c8351c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('한석규', 0.8852329254150391), ('안성기', 0.8577396869659424), ('이민호', 0.854917049407959), ('박중훈', 0.8466079235076904), ('설경구', 0.8429393172264099), ('전도연', 0.8400722146034241), ('김수현', 0.8388470411300659), ('류덕환', 0.8376166224479675), ('채민서', 0.8339608907699585), ('최민수', 0.8325356245040894)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"최민식\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cPQl6Nztm7Oy"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "3z7UYp5fm9hD",
    "outputId": "257160aa-ee36-4665-e20f-b9a6724d756b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('슬래셔', 0.8760747909545898), ('무협', 0.8499919176101685), ('호러', 0.8483381867408752), ('무비', 0.8473070859909058), ('물의', 0.8314706087112427), ('느와르', 0.8301161527633667), ('블랙', 0.8200322389602661), ('물', 0.8157172203063965), ('블록버스터', 0.8052295446395874), ('멜로', 0.796582818031311)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"히어로\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OOYwQj7RnAJ5"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 3.3 한국어 Word2Vec 만들기 (위키피디아)\n",
    "\n",
    "- 이번에는 위키피디아 한국어 덤프 파일을 다운받아서 한국어로 Word2Vec을 직접 진행해본다.\n",
    "- 영어와 크게 다른 점은 없지만 **한국어는 형태소 토큰화를 해야만 좋은 성능을 얻을 수 있다.**\n",
    "- 간단히 말해 형태소 분석기를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "910RI-4cnSMB"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.1 위키피디아 한국어 덤프 파일 다운로드\n",
    "\n",
    "- [다운로드 링크](https://dumps.wikimedia.org/kowiki/latest/)\n",
    "- 위 링크에는 많은 위키피디아 덤프 파일들이 존재한다.\n",
    "- 그 중에서 사용할 데이터는 `kowiki-latest-pages-articles.xml.bz2` 파일이다.\n",
    "- 해당 파일은 xml 파일이므로, Word2Vec을 원활하게 진행하기 위해 파일 형식을 변환해줄 필요가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gKOKo7jmnjPr"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.2 위키피디아 익스트랙터 다운로드\n",
    "\n",
    "- 해당 파일을 모두 다운로드 받았으면 위키피디아 덤프 파일을 텍스트 형식으로 변환시켜주는 오픈소스인 '위키피디아 익스트랙터'를 사용할 것이다.\n",
    "- '위키피디아 익스트랙터'를 다운로드 받기 위해서는 윈도우의 명령 프롬프트나 MAC과 리눅스의 터미널에서 아래의 git clone 명령어를 통해 다운로드 받을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "q1H6A5PjtTdv",
    "outputId": "71c6a877-cb57-49da-8f4e-807301641919"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'wikiextractor'...\n"
     ]
    }
   ],
   "source": [
    "!git clone \"https://github.com/attardi/wikiextractor.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ysmROorgtVdm"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.3 위키피디아 한국어 덤프 파일 변환\n",
    "\n",
    "- 위키피디아 익스트랙터와 위키피디아 한국어 덤프 파일을 동일한 디렉토리 경로에 두고, 아래 명령어를 실행하면 위키피디아 덤프 파일이 텍스트 파일로 변환된다.\n",
    "- 컴퓨터마다 다르지만 보통 10분 내외의 시간이 걸린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lnSW_5ckthIF"
   },
   "outputs": [],
   "source": [
    "!python WikiExtractor.py kowiki-latest-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- 텍스트 파일로 변환된 위키피디아 한국어 덤프는 총 6개의 디렉토리(2018년 10월 기준)로 구성되어져 있다.\n",
    "- AA ~ AF의 디렉토리로 각 디렉토리 내에는 `wiki_00` ~ `wiki_90`이라는 파일들이 들어 있다.\n",
    "- 각 파일들을 열어보면 아래와 같은 구성이 반복되고 있다.\n",
    "\n",
    "```\n",
    "<doc id=\"문서 번호\" url=\"실제 위키피디아 문서 주소\" title=\"문서 제목\">\n",
    "내용\n",
    "</doc>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예를 들어, AA 디렉토리의 `wiki_00` 파일을 읽어보면, 지미 카터에 대한 내용이 나온다.\n",
    "\n",
    "```\n",
    "<doc id=\"5\" url=\"https://ko.wikipedia.org/wiki?curid=5\" title=\"지미 카터\">\n",
    "지미 카터\n",
    "제임스 얼 \"지미\" 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39번째 대통령(1977년 ~ 1981년)이다.\n",
    "지미 카터는 조지아 주 섬터 카운티 플레인스 마을에서 태어났다. 조지아 공과대학교를 졸업하였다. 그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다. 1953년 미국 해군 대\n",
    "위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다.\n",
    "... 이하 중략...\n",
    "</doc>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 이 6개 AA ~ AF 디렉토리 안의 `wiki_00` ~ `wiki_90` 파일들을 하나의 텍스트 파일로 통합한다.\n",
    "- 만약, 더 간단히 하고 싶다면 모든 디렉토리 파일을 통합하지 않고, 하나의 디렉토리 내의 파일들에 대해서만 통합 작업을 진행하고 모델의 입력으로 사용할 수도 있다.\n",
    "- 하지만 모델의 성능은 전체 파일에 대해서 진행한 경우 보다 좋지 않을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 작업은 6개의 디렉토리 내 파일들에 대해서 각 하나의  파일로 통합 후, 6개의 파일을 다시 하나로 통합하는 순서로 진행된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.4 훈련 데이터 만들기\n",
    "\n",
    "- 우선 AA 디렉토리 안의 모든 파일인 `wiki_00` ~ `wiki_90`에 대해서 `wikiAA.txt`로 통합해보자.\n",
    "- 프롬프트에서 아래의 커맨드를 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!copy text\\AA\\wiki* wikiAA.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 해당 커맨드는 AA 디렉토리 안의 `wiki`로 시작되는 모든 파일을 `wikiAA.txt` 파일에 전부 복사하라는 의미를 담고 있다.\n",
    "- 결과적으로 `wiki_00` ~ `wiki_90`파일의 모든 내용은 `wikiAA.txt` 파일이라는 하나의 파일에 내용이 들어가게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 디렉토리에 대해서도 동일하게 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!copy text\\AB\\wiki* wikiAB.txt\n",
    "!copy text\\AC\\wiki* wikiAC.txt\n",
    "!copy text\\AD\\wiki* wikiAD.txt\n",
    "!copy text\\AE\\wiki* wikiAE.txt\n",
    "!copy text\\AF\\wiki* wikiAF.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- 이렇게 되면 현재 경로에는 각 디렉토리의 파일들을 하나로 합친 `wikiAA.txt`부터 `wikiAF.txt`라는 6개의 파일이 생긴다.\n",
    "- 그럼 이제 이 파일들에 대해서도 하나의 파일로 합치는 작업을 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!copy wikiA* wiki_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 모든 텍스트 파일을 하나로 만든 훈련 데이터가 완성되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.5 훈련 데이터 전처리 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('wiki_data.txt', encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- 파일이 정상적으로 저장되었는 지 5개의 줄만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if line != \"\\n\":\n",
    "        i = i+1\n",
    "        print(\"%d번 째 줄 : \"%i + line)\n",
    "    if i == 5:\n",
    "        break\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- 이제 본격적으로 Word2Vec을 위한 학습 데이터를 만들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "# 파일을 다시 처음부터 읽음\n",
    "fread = open('wiki_data.txt', encoding='utf8')\n",
    "\n",
    "n = 0\n",
    "result = []\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # 한 줄씩 읽음\n",
    "    line = fread.readline()\n",
    "    \n",
    "    # 모두 읽으면 while문 종료\n",
    "    if not line: \n",
    "        break\n",
    "    \n",
    "    n = n+1\n",
    "    \n",
    "    if n % 5000 == 0: # 5,000의 배수로 While문이 실행될 때마다 몇 번째 While문 실행인 지 출력\n",
    "        print(\"%d번 째 While문.\"%n)\n",
    "        \n",
    "    # 단어 토큰화\n",
    "    tokenlist = okt.pos(line, stem=True, norm=True)\n",
    "    \n",
    "    temp = []\n",
    "    \n",
    "    for word in tokenlist:\n",
    "        if word[1] in [\"Noun\"]: # 명사일 때만\n",
    "            temp.append((word[0])) # 해당 단어를 저장함\n",
    "            \n",
    "    if temp: # 만약 이번에 읽은 데이터에 명사가 존재할 경우에만\n",
    "        result.append(temp) # 결과에 저장\n",
    "        \n",
    "fread.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 여기서는 형태소 분석기로 WoNLPy의 `Okt`를 사용하여 명사만을 추출하여 훈련 데이터를 구성했다.\n",
    "- 위 작업은 시간이 꽤 걸린다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- 훈련 데이터를 모두 만들었다면, 훈련 데이터의 길이를 확인해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('총 샘플의 개수 : {}'.format(len(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 약 240만 여개의 줄(line)이 명사 토큰화가 되어 저장되어 있는 상태이다.\n",
    "- 이제 이를 Word2Vec으로 학습시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.6 Word2Vec 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(result, size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- 학습을 다했다면 이제 임의의 입력 단어로부터 유사한 단어들을 구해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result1 = model.wv.most_similar(\"대한민국\")\n",
    "print(model_result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result2 = model.wv.most_similar(\"어벤져스\")\n",
    "print(model_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result3 = model.wv.most_similar(\"반도체\")\n",
    "print(model_result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3.4 사전 훈련된 Word2Vec 임베딩(Pre-trained Word2Vec embedding) 소개\n",
    "\n",
    "- 자연어 처리 작업을 할 때, 케라스의 `Embedding()`를 사용하여 갖고 있는 훈련 데이터로부터 처음부터 임베딩 벡터를 훈련시키기도 하지만, 위키피디아 등의 방대한 데이터로 사전에 훈련된 워드 임베딩(pre-trained word embedding vector)를 가지고 와서 해당 벡터들의 값을 원하는 작업에 사용할 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예를 들어, 감성 분류 작업을 하는 데 훈련 데이터의 양이 부족한 상황이라면, 다른 방대한 데이터를 Word2Vec이나 GloVe 등으로 사전에 학습시켜놓은 임베딩 벡터들을 가지고 와서 모델의 입력으로 사용하는 것도 때로는 더 좋은 성능을 얻을 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 여기서는 사전 훈련된 워드 임베딩을 가져와서 간단히 단어들의 유사도를 구해보는 실습을 해본다.\n",
    "- 실제로 모델에 적용해보는 실습은 사전 훈련된 임베딩 챕터에서 진행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.1 영어\n",
    "\n",
    "- 이번에는 구글이 제공하는 사전 훈련된 Word2Vec 모델을 사용하는 방법에 대해서 알아보도록 하자.\n",
    "- 구글은 사전 훈련된 3백만 개의 Word2Vec 단어 벡터들을 제공한다.\n",
    "- 각 임베딩 벡터의 차원은 300이다.\n",
    "- `gensim`을 통해서 이 모델을 불러오는 건 매우 간단하다.\n",
    "- 이 모델을 다운로드하고 파일 경로를 기재하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [모델 다운로드 경로](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 압축 파일의 용량은 약 1.5GB이지만, 파일의 압축을 풀면 약 3.3GB의 파일이 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# 구글의 사전 훈련된 Word2Vec 모델을 로드한다.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 크기 확인\n",
    "print(model.vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델의 크기는 3,000,000 x 300. 즉, 3백만 개의 단어와 각 단어의 차원은 300\n",
    "- 파일의 크기가 3GB가 넘는 이유를 계산해보면 아래와 같다.\n",
    "  - 3백만개의 단어들 x 300의 차원(feature) x 4bytes/feature = ~3.35GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 단어의 유사도 계산\n",
    "print(model.similarity('this', 'is'))\n",
    "print(model.similarity('post', 'book'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 'book'의 벡터 출력\n",
    "print(model['book'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.2 한국어\n",
    "\n",
    "- 한국어의 미리 학습된 Word2Vec 모델은 [박규병님의 깃허브 주소](https://github.com/Kyubyong/wordvectors)에 공개되어져 있다.\n",
    "- 박규병님이 공개한 직접적인 다운로드 링크는 아래와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [다운로드 링크](https://drive.google.com/file/d/0B0ZXk88koS2KbDhXdWg1Q2RydlU/view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 링크로부터 77MB 크기의 `ko.zip` 파일을 다운로드 받아서 압축을 풀면 `ko.bin`이라는 50MB 크기의 파일이 있다.\n",
    "- 이 파일을 로드하고 유사도를 계산해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec.load('ko.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.wv.most_similar(\"강아지\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3.5 Word2Vec의 활용\n",
    "\n",
    "- Word2Vec 모델은 자연어 처리에서 단어를 밀집 벡터로 만들어주는 단어 임베딩 방법론이다.\n",
    "- 하지만 최근에 들어서는 자연어 처리를 넘어서 추천 시스템에도 사용되고 있는 모델이다.\n",
    "- 우선 적당하게 데이터를 나열해주면 Word2Vec은 위치가 근접한 데이터를 유사도가 높은 벡터를 만들어준다는 점에서 착안된 아이디어이다.\n",
    "- [해당 링크](https://brunch.co.kr/@goodvc78/16?fbclid=IwAR1QZZAeZe_tNWxnxVCRwl8PIouBPAaqSIJ1lBxJ-EKtfDfmLehi1MUV_Lk)에서 그림 10번을 보면 쉽게 이해할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch10_v03_Word2Vec-Practice.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
