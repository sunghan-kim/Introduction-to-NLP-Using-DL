{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oqoE13E9yB9r"
   },
   "source": [
    "# Ch10. 워드 임베딩 (Word Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xwG-HEqRyH5y"
   },
   "source": [
    "# v04. 글로브 (GloVe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h1KO3yINyO5h"
   },
   "source": [
    "- 글로브(Global Vectors for Word Representation, GloVe)는 카운트 기반과 예측 기반을 모두 사용하는 방법론이다.\n",
    "- 2014년에 미국 스탠포드대학에서 개발한 단어 임베딩 방법론이다.\n",
    "- 앞서 학습하였던 기존의 카운트 기반의 LSA(Latent Semantic Analysis)와 예측 기반의 Word2Vec의 단점을 지적하며 이를 보완한다는 목적으로 나왔다.\n",
    "- 실제로도 Word2Vec 만큼 뛰어난 성능을 보여준다.\n",
    "- 현재까지의 연구에 따르면 단정적으로 Word2Vec와 GloVe 중에서 어떤 것이 더 뛰어나다고 말할 수는 없고, 이 두 가지 전부를 사용해보고 성능이 더 좋은 것을 사용하는 것이 바람직하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xlKQ7gTmzAws"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 4.1 기존 방법론에 대한 비판\n",
    "\n",
    "- 기존의 방법론을 복습해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VhBytTi0zPvW"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 4.1.1 LSA\n",
    "\n",
    "- LSA는 DTM이나 TF-IDF 행렬과 같이 각 문서에서의 각 단어의 빈도 수를 카운트한 행렬이라는 전체적인 통계 정보를 입력으로 받아 차원을 축소(Truncated SVD)하여 잠재된 의미를 끌어내는 방법론이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f2ZwnH9YzdRm"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 4.1.2 Word2Vec\n",
    "\n",
    "- 반면, Word2Vec는 실제값과 예측값에 대한 오차를 손실 함수를 통해 줄여나가며 학습하는 예측 기반의 방법론이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CPYAWpPJzon_"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 4.1.3 각각의 방법론의 장단점\n",
    "\n",
    "- 서로 다른 방법을 사용하는 이 두 방법론은 각각 장, 단점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Igb_mohLzxdH"
   },
   "source": [
    "- LSA\n",
    "  - (장점) : 카운트 기반으로 코퍼스의 전체적인 통계 정보를 고려한다.\n",
    "  - (단점) : 왕 : 남자 = 여왕 : ?(정답은 여자)와 같은 단어 의미의 유추 작업(Analogy task)에는 성능이 떨어진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XkWO1Whh0Ag-"
   },
   "source": [
    "- Word2Vec\n",
    "  - (장점) : 예측 기반으로 단어 간 유추 작업에는 LSA보다 뛰어나다.\n",
    "  - (단점) : 임베딩 벡터가 윈도우 크기 내에서만 주변 단어를 고려하기 때문에 코퍼스의 전체적인 통계 정보를 반영하지 못한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qtHCEYfm0IVg"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 4.1.4 GloVe\n",
    "\n",
    "- GloVe는 이러한 기존 방법론들의 각각의 한계를 지적하며, LSA의 메커니즘이었던 카운트 기반의 방법과 Word2Vec의 메커니즘이었던 예측 기반의 방법론 두 가지를 모두 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xxC57RVS0i7v"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 4.2 윈도우 기반 동시 등장 행렬 (Window based Co-occurrence Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4FD27MI74S9j"
   },
   "source": [
    "### 4.2.1 단어의 동시 등장 행렬\n",
    "\n",
    "- 행과 열을 전체 단어 집합의 단어들로 구성\n",
    "- i 단어의 윈도우 크기(Window Size) 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "byqhOyRi21DO"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 4.2.2 예제\n",
    "\n",
    "- 아래와 같은 텍스트가 있다고 하자.\n",
    "\n",
    "> I like deep learning  \n",
    "I like NLP  \n",
    "I enjoy flying\n",
    "\n",
    "- 윈도우 크기가 N 일 때는 좌, 우에 존재하는 N개의 단어만 참고하게 된다.\n",
    "- 윈도우 크기가 1일 때, 위의 텍스트를 가지고 구성한 동시 등장 행렬은 다음과 같다.\n",
    "\n",
    "| 카운트   | I    | like | enjoy | deep | learning | NLP  | flying |\n",
    "| :------- | :--- | :--- | :---- | :--- | :------- | :--- | :----- |\n",
    "| I        | 0    | 2    | 1     | 0    | 0        | 0    | 0      |\n",
    "| like     | 2    | 0    | 0     | 1    | 0        | 1    | 0      |\n",
    "| enjoy    | 1    | 0    | 0     | 0    | 0        | 0    | 1      |\n",
    "| deep     | 0    | 1    | 0     | 0    | 1        | 0    | 0      |\n",
    "| learning | 0    | 0    | 0     | 1    | 0        | 0    | 0      |\n",
    "| NLP      | 0    | 1    | 0     | 0    | 0        | 0    | 0      |\n",
    "| flying   | 0    | 0    | 1     | 0    | 0        | 0    | 0      |\n",
    "\n",
    "- 해당 테이블은 스탠포드 대학교의 자연어 처리 강의를 참고하였다. ([링크](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture02-wordvecs2.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MeSWU_va3fic"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 4.2.3 윈도우 기반 동시 등장 행렬의 특징\n",
    "\n",
    "- 위 행렬은 행렬을 전치(Transpose)해도 동일한 행렬이 된다는 특징이 있다.\n",
    "- 그 이유는 i 단어의 윈도우 크기 내에서 k 단어가 등장한 빈도는 반대로 k 단어의 윈도우 크기 내에서 i 단어가 등장한 빈도와 동일하기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T7UdG5Ny3yoK"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 4.3 동시 등장 확률 (Co-occurrence Probability)\n",
    "\n",
    "- 아래의 표는 어떤 동시 등장 행렬을 가지고 정리한 동시 등장 확률(Co-occurrence Probability)을 보여준다.\n",
    "- 그렇다면 동시 등장 확률이란 무엇일까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tsTIPIS84EYq"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 4.3.1 동시 등장 확률 $P(k \\, | \\, i)$\n",
    "\n",
    "- 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트한다.\n",
    "- 특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "enhTTGVt4kZQ"
   },
   "source": [
    "- $P(k \\, | \\, i)$에서 i를 중삼 단어(Center Word), k를 주변 단어(Context Word)라고 할 때\n",
    "  - 분모값 : 동시 등장 행렬에서 중심 단어 i의 행의 모든 값을 더한 값\n",
    "  - 분자값 : i행 k열의 값\n",
    "\n",
    "$$\n",
    "P(k \\, | \\, i) = \\frac{\\text{value of the i row and k column}}{\\text{sum of all values in i row}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7RI86oXH5-_i"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 4.3.2 표로 정리한 동시 등장 확률\n",
    "\n",
    "- 다음은 GloVe의 제안 논문에서 가져온 동시 등장 확률을 표로 정리한 하나의 예이다.\n",
    "\n",
    "| 동시 등장 확률과 크기 관계 비(ratio) | k=solid  | k=gas    | k=water | k=fasion |\n",
    "| :----------------------------------- | :------- | :------- | :------ | :------- |\n",
    "| P(k l ice)                           | 0.00019  | 0.000066 | 0.003   | 0.000017 |\n",
    "| P(k l steam)                         | 0.000022 | 0.00078  | 0.0022  | 0.000018 |\n",
    "| P(k l ice) / P(k l steam)            | 8.9      | 0.085    | 1.36    | 0.96     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gcya781J7p8E"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.3.2.1 k = solid\n",
    "\n",
    "- `solid`가 등장했을 때 `ice`가 등장할 확률인 0.00019은 `solid`가 등장했을 때 `steam`이 등장할 확률인 0.000022보다 약 8.9배 더 크다.\n",
    "- `solid`는 '단단한'이라는 의미를 가졌으므로, '증기'라는 의미를 가지는 `steam`보다는 당연히 '얼음'의 뜻을 가진 `ice`라는 단어와 더 자주 등장할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l_w_TTdz7Bl5"
   },
   "source": [
    "- k가 `solid`일 때, $P(\\text{solid} \\, | \\, \\text{ice}) / P(\\text{solid} \\, | \\, \\text{steam})$를 계산한 값은 8.9가 나온다.\n",
    "- 이 값은 1보다는 매우 큰 값이다. ($P(\\text{solid} \\, | \\, \\text{ice})$의 값은 크고, $P(\\text{solid} \\, | \\, \\text{steam})$의 값은 작기 때문)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cv_DlSs87vpL"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.3.2.2 k = gas\n",
    "\n",
    "- `gas`는 `ice`보다는 `steam`과 더 자주 등장한다.\n",
    "- 그러므로 $P(\\text{gas} \\, | \\, \\text{ice}) / P(\\text{gas} \\, | \\, \\text{steam})$를 계산한 값은 0.085가 나온다.\n",
    "- 이 값은 1보다 훨씬 작은 값이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l6P1J6xq8EsQ"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.3.2.3 k = water\n",
    "\n",
    "- k가 `water`인 경우에는 `solid`와 `steam` 두 단어 모두와 동시 등장하는 경우가 많으므로 1에 가까운 값(1.36)이 나온다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zo-nzsmi8dY7"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.3.2.4 k = fasion\n",
    "\n",
    "- k가 `fasion`인 경우에는 `solid`와 `steam` 두 단어 모두와 동시 등장하는 경우가 적으므로 1에 가까운 값(0.96)에 가까운 값이 나온다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BAlZaFaK8tcB"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 4.3.3 표 내용 정리\n",
    "\n",
    "- 보기 쉽도록 조금 단순화해서 표현한 표는 다음과 같다.\n",
    "\n",
    "| 동시 등장 확률과 크기 관계 비(ratio) | k=solid | k=gas   | k=water    | k=fasion   |\n",
    "| :----------------------------------- | :------ | :------ | :--------- | :--------- |\n",
    "| P(k l ice)                           | 큰 값   | 작은 값 | 큰 값      | 작은 값    |\n",
    "| P(k l steam)                         | 작은 값 | 큰 값   | 큰 값      | 작은 값    |\n",
    "| P(k l ice) / P(k l steam)            | 큰 값   | 작은 값 | 1에 가까움 | 1에 가까움 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zzzwbPqM84f7"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 4.4 손실 함수 (Loss function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5j0CWa969CM6"
   },
   "source": [
    "### 4.4.1 손실 함수 관련 용어 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4lBdNoP9MTR"
   },
   "source": [
    "- $X$\n",
    "  - 동시 등장 행렬 (Co-occurrence Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zlFpmAxP9R2h"
   },
   "source": [
    "- $X_{ij}$\n",
    "  - 중심 단어 $i$가 등장했을 때 윈도우 내 주변 단어 $j$가 등장하는 횟수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7iZVzA3L9deG"
   },
   "source": [
    "- $X_i$ ($= \\sum_j X_{ij}$)\n",
    "  - 동시 등장 행렬에서 $i$행의 값을 모두 더한 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bBwBSYe89p4N"
   },
   "source": [
    "- $P_{ik}$ ($= P(k \\, | \\, i) \\; = \\; \\frac{X_{ik}}{X_{i}}$)\n",
    "  - 중심 단어 $i$가 등장했을 때, 윈도우 내 주변 단어 $k$가 등장할 확률\n",
    "  - ex) $P(\\text{solid} \\, | \\, \\text{ice})$ = 단어 `ice`가 등장했을 때, 단어 `solid`가 등장할 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VlgMp4Qh-Tf9"
   },
   "source": [
    "- $\\frac{P_{ik}}{P_{jk}}$\n",
    "  - $P_{ik}$를 $P_{jk}$로 나눠준 값\n",
    "  - ex) $P(\\text{solid} \\, | \\, \\text{ice}) / P(\\text{solid} \\, | \\, \\text{steam}) = 8.9$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H27NeHgT-uWg"
   },
   "source": [
    "- $w_i$\n",
    "  - 중심 단어 $i$의 임베딩 벡터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WpNo6gUv-yX7"
   },
   "source": [
    "- $\\tilde{w_k}$\n",
    "  - 주변 단어 $k$의 임베딩 벡터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RWdOE9o6_EJV"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 4.4.2 GloVe의 아이디어\n",
    "\n",
    "> \"임베딩된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것\"\n",
    "\n",
    "- 즉, 이를 만족하도록 임베딩 벡터를 만드는 것이 목표이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3yU7lSEc_ULj"
   },
   "source": [
    "- 이를 식으로 표현하면 다음과 같다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "dot \\; product \\left( w_i \\; \\tilde{w_k} \\right) \\; \\approx \\; P(k \\, | \\, i) = P_{ik}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XdvVXG5v_q4Z"
   },
   "source": [
    "- 더 정확히는 GloVe는 아래와 같은 관계를 가지도록 임베딩 벡터를 설계한다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "dot \\; product \\left( w_i \\; \\tilde{w_k} \\right) \\; \\approx \\; log \\; P(k \\, | \\, i) = log \\; P_{ik}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SOMdCi7UB-_J"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 4.4.3 임베딩 벡터들을 만들기 위한 손실 함수 설계\n",
    "\n",
    "- 가장 중요한 것은 **단어 간의 관계를 잘 표현하는 함수여야 한다는 것**이다.\n",
    "- 이를 위해 앞서 배운 개념인 $P_{ik} / P_{jk}$를 식에 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EvHHJgnzChXc"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.4.3.1 초기 식\n",
    "\n",
    "- GloVe의 연구진들은 벡터 $w_i$, $w_j$, $\\tilde{w_k}$를 가지고 어떤 함수 $F$를 수행하면, $P_{ik} / P_{jk}$가 나온다는 초기 식으로부터 전개를 시작한다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "F(w_i, \\; w_j, \\; \\tilde{w_k}) = \\frac{P_{ik}}{P_{jk}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ARw0ibwYC6Fi"
   },
   "source": [
    "- 아직 이 함수 $F$가 어떤 식을 가지고 있는 지는 정해진 게 없다.\n",
    "- 위의 목적에 맞게 근사할 수 있는 함수식은 무수히 많을 것이다.\n",
    "- 그 중 최적의 식에 다가가기 위해서 차근 차근 디테일을 추가해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "csaX-C7mDGPr"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.4.3.2 뺄셈 적용\n",
    "\n",
    "- 함수 $F$는 두 단어 사이의 동시 등장 확률의 크기 관계 비(ratio) 정보를 벡터 공간에 인코딩하는 것이 목적이다.\n",
    "- 이를 위해 GloVe 연구진들은 $w_i$와 $w_j$라는 두 벡터의 차이를 함수 $F$의 입력으로 사용하는 것을 제안한다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "F(w_i - w_j, \\; \\tilde{w_k}) = \\frac{P_{ik}}{P_{jk}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P35xfGBcDhDB"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.4.3.3 내적 적용\n",
    "\n",
    "- 그런데 위 식에서 우변($\\frac{P_{ik}}{P_{jk}}$)은 스칼라 값이고, 좌변은 벡터값이다.\n",
    "- 이를 성립하게 해주기 위해서 함수 $F$의 두 입력에 내적(Dot Product)을 수행한다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "F( \\left( w_i - w_j \\right)^T \\cdot \\tilde{w_k}) = \\frac{P_{ik}}{P_{jk}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "td-rD1ssED-5"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.4.3.4 정리\n",
    "\n",
    "- 정리하면, 선형 공간(Linear Space)에서 단어의 의미 관계를 표현하기 위해 **뺄셈**과 **내적**을 택했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "njHl8YTPE1pU"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.4.3.5 준동형(Homomorphism)\n",
    "\n",
    "- 여기서 함수 $F$가 만족해야 할 필수 조건이 있다.\n",
    "- 중심 단어 $w$와 주변 단어 $\\tilde{w}$ 라는 선택 기준은 실제로는 **무작위 선택**이다.\n",
    "- 그러므로 이 둘의 관계는 **자유롭게 교환될 수 있도록** 해야 한다.\n",
    "- 이 것이 성립되게 하기 위해서 GloVe 연구진은 함수 $F$가 **실수의 덧셈**과 **양수의 곱셈**에 대해서 **준동형(Homomorphism)**을 만족하도록 한다.\n",
    "- 정리하면 $a$와 $b$에 대해서 함수 $F$가 $F(a+b)$가 $F(a)F(b)$와 같도록 만족시켜야 한다는 의미이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LeUmqA-dFtrD"
   },
   "source": [
    "- 식으로 나타내면 아래와 같다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "F(a + b) = F(a) F(b) \\quad \\forall a, \\, b \\; \\in \\; \\mathbb{R}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KPMLVM3tGdIb"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.4.3.6 준동형식 변형 (1)\n",
    "\n",
    "- 이제 이 준동형식을 현재 전개하던 GloVe 식에 적용할 수 있도록 조금씩 바꿔보자.\n",
    "- 전개하던 GloVe 식에 따르면, 함수 $F$는 결과값으로 스칼라 값인 $\\frac{P_{ik}}{P_{jk}}$이 나와야 한다.\n",
    "- 준동형식에서  \n",
    "  - $a$와 $b$가 각각 벡터값인 경우 $\\rightarrow$ 함수 $F$의 결과값으로는 스칼라 값이 나올 수 없다.\n",
    "  - $a$와 $b$가 각각 **두 벡터의 내적값**인 경우 $\\rightarrow$ 함수 $F$의 결과값으로 스칼라 값이 나올 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A_A80TV0HXdL"
   },
   "source": [
    "- 그러므로 위의 준동형식을 아래와 같이 바꿔보자.\n",
    "- 여기서 $v_1, \\, v_2, \\, v_3, \\, v_4$는 각각 벡터값이다.\n",
    "- 아래의 $V$는 벡터를 의미한다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "F \\left( v_1^T \\, v_2 + v_3^T \\, v_4 \\right) = F(v_1^T \\, v_2) \\, F(v_3^T \\, v_4) \\quad \\forall \\, v_1, \\, v_2, \\, v_3, \\, v_4 \\; \\in \\; V\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oaHX7YE_INPa"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.4.3.7 준동형식 변형 (2)\n",
    "\n",
    "- 그런데 앞서 작성한 GloVe 식에서는 $w_i$와 $w_j$라는 두 벡터의 차이를 함수 $F$의 입력으로 받았다.\n",
    "- GloVe 식에 바로 적용을 위해 준동형식을 뺄셈에 대한 준동형식으로 변경한다.\n",
    "- 그렇게 되면 곱셈도 나눗셈으로 바뀐다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "F \\left( v_1^T \\, v_2 - v_3^T \\, v_4 \\right) = \\frac{F(v_1^T \\, v_2)}{F(v_3^T \\, v_4)} \\quad \\forall \\, v_1, \\, v_2, \\, v_3, \\, v_4 \\; \\in \\; V\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ykG_Em9Ja_D"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.4.3.8 준동형식 적용\n",
    "\n",
    "- 이제 이 준동형식을 GloVe 식에 적용해보자.\n",
    "- 우리의 목적은 함수 F의 우변을 다음과 같이 바꿔야 한다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "F( \\left( w_i - w_j \\right)^T \\cdot \\tilde{w_k}) = \\frac{F(w_i^T \\, \\tilde{w_k})}{F(w_j^T \\, \\tilde{w_k})}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qhQX22OZK3ld"
   },
   "source": [
    "- 이전 식에 따르면 우변은 본래 $\\frac{P_{ik}}{P_{jk}}$ 이였으므로, 결과적으로 다음과 같다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "\\frac{P_{ik}}{P_{jk}} = \\frac{F(w_i^T \\, \\tilde{w_k})}{F(w_j^T \\, \\tilde{w_k})}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8pnYALcGLG9U"
   },
   "source": [
    "$\n",
    "\\qquad\n",
    "F(w_i^T \\, \\tilde{w_k}) = P_{ik} = \\frac{X_{ik}}{X_i}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F2NxnzN5LVWi"
   },
   "source": [
    "- 좌변을 풀어쓰면 다음과 같다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "F( w_i^T \\, \\tilde{w_k} \\; - \\; w_j^T \\, \\tilde{w_k}) =\n",
    "\\frac{F(w_i^T \\, \\tilde{w_k})}{F(w_j^T \\, \\tilde{w_k})}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8i8LCWOALnzB"
   },
   "source": [
    "- 이는 뺄셈에 대한 준동형식의 형태와 정확히 일치한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q0yK4tDULr0p"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.4.3.9 함수 $F$\n",
    "\n",
    "- 이제 이를 만족하는 함수 $F$를 찾아야 한다.\n",
    "- 그리고 이를 정확하게 만족시키는 함수가 있는데 바로 **지수 함수(Exponential function)**이다.\n",
    "- $F$를 지수 함수 $exp$라고 해보자.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "exp( w_i^T \\, \\tilde{w_k} \\; - \\; w_j^T \\, \\tilde{w_k}) =\n",
    "\\frac{exp(w_i^T \\, \\tilde{w_k})}{exp(w_j^T \\, \\tilde{w_k})}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8A8xMXANMVzs"
   },
   "source": [
    "$\n",
    "\\qquad\n",
    "exp(w_i^T \\, \\tilde{w_k}) = P_{ik} = \\frac{X_{ik}}{X_i}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c4UBME4mMcRB"
   },
   "source": [
    "- 위의 두 번째 식으로부터 다음과 같은 식을 얻을 수 있다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "w_i^T \\, \\tilde{w_k} = log \\, P_{ik} = log \\left( \\frac{X_{ik}}{X_i} \\right) = log \\, X_{ik} - log \\, X_i\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UcJuvH-9M9aI"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.4.3.10 편향 적용\n",
    "\n",
    "- 그런데 여기서 상기해야 할 것은 앞서 언급했듯이, 사실 $w_i$와 $\\tilde{w}$는 두 값의 위치를 서로 바꾸어도 식이 성립해야 한다.\n",
    "- $X_{ik}$의 정의를 생각해보면 $X_{ki}$와도 같다.\n",
    "- 그런데 이게 성립되려면 위의 식에서 $log \\, X_i$ 항이 걸림돌이다.\n",
    "- 이 부분만 없다면 이를 성립시킬 수 있다.\n",
    "- 그래서 GloVe 연구팀은 $log \\, X_i$ 항을 $w_i$에 대한 편향 $b_i$라는 상수항으로 대체하기로 한다.\n",
    "- 같은 이유로 $\\tilde{w_k}$에 대한 편향 $\\tilde{b_k}$를 추가한다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "w_i^T \\, \\tilde{w_k} + b_i + \\tilde{b_k} = log \\, X_{ik}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eoh-0WiQN5C-"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.4.3.11 손실 함수 일반화\n",
    "\n",
    "- 위의 식이 손실 함수의 핵심이 되는 식이다.\n",
    "- 우변의 값과의 차이를 최소화하는 방향으로 좌변의 4개의 항은 학습을 통해 값이 바뀌는 변수들이 된다.\n",
    "- 즉, 손실 함수는 다음과 같이 일반화될 수 있다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "Loss \\, function = \\sum_{m,n=1}^V \\left( w_m^T \\, \\tilde{w_n} + b_m + \\tilde{b_n} - log \\, X_{mn} \\right)^2\n",
    "$\n",
    "\n",
    "- 여기서 $V$는 단어 집합의 크기를 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__HuymJLOlqr"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.4.3.12 손실 함수의 보완 필요성\n",
    "\n",
    "- 그런데 아직 최적의 손실 함수라기에는 부족하다.\n",
    "- GloVe 연구진은 $log \\, X_{ik}$에서 $X_{ik}$ 값이 0이 될 수 있음을 지적한다.\n",
    "- 대안 중 하나는 $log \\, X_{ik}$ 항을 $log \\left( 1 + X_{ik} \\right)$로 변경하는 것이다.\n",
    "- 하지만 이렇게 해도 여전히 해결되지 않는 문제가 있다.\n",
    "- 바로 **동시 등장 행렬 $X$는 마치 DTM처럼 희소 행렬(Sparse Matrix)일 가능성이 다분하다는 점**이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ILug4bmLPRNO"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.4.3.13 가중치 함수 도입\n",
    "\n",
    "- 동시 등장 행렬 $X$에는 많은 값이 0 이거나, 동시 등장 빈도가 적어서 많은 값이 작은 수치를 가지는 경우가 많다.\n",
    "- 앞서 빈도수를 가지고 가중치를 주는 고민을 하는 TF-IDF나 LSA와 같은 몇 가지 방법들을 본 적이 있다.\n",
    "- GloVe의 연구진은 동시 등장 행렬에서 동시 등장 빈도의 값 $X_{ik}$이 굉장히 낮은 경우에는 정보에 거의 도움이 되지 않는다고 판단한다.\n",
    "- 그래서 이에 대한 가중치를 주는 고민을 하게 된다.\n",
    "- GloVe 연구팀이 선택한 것은 바로 **$X_{ik}$의 값에 영향을 받는 가중치 함수(Weighting function) $f \\left( X_{ik} \\right)$를 손실 함수에 도입하는 것**이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VTNd7PJlP6o-"
   },
   "source": [
    "- GloVe에 도입되는 $f \\left( X_{ik} \\right)$의 그래프는 아래와 같다.\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/22885/%EA%B0%80%EC%A4%91%EC%B9%98.PNG)\n",
    "\n",
    "- $X_{ik}$의 값이 작다 $\\rightarrow$ 상대적으로 함수의 값은 작도록 한다.\n",
    "- $X_{ik}$의 값이 크다 $\\rightarrow$ 상대적으로 함수의 값은 크도록 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ADlwikSQfog"
   },
   "source": [
    "- 하지만 $X_{ik}$가 지나치게 높다고 해서 지나친 가중치를 주지 않기 위해서 또한 함수의 최대값이 정해져 있다. (최대값은 1)\n",
    "  - ex) 'It is'와 같은 불용어의 동시 등장 빈도수가 높다고 해서 지나친 가중을 받아서는 안된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XSJbqJ3-Q7y1"
   },
   "source": [
    "- 이 함수의 값을 손실 함수에 곱해주면 가중치의 역할을 할 수 있다.\n",
    "- 이 함수 $f(x)$의 식은 다음과 같이 정의된다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "f(x) = min \\left( 1, \\; (x / x_{max})^{3/4} \\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DRhoZ88QRKy9"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 4.4.3.14 최종 일반화된 손실 함수\n",
    "\n",
    "- 최종적으로 다음과 같은 일반화된 손실 함수를 얻어낼 수 있다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "Loss \\, function = \\sum_{m,n=1}^V f \\left( X_{mn} \\right) \\, \\left( w_m^T \\, \\tilde{w_n} + b_m + \\tilde{b_n} - log \\, X_{mn} \\right)^2\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "itaWXRaRRjGB"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 4.5 GloVe 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eZKuqQElRotd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch10_v04_GloVe.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
