{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MInW092J8tIC"
   },
   "source": [
    "# Ch10. 워드 임베딩 (Word Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gp7DxrEN80Aa"
   },
   "source": [
    "# v05. 사전 훈련된 워드 임베딩 (Pre-trained Word Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LzeQbuy-Vdte"
   },
   "source": [
    "- 이번 챕터에서는 **케라스의 임베딩 층(embedding layer)**과 **사전 훈련된 워드 임베딩(pre-trained word embedding)**을 가져와서 사용하는 것을 비교한다.\n",
    "- 자연어 처리를 구현하려고 할 때 갖고 있는 훈련 데이터의 단어들을 임베딩 층(embedding layer)을 구현하여 임베딩 벡터로 학습하는 경우가 있다.\n",
    "- 케라스에서는 이를 `Embedding()`이라는 도구를 사용하여 구현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0q-Fr_IVzR1"
   },
   "source": [
    "- 그런데 위키피디아 등과 같은 방대한 코퍼스를 가지고 Word2Vec, FastText, GloVe 등을 통해서 이미 미리 훈련된 임베딩 벡터를 불러오는 방법을 사용하는 경우도 있다.\n",
    "- 이는 현재 갖고 있는 훈련 데이터를 임베딩 층으로 처음부터 학습을 하는 방법과는 대조된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nkuhtddPWn2z"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 5.1 케라스 임베딩 층 (Keras Embedding layer)\n",
    "\n",
    "- 케라스는 훈련 데이터의 단어들에 대해 워드 임베딩을 수행하는 도구 `Embedding()`을 제공한다.\n",
    "- `Embedding()`은 인공 신경망 구조 관점에서 임베딩 층(embedding layer)을 구현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jm_au1MOW0f7"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 5.1.1 임베딩 층은 룩업 테이블이다.\n",
    "\n",
    "- 임베딩 층의 입력으로 사용하기 위해서 입력 시퀀스의 각 단어들은 모두 정수 인코딩이 되어야 한다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "\\text{어떤 단어} \\; \\rightarrow \\; \n",
    "\\text{단어에 부여된 고유한 정수값} \\; \\rightarrow \\; \n",
    "\\text{임베딩 층 통과} \\; \\rightarrow \\; \n",
    "\\text{밀집 벡터}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2iftPq86W3Fi"
   },
   "source": [
    "- 임베딩 층은 입력 정수에 대해 밀집 벡터(dense vector)로 맵핑한다.\n",
    "- 이 밀집 벡터는 인공 신경망의 학습 과정에서 가중치가 학습되는 것과 같은 방식으로 훈련된다.\n",
    "- 훈련 과정에서 단어는 모델이 풀고자하는 작업에 맞는 값으로 업데이트된다.\n",
    "- 그리고 이 밀집 벡터를 임베딩 벡터라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AvdBbCH7Z0qb"
   },
   "source": [
    "- 정수를 밀집 벡터 또는 임베딩 벡터로 맵핑한다는 것은 어떤 의미일까?\n",
    "- 특정 단어와 맵핑되는 정수를 인덱스로 가지는 테이블로부터 임베딩 벡터 값을 가져오는 룩업 테이블이라고 볼 수 있다.\n",
    "- 그리고 이 테이블은 단어 집합의 크기만큼의 행을 가지므로 모든 단어는 고유한 임베딩 벡터를 가진다.\n",
    "\n",
    "$\\qquad$ ![](https://wikidocs.net/images/page/33793/lookup_table.PNG)\n",
    "\n",
    "- 위의 그림은 단어 \"great\"이 정수 인코딩된 후 테이블로부터 해당 인덱스에 위치한 임베딩 벡터를 꺼내오는 모습을 보여준다.\n",
    "- 위의 그림에서는 임베딩 벡터의 차원이 4로 설정되어 있다.\n",
    "- 그리고 단어 \"great\"은 정수 인코딩 과정에서 1,918의 정수로 인코딩되었다.\n",
    "- 그에 따라 단어 집합의 크기 만큼의 행을 가지는 테이블에서 인덱스 1,918번에 위치한 행을 단어 \"great\"의 임베딩 벡터로 사용한다.\n",
    "- 이 임베딩 벡터는 모델의 입력이 되고, 역전파 과정에서 단어 \"great\"의 임베딩 벡터값이 학습된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tBQNoGMgbUrw"
   },
   "source": [
    "- 룩업 테이블의 개념을 이론적으로 우선 접하고, 처음 케라스를 배울 때 어떤 분들은 임베딩 층의 원-핫 벡터가 아니어도 동작한다는 점에 헷갈려 한다.\n",
    "- 케라스는 단어를 정수 인덱스로 바꾸고, 원-핫 벡터로 한 번 더 바꾸고 나서 임베딩 층의 입력으로 사용하는 것이 아니다.\n",
    "- 단어를 정수 인덱스로만 바꾼 채로 임베딩 층의 입력으로 사용해도 룩업 테이블된 결과인 임베딩 벡터를 리턴한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a2k4RM8rngsV"
   },
   "source": [
    "- 케라스의 임베딩 층 구현 코드는 다음과 같다.\n",
    "\n",
    "```python\n",
    "# 아래의 각 인자는 저자가 임의로 선정한 숫자들이며 의미 있는 선정 기준이 아님\n",
    "v = Embedding(20000, 128, input_length=500)\n",
    "# vocab_size = 20000\n",
    "# output_dim = 128\n",
    "# input_length = 500\n",
    "```\n",
    "\n",
    "- 임베딩 층은 다음과 같은 세 개의 인자를 받는다.\n",
    "  1. `vocab_size` : 텍스트 데이터의 전체 단어 집합의 크기이다.\n",
    "  2. `output_dim` : 워드 임베딩 후의 임베딩 벡터의 차원이다.\n",
    "  3. `input_length` : 입력 시퀀스의 길이이다. 만약 갖고 있는 각 샘플의 길이가 500개의 단어로 구성되어 있다면 이 값은 500이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9CK5JFlanwEF"
   },
   "source": [
    "- `Embedding()`은 `(number of samples, input_length)`인 2D 정수 텐서를 입력받는다.\n",
    "- 이 때 각 sample은 정수 인코딩된 결과로, 정수의 시퀀스이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DWlNkbkgoUiN"
   },
   "source": [
    "- `Embedding()`은 워드 임베딩 작업을 수행하고 `(number of samples, input_length, embeddingi word dimentionality)`인 3D 실수 텐서를 리턴한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-HcCtApod8c"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 5.1.2 임베딩 층 사용하기\n",
    "\n",
    "- RNN 챕터에서 이미 사용한 바 있지만, 임베딩 층을 복습해보자.\n",
    "- 문장의 긍, 부정을 판단하는 감성 분류 모델을 만들어 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G1fIf_9Dpmpc"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 5.1.2.1 필요 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yROqaytoonZ3"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4zP66-yLo0ki"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 5.1.2.2 데이터 생성\n",
    "\n",
    "- 문장과 레이블 데이터를 생성한다.\n",
    "- 긍정인 문장은 레이블 1, 부정인 문장은 레이블이 0이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TBbKboHcovvg"
   },
   "outputs": [],
   "source": [
    "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n",
    "y_train = [1, 0, 0, 1, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "exDqHsNyo6lO"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 5.1.2.3 토큰화 수행\n",
    "\n",
    "- 케라스의 `Tokenizer()`를 사용하여 토큰화를 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "2Y_z3z3apAFF",
    "outputId": "5432e753-7311-4e05-9408-5dde2d919481"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(sentences)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B7REeSH1pH6F"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 5.1.2.4 정수 인코딩 수행\n",
    "\n",
    "- 각 문장에 대해서 정수 인코딩을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "JqP5EPX1pMa8",
    "outputId": "3937cfae-eb0e-428e-a681-cbc74a40e37d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]\n"
     ]
    }
   ],
   "source": [
    "X_encoded = t.texts_to_sequences(sentences)\n",
    "print(X_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qAaCmD0apSPG"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 5.1.2.5 패딩\n",
    "\n",
    "- 문장 중에서 가장 길이가 긴 문장의 길이는 4이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "8Fc7EVJIpWp9",
    "outputId": "ae7543eb-24b8-4c2f-f558-cba0c885c695"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(l) for l in X_encoded)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wI1If7iLpaPF"
   },
   "source": [
    "<br>\n",
    "\n",
    "- 모든 문장을 패딩하여 길이를 4로 만들어 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "CKIZn_K-p1q4",
    "outputId": "8537d638-9b9d-4ae3-cf74-08613a06d49f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  0  0]\n",
      " [ 7  8  0  0]\n",
      " [ 9 10  0  0]\n",
      " [11 12  0  0]\n",
      " [13  0  0  0]\n",
      " [14 15  0  0]]\n"
     ]
    }
   ],
   "source": [
    "X_train = pad_sequences(X_encoded, maxlen=max_len, padding='post')\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RYEQq4RCp_Gh"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 5.1.2.6 모델 설계\n",
    "\n",
    "- 출력층에 1개의 뉴런에 활성화 함수로는 시그모이드 함수를 사용하여 이진 분류를 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Xv07wNVqDrN"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 4, input_length=max_len)) # 모든 임베딩 벡터는 4차원이다.\n",
    "model.add(Flatten()) # Dense의 입력으로 넣기 위함\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1JygjtSrqfcW"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 5.1.2.7 모델 훈련 및 평가\n",
    "\n",
    "- 테스트 데이터에 대한 정확도가 아니며 훈련 데이터도 양이 적어서 정확도에 의미는 없다.\n",
    "- 하지만 여기서 말하고자 하는 점은 현재 각 단어들의 임베딩 벡터들의 값은 학습 과정에서 다른 가중치들과 함께 학습된 값이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q9g30NUqqtyO"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(X_train, y_train, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dbqveFTqq2k4"
   },
   "source": [
    "```\n",
    "Epoch 1/100\n",
    "1/1 - 0s - loss: 0.7002 - acc: 0.4286\n",
    "Epoch 2/100\n",
    "1/1 - 0s - loss: 0.6987 - acc: 0.5714\n",
    "Epoch 3/100\n",
    "1/1 - 0s - loss: 0.6971 - acc: 0.5714\n",
    "\n",
    "...중략...\n",
    "\n",
    "Epoch 98/100\n",
    "1/1 - 0s - loss: 0.5297 - acc: 1.0000\n",
    "Epoch 99/100\n",
    "1/1 - 0s - loss: 0.5276 - acc: 1.0000\n",
    "Epoch 100/100\n",
    "1/1 - 0s - loss: 0.5255 - acc: 1.0000\n",
    "<tensorflow.python.keras.callbacks.History at 0x7f0db02e5c18>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "48UBCJ-Qq-KV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch10_v05_Pre-trained-Word-Embedding.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
