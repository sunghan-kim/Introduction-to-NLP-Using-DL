{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lm3NfoYQqL9S"
   },
   "source": [
    "# Ch10. 워드 임베딩 (Word Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N_iesEZaqY89"
   },
   "source": [
    "# v02. 워드투벡터 (Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g-akwHY-qcQl"
   },
   "source": [
    "- 원-핫 벡터는 단어 간 유사도를 계산할 수 없는 단점이 있다.\n",
    "- 그래서 단어 간 유사도를 반영할 수 있도록 단어의 의미를 벡터화 할 수 있는 방법이 필요하다.\n",
    "- 그리고 이를 위해서 사용되는 대표적인 방법이 **워드투벡터(Word2Vec)**이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KwHyk8r8qtQE"
   },
   "source": [
    "- Word2Vec의 개념을 설명하기에 앞서 Word2Vec가 어떤 일을 할 수 있는 지 먼저 확인해보자.\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/22660/word2vec.PNG)\n",
    "\n",
    "- [http://w.elnn.kr/search/](http://w.elnn.kr/search/)\n",
    "\n",
    "- 위 사이트는 한국어 단어에 대해서 벡터 연산을 해볼 수 있는 사이트이다.\n",
    "- 위 사이트에서는 단어들(실제로는 Word2Vec 벡터)로 더하기, 빼기 연산을 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LG3NDL0Ersha"
   },
   "source": [
    "- 예를 들어 아래의 식에서 좌변을 집어 넣으면, 우변의 답들이 나온다.\n",
    "\n",
    "> 고양이 + 애교 = 강아지  \n",
    "한국 - 서울 + 도쿄 = 일본  \n",
    "박찬호 - 야구 + 축구 = 호나우두\n",
    "\n",
    "- 단어가 가지고 있는 어떤 의미들을 가지고 연산을 하고 있는 것처럼 보인다.\n",
    "- 이런 연산이 가능한 이유는 각 단어 벡터가 단어 간 유사도를 반영한 값을 가지기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FzW-_Z5mr_fJ"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 2.1 희소 표현 (Sparse Representation)\n",
    "\n",
    "- 원-핫 인코딩을 통해서 나온 원-핫 벡터들은 표현하고자 하는 단어의 인덱스의 값만 1이고, 나머지 인덱스에는 전부 0으로 표현되는 벡터 표현 방법이었다.\n",
    "- 이렇게 벡터 또는 행렬(matrix)의 값이 대부분이 0으로 표현되는 방법을 희소 표현(sparse representation)이라고 한다.\n",
    "- 그러니까 원-핫 벡터는 희소 벡터(sparse vector)이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E2tWM61euGD-"
   },
   "source": [
    "- 하지만 이러한 표현 방법은 각 단어간 유사성을 표현할 수 없다는 단점이 있다.\n",
    "- 이를 위한 대안으로 단어의 '의미'를 다차원 공간에 벡터화하는 방법을 찾게 된다.\n",
    "- 이러한 표현 방법을 **분산 표현(distributed representation)**이라고 한다.\n",
    "- 그리고 이렇게 분산 표현을 이용하여 단어의 유사도를 벡터화하는 작업은 워드 임베딩(embedding) 작업에 속한다.  \n",
    "$\\rightarrow$ 이렇게 표현된 벡터 또한 **임베딩 벡터(embedding vector)**라고 한다.  \n",
    "- 또한 저차원을 가지므로 **밀집 벡터(dense vector)**에도 속한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wRsRp6pRuvBd"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 2.2. 분산 표현 (Distributed Representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EE6nIVjjyv3z"
   },
   "source": [
    "### 2.2.1 분포 가설 (distributional hypothesis)\n",
    "\n",
    "- 분산 표현(distributed representation) 방법은 기본적으로 **분포 가설(distributional hypothesis)**이라는 가정 하에 만들어진 표현 방법이다.\n",
    "- 이 가정은 **'비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다'**라는 가정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IOc9SLl2yYMr"
   },
   "source": [
    "- \"강아지\"란 단어는 \"귀엽다\", \"예쁘다\", \"애교\" 등의 단어가 주로 함께 등장한다.\n",
    "- 이는 분포 가설에 따라서 저런 내용을 가진 텍스트를 벡터화한다면 저 단어들은 의미적으로 가까운 단어가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jiKnAx_nykgL"
   },
   "source": [
    "- 분산 표현은 분포 가설을 이용하여 단어들의 셋을 학습하고, 벡터에 단어의 의미를 여러 차원에 분산하여 표현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q9fph3pIytG6"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.2.2 벡터의 차원 감소\n",
    "\n",
    "- 이렇게 표현된 벡터들은 원-핫 벡터처럼 벡터의 차원이 단어 집합(vocabulary)의 크기일 필요가 없다.\n",
    "- 그렇기 때문에 벡터의 차원이 상대적으로 저차원으로 줄어든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GFHbJoFYy-SA"
   },
   "source": [
    "- 예를 들어 단어가 10,000개 있고 인덱스가 1부터 시작한다고 했을 때 강아지란 단어의 인덱스는 5였다면 강아지란 단어를 표현하는 원-핫 벡터는 다음과 같다.\n",
    "\n",
    "> Ex) 강아지 = `[ 0 0 0 0 1 0 0 0 0 0 0 0 ... 중략 ... 0]`\n",
    "\n",
    "- 1이란 값 뒤에 0이 9,995개가 있는 벡터가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Jpd9jgUzTrp"
   },
   "source": [
    "- 하지만 Word2Vec로 임베딩 된 벡터는 굳이 벡터의 차원이 단어 집합의 크기가 될 필요가 없다.\n",
    "- 강아지란 단어를 표현하기 위해 사용자가 설정한 차원을 가지는 벡터가 되면서 각 차원은 실수형의 값을 가진다.\n",
    "\n",
    "> Ex) 강아지 = `[0.2 0.3 0.5 0.7 0.2 ... 중략 ... 0.2]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SrQmRqbpzgtB"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.2.3 요약\n",
    "\n",
    "- 희소 표현\n",
    "  - 고차원에 각 차원이 분리된 표현 방법  \n",
    "\n",
    "\n",
    "- 분산 표현\n",
    "  - 저차원에 **단어의 의미를 여러 차원에다가 분산**하여 표현\n",
    "  - 이런 표현 방법을 사용하면 **단어 간 유사도**를 계산할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zNGISeQjz4Qo"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.2.4 분산 표현 학습 방법\n",
    "\n",
    "- NNLM, RNNLM 등이 있다.\n",
    "- 요즘에는 해당 방법들의 속도를 대폭 개선시킨 Word2Vec가 많이 쓰이고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klRtPVSFz_73"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 2.3 CBOW (Continuous Bag of Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ksnXP0Qc0HLH"
   },
   "source": [
    "### 2.3.1 Word2Vec의 두 가지 방식\n",
    "\n",
    "- CBOW (Continuous Bag of Words)\n",
    "  - 주변에 있는 단어들을 가지고, 중간에 있는 단어들을 예측하는 방법  \n",
    "\n",
    "\n",
    "- Skip-Gram\n",
    "  - 중간에 있는 단어로 주변 단어들을 예측하는 방법  \n",
    "\n",
    "\n",
    "- 위 두 가지 방법의 메커니즘 자체는 거의 동일하다.\n",
    "- 그렇기 때문에 CBOW를 이해하면 Skip-Gram도 손쉽게 이해 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qFkIaTdl0flX"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.3.2 중심 단어(center word)와 주변 단어(context word)\n",
    "\n",
    "> **예문 : \"The fat cat sat on the mat**\n",
    "\n",
    "- 갖고 있는 코퍼스에 위와 같은 문장이 있다고 하자.\n",
    "- 가운데 단어를 예측하는 것이 CBOW이다.\n",
    "- 즉, `{\"The\", \"fat\", \"cat\", \"on\", \"the\", \"mat}`으로부터 \"sat\"을 예측하는 것이 CBOW가 하는 일이다.\n",
    "- 이 때 예측해야 하는 단어 \"sat\"을 **중심 단어(center word)**라고 한다.\n",
    "- 예측에 사용되는 단어들을 **주변 단어(context word)**라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i19TEGiQ1DPP"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.3.3 윈도우 (window)\n",
    "\n",
    "- 중심 단어를 예측하기 위해서 앞, 뒤로 몇 개의 단어를 볼 지를 결정했다면 이 범위를 **윈도우(window)**라고 한다.\n",
    "- 예를 들어 윈도우 크기가 2이고, 예측하고자 하는 중심 단어가 \"sat\"이라고 한다면 앞의 두 단어인 \"fat\", \"cat\", 그리고 뒤의 두 단어인 \"on\", \"the\"를 참고한다.\n",
    "- 윈도우 크기가 n이라고 한다면, 실제 중심 단어를 예측하기 위해 참고하려고 하는 주변 단어의 개수는 2n이 될 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZoZm7lOJ1n2c"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.3.4 슬라이딩 윈도우 (sliding window)\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/22660/%EB%8B%A8%EC%96%B4.PNG)\n",
    "\n",
    "- 윈도우 크기를 정했다면, 윈도우를 계속 움직여서 주변 단어와 중심 단어 선택을 바꿔가며 학습을 위한 데이터 셋을 만들 수 있다.\n",
    "- 이 방법을 **슬라이딩 윈도우(sliding window)**라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r0_x6bGI13Xz"
   },
   "source": [
    "- 위 그림에서 좌측의 중심 단어와 주변 단어의 변화는 윈도우 크기가 2일 때, 슬라이딩 윈도우가 어떤 식으로 이루어지면서 데이터 셋을 만드는 지 보여준다.\n",
    "- Word2Vec에서 입력은 모두 원-핫 벡터가 되어야 한다.\n",
    "- 우측 그림은 중심 단어와 주변 단어를 어떻게 선택했을 때에 따라서 각각 어떤 원-핫 벡터가 되는 지를 보여준다.\n",
    "- 위 그림은 결국 CBOW를 위한 전체 데이터 셋을 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LObkIf5KJfb_"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.3.5 CBOW의 인공 신경망 도식화\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/22660/word2vec_renew_1.PNG)\n",
    "\n",
    "- 입력층(Input layer)의 입력으로서 앞, 뒤 사용자가 정한 윈도우 크기 범위 안에 있는 주변 단어들의 원-핫 벡터가 들어간다.\n",
    "- 출력층(Output layer)에서 예측하고자 하는 중간 단어의 원-핫 벡터가 필요하다.\n",
    "- 뒤에서 설명하겠지만, Word2Vec의 학습을 위해서 이 중간 단어의 원-핫 벡터가 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KUvx_nEeKPR3"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.3.6 Word2Vec의 은닉층\n",
    "\n",
    "- 또한 위 그림에서 알 수 있는 사실은, Word2Vec은 딥 러닝 모델(Deep Learning Model)은 아니라는 점이다.\n",
    "- 보통 딥 러닝이라 함은, 입력층과 출력층 사이의 은닉층의 개수가 충분히 쌓인 신경망을 학습할 때를 말한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V_fvVyWxLCOc"
   },
   "source": [
    "- Word2Vec은 입력층과 출력층 사이에 하나의 은닉층만이 존재한다.\n",
    "- 이렇게 은닉층(hidden Layer)이 1개인 경우에는 일반적으로 심층신경망(Deep Neural Network)이 아니라 **얕은신경망(Shallow Neural Network)**이라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dWxdbuG4K-pR"
   },
   "source": [
    "- 또한 Word2Vec의 은닉층은 일반적인 은닉층과는 달리 **활성화 함수가 존재하지 않는다.**\n",
    "- 그 대신 룩업 테이블이라는 연산을 담당하는 층으로 일반적인 은닉층과 구분하기 위해 **투사층(projection layer)**이라고 부르기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EG-1cg_eK_Ss"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 2.3.7 CBOW의 동작 메커니즘\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/22660/word2vec_renew_2.PNG)\n",
    "\n",
    "- 위 그림에서 주목해야 할 것은 두 가지이다.\n",
    "  1. 투사층의 크기\n",
    "  2. 가중치 행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "joy2khkzLcNi"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 2.3.7.1 투사층의 크기\n",
    "\n",
    "- 위 그림에서 투사층의 크기는 M이다.\n",
    "- CBOW에서 투사층의 크기 M은 임베딩하고 난 벡터의 차원이 된다.\n",
    "- 다시 말해, 위의 그림에서 투사층의 크기는 M=5이기 때문에 CBOW를 수행하고 나서 얻는 각 단어의 임베딩 벡터의 차원은 5가 될 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n3PokgQDLdHa"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 2.3.7.2 가중치 행렬\n",
    "\n",
    "- 입력층과 투사층 사이의 가중치 W는 V x M 행렬이다.\n",
    "- 투사층과 출력층 사이의 가중치 W'는 M x V 행렬이다.\n",
    "- 여기서 V는 단어 집합의 크기를 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AEmnVbTjMKDk"
   },
   "source": [
    "- 즉, 위의 그림처럼 원-핫 벡터의 차원은 7이고, M은 5라고 하면  \n",
    "$\\rightarrow$ 가중치 W는 7 x 5 행렬이다.  \n",
    "$\\rightarrow$ 가중치 W'는 5 x 7 행렬이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v9gq-HzBMKfq"
   },
   "source": [
    "- 주의할 점은 **이 두 행렬은 동일한 행렬을 전치(transpose)한 것이 아니라, 서로 다른 행렬이라는 점**이다.\n",
    "- 인공 신경망의 훈련 전에 이 가중치 행렬 W와 W'는 대게 굉장히 작은 랜덤 값을 가지게 된다.\n",
    "- CBOW는 주변 단어로 중심 단어를 더 정확히 맞추기 위해 계속해서 이 W와 W'를 학습해가는 구조이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ya_ci-JsMcLx"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 2.3.7.3 룩업 테이블 (lookup table)\n",
    "\n",
    "- 입력으로 들어오는 주변 단어의 원-핫 벡터와 가중치 W 행렬의 곱이 어떻게 이루어지는 지 보자.\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/22660/word2vec_renew_3.PNG)\n",
    "\n",
    "- 위 그림에서 각 주변 단어의 원-핫 벡터를 $x$로 표기했다.\n",
    "- 입력 벡터는 원-핫 벡터이다.\n",
    "- i번 째 인덱스에 1이라는 값을 가지고 그 이외의 0의 값을 가지는 입력 벡터와 가중치 W 행렬의 곱은 사실 W 행렬의 i번째 행을 그대로 읽어오는 것(lookup)과 동일하다.\n",
    "- 그래서 이 작업을 룩업 테이블(lookup table)이라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IEECFj4KOqxN"
   },
   "source": [
    "- 앞서 CBOW의 목적은 W와 W'를 잘 훈련 시키는 것이라고 언급했다.\n",
    "- 사실 그 이유가 여기서 lookup해 온 W의 각 행벡터가 사실 Word2Vec을 수행한 후 각 단어의 M 차원의 크기를 갖는 임베딩 벡터들이기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_yZpOvOWO7Jr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch10_v02_Word2Vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
