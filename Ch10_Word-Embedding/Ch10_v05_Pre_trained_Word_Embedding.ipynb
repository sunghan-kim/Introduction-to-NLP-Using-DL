{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MInW092J8tIC"
   },
   "source": [
    "# Ch10. 워드 임베딩 (Word Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gp7DxrEN80Aa"
   },
   "source": [
    "# v05. 사전 훈련된 워드 임베딩 (Pre-trained Word Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LzeQbuy-Vdte"
   },
   "source": [
    "- 이번 챕터에서는 **케라스의 임베딩 층(embedding layer)**과 **사전 훈련된 워드 임베딩(pre-trained word embedding)**을 가져와서 사용하는 것을 비교한다.\n",
    "- 자연어 처리를 구현하려고 할 때 갖고 있는 훈련 데이터의 단어들을 임베딩 층(embedding layer)을 구현하여 임베딩 벡터로 학습하는 경우가 있다.\n",
    "- 케라스에서는 이를 `Embedding()`이라는 도구를 사용하여 구현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0q-Fr_IVzR1"
   },
   "source": [
    "- 그런데 위키피디아 등과 같은 방대한 코퍼스를 가지고 Word2Vec, FastText, GloVe 등을 통해서 이미 미리 훈련된 임베딩 벡터를 불러오는 방법을 사용하는 경우도 있다.\n",
    "- 이는 현재 갖고 있는 훈련 데이터를 임베딩 층으로 처음부터 학습을 하는 방법과는 대조된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nkuhtddPWn2z"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 5.1 케라스 임베딩 층 (Keras Embedding layer)\n",
    "\n",
    "- 케라스는 훈련 데이터의 단어들에 대해 워드 임베딩을 수행하는 도구 `Embedding()`을 제공한다.\n",
    "- `Embedding()`은 인공 신경망 구조 관점에서 임베딩 층(embedding layer)을 구현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jm_au1MOW0f7"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 5.1.1 임베딩 층은 룩업 테이블이다.\n",
    "\n",
    "- 임베딩 층의 입력으로 사용하기 위해서 입력 시퀀스의 각 단어들은 모두 정수 인코딩이 되어야 한다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "\\text{어떤 단어} \\; \\rightarrow \\; \n",
    "\\text{단어에 부여된 고유한 정수값} \\; \\rightarrow \\; \n",
    "\\text{임베딩 층 통과} \\; \\rightarrow \\; \n",
    "\\text{밀집 벡터}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2iftPq86W3Fi"
   },
   "source": [
    "- 임베딩 층은 입력 정수에 대해 밀집 벡터(dense vector)로 맵핑한다.\n",
    "- 이 밀집 벡터는 인공 신경망의 학습 과정에서 가중치가 학습되는 것과 같은 방식으로 훈련된다.\n",
    "- 훈련 과정에서 단어는 모델이 풀고자하는 작업에 맞는 값으로 업데이트된다.\n",
    "- 그리고 이 밀집 벡터를 임베딩 벡터라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AvdBbCH7Z0qb"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 5.1.1.1 정수를 밀집 벡터 또는 임베딩 벡터로 맵핑한다는 것의 의미\n",
    "\n",
    "- 특정 단어와 맵핑되는 정수를 인덱스로 가지는 테이블로부터 임베딩 벡터 값을 가져오는 룩업 테이블이라고 볼 수 있다.\n",
    "- 그리고 이 테이블은 단어 집합의 크기만큼의 행을 가지므로 모든 단어는 고유한 임베딩 벡터를 가진다.\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/33793/lookup_table.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tBQNoGMgbUrw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch10_v05_Pre-trained-Word-Embedding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
