{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "htH2IJ7FWTr4"
   },
   "source": [
    "# Ch13. 기계 번역 (Neural Machine Translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hRMJEmxEWXMH"
   },
   "source": [
    "# v01. 시퀀스-투-시퀀스(Sequence-to-Sequence, seq2seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jclKRAp8WhFX"
   },
   "source": [
    "**참고**\n",
    "\n",
    "- 이번 챕터의 실습은 케라스 함수형 API에 대한 이해가 필요하다.\n",
    "- [4-7 챕터](https://wikidocs.net/38861)를 참고하면 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6xe6yXizugXm"
   },
   "source": [
    "- 시퀀스-투-시퀀스(Sequence-to-Sequence)는 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 다양한 분야에서 사용되는 모델이다.\n",
    "- 예를 들어 **챗봇(Chatbot)**과 **기계 번역(Machine Translation)**이 그러한 대표적인 예이다.\n",
    "  - 챗봇 : 입력 시퀀스와 출력 시퀀스를 각각 질문과 대답으로 구성\n",
    "  - 기계 번역 : 입력 시퀀스와 출력 시퀀스를 각각 입력 문장과 번역 문장으로 만듬\n",
    "- 그 외에도 **내용 요약(Text Summarization)**, **STT(Sppech to Text)** 등에서 쓰일 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gUTCesSF_M-U"
   },
   "source": [
    "- 이번 챕터에서는 기계 번역을 예제로 시퀀스-투-시퀀스를 설명한다.\n",
    "- 앞으로는 줄여서 seq2seq이라는 이름으로 설명한다.\n",
    "- seq2seq2에 대한 구조를 이해하고, 케라스(keras)를 통해 직접 구현해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rWYsdZCQ_Ws5"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 1.1 시퀀스-투-시퀀스 (Sequence)\n",
    "\n",
    "- seq2seq는 번역기에서 대표적으로 사용되는 모델이다.\n",
    "- 앞으로의 설명 방식은 내부가 보이지 않는 커다란 블랙 박스에서 점차적으로 확대해가는 방식으로 설명한다.\n",
    "- 참고로 여기서 설명하는 내용의 대부분은 RNN 챕터에서 언급한 내용들이다.\n",
    "- 단지 이것을 가지고 어떻게 조립했느냐에 따라서 seq2seq라는 구조가 만들어진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MEuEuUDGANsW"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.1.1 seq2seq 모델의 구성\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/24996/%EC%8B%9C%ED%80%80%EC%8A%A4%ED%88%AC%EC%8B%9C%ED%80%80%EC%8A%A4.PNG)\n",
    "\n",
    "- 위의 그림은 seq2seq 모델로 만들어진 번역기가 'I am a student'라는 영어 문장을 입력 받아서, 'je suis étudiant'라는 프랑스 문장을 출력하는 모습을 보여준다.\n",
    "- 그렇다면, seq2seq 모델 내부의 모습은 어떻게 구성되어 있을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gJ0e1UCdAfbO"
   },
   "source": [
    "$\\quad$ ![](https://wikidocs.net/images/page/24996/seq2seq%EB%AA%A8%EB%8D%B811.PNG)\n",
    "\n",
    "- seq2seq는 크게 두 개로 구성된 아키텍처로 구성된다.\n",
    "- 바로 **인코더**와 **디코더**이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYr4dsgxAsA1"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 1.1.1.1 인코더 (Encoder)\n",
    "\n",
    "- 인코더는 입력 문장의 모든 단어들을 순차적으로 입력받은 뒤에 마지막에 이 모든 단어 정보들을 압축해서 하나의 벡터로 만든다.\n",
    "- 이를 **컨텍스트 벡터(context vector)**라고 한다.\n",
    "- 입력 문장의 정보가 하나의 컨텍스트 벡터로 모두 압축되면 인코더는 컨텍스트 벡터를 디코더로 전송한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "337wBqtUA-JN"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 1.1.1.2 디코더 (Decoder)\n",
    "\n",
    "- 디코더는 컨텍스트 벡터를 받아서 번역된 단어를 한 개씩 순차적으로 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wMNF0s9VBGCs"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 1.1.1.3 컨텍스트 벡터\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/24996/%EC%BB%A8%ED%85%8D%EC%8A%A4%ED%8A%B8_%EB%B2%A1%ED%84%B0.PNG)\n",
    "\n",
    "- 위의 그림에서는 컨텍스트 벡터를 4의 사이즈로 표현하였다.\n",
    "- 실제 현업에서 사용되는 seq2seq 모델에서는 보통 수백 이상의 차원을 갖고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hD02LnlVBSjz"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.1.2 인코더 및 디코더 내부 구조\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/24996/%EC%9D%B8%EC%BD%94%EB%8D%94%EB%94%94%EC%BD%94%EB%8D%94%EB%AA%A8%EB%8D%B8.PNG)\n",
    "\n",
    "- 인코더 아키텍처와 디코더 아키텍처의 내부는 사실 **두 개의 RNN 아키텍처**이다.\n",
    "  - 입력 문장을 받는 RNN 셀 $\\rightarrow$ 인코더 (인코더의 RNN 셀을 주황색으로 표현)\n",
    "  - 출력 문장을 출력하는 RNN 셀 $\\rightarrow$ 디코더 (디코더의 RNN 셀을 초록색으로 표현)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "unNxUrPTB2my"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 1.1.2.1 RNN 셀의 종류\n",
    "\n",
    "- 물론, 성능 문제로 인해 실제로는 바닐라 RNN이 아니라 **LSTM 셀** 또는 **GRU 셀**들로 구성된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9hlxK850CDy6"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 1.1.2.2 인코더 내부 구조\n",
    "\n",
    "- 입력 문장은 단어 토큰화를 통해서 단어 단위로 쪼개진다.\n",
    "- 단어 토큰 각각은 RNN 셀의 각 시점의 입력이 된다.\n",
    "- 인코더 RNN 셀은 모든 단어를 입력 받은 뒤에 **인코더 RNN 셀의 마지막 시점의 은닉 상태**를 디코더 RNN 셀로 넘겨준다.\n",
    "- 이를 **컨텍스트 벡터**라고 한다.\n",
    "- 컨텍스트 벡터는 **디코더 RNN 셀의 첫 번째 은닉 상태**로 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8og8m6sOCoQ6"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 1.1.2.3 디코더 내부 구조\n",
    "\n",
    "- 디코더는 기본적으로 RNNLM(RNN Language Model)이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_u9Y4GEACrCD"
   },
   "source": [
    "- 디코더는 초기 입력으로 문장의 시작을 의미하는 심볼 `<sos>`가 들어간다.\n",
    "- 디코더는 `<sos>`가 입력되면, 다음에 등장할 확률이 높은 단어를 예측한다.\n",
    "- 첫 번째 시점(time step)의 디코더 RNN 셀은 다음에 등장할 단어로 `je`를 예측했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "504RXamgDP6S"
   },
   "source": [
    "- 첫 번째 시점의 디코더 RNN 셀은 예측된 단어 `je`를 다음 시점의 RNN 셀의 입력으로 입력한다.\n",
    "- 그리고 두 번째 시점의 디코더 RNN 셀은 입력된 단어 `je`로부터 다시 다음에 올 단어인 `suis`를 예측한다.\n",
    "- 그런 다음 다시 이것을 다음 시점의 RNN 셀의 입력으로 보낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kj8HFWadDbqn"
   },
   "source": [
    "- 디코더는 이런 식으로 기본적으로 다음에 올 단어를 예측하고, 그 예측한 단어를 다음 시점의 RNN 셀의 입력으로 넣는 행위를 반복한다.\n",
    "- 이 행위는 문장의 끝을 의미하는 심볼인 `<eos>`가 다음 단어로 예측될 때까지 반복된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YcoorPwwDnL2"
   },
   "source": [
    "- **지금 설명하는 것은 테스트 과정** 동안의 이야기이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MYmhGb9IDqqX"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.1.3 seq2seq의 훈련 과정과 테스트 과정의 차이\n",
    "\n",
    "- seq2seq는 훈련 과정과 테스트 과정(또는 실제 번역기를 사람이 쓸 때)의 작동 방식이 조금 다르다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L3oUHl3FFsm8"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 1.1.3.1 훈련 과정\n",
    "\n",
    "- 훈련 과정에서는 디코더에게 인코더가 보낸 컨텍스트 벡터와 실제 정답인 상황인 `<sos> je suis étudiant`를 입력받았을 때, `je suis étudiant <eos>`가 나와야 된다고 정답을 알려주면서 훈련한다.\n",
    "- 이에 대해서는 뒤에 **교사 강요(teacher forcing)**를 설명하면서 다시 언급한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nGdRGp3CFwTZ"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 1.1.3.2 테스트 과정\n",
    "\n",
    "- 반면 테스트 과정에서는 앞서 설명한 과정과 같이 디코더는 오직 컨텍스트 벡터와 `<sos>`만을 입력으로 받은 후에 다음에 올 단어를 예측한다.\n",
    "- 그런 다음 그 단어를 다음 시점의 RNN 셀의 입력으로 넣는 행위를 반복한다.\n",
    "- 즉, 앞서 설명한 과정과 위의 그림은 테스트 과정에 해당된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XkdFhaztGHDD"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.1.4 입출력에 쓰이는 단어 토큰들\n",
    "\n",
    "- 이번에는 입, 출력에 쓰이는 단어 토큰들이 있는 부분을 좀 더 확대해보자.\n",
    "\n",
    "$\\qquad$ ![](https://wikidocs.net/images/page/24996/%EB%8B%A8%EC%96%B4%ED%86%A0%ED%81%B0%EB%93%A4%EC%9D%B4.PNG)\n",
    "\n",
    "- 기계는 텍스트보다 숫자를 잘 처리한다.\n",
    "- 그리고 자연어 처리에서 텍스트를 벡터로 바꾸는 방법으로는 워드 임베딩(9챕터 참고)이 사용된다.\n",
    "- 즉, seq2seq에서 사용되는 모든 단어들은 워드 임베딩을 통해 임베딩 벡터로서 표현된 임베딩 벡터이다.\n",
    "- 위 그림은 모든 단어에 대해서 임베딩 과정을 거치게 하는 단계인 임베딩 층(embedding layer)의 모습을 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NxA2ItbRGtYB"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.1.5 각각의 입력 단어의 임베딩 벡터\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/24996/%EC%9E%84%EB%B2%A0%EB%94%A9%EB%B2%A1%ED%84%B0.PNG)\n",
    "\n",
    "- 예를 들어, `I`, `am`, `a`, `student`라는 단어들에 대한 임베딩 벡터는 위와 같은 모습을 가진다.\n",
    "- 여기서는 그림으로 표현하고자 사이즈를 5로 하였지만, 보통 실제 임베딩 벡터는 수백 개의 차원을 가질 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BCvak4YNHFi3"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.1.6 RNN 셀\n",
    "\n",
    "- 이제 RNN 셀에 대해서 확대해보자.\n",
    "- 하나의 RNN 셀은 각각의 시점(time step)마다 두 개의 입력을 받는다.\n",
    "\n",
    "$\\qquad$ ![](https://wikidocs.net/images/page/24996/rnn%EA%B7%BC%ED%99%A9.PNG)\n",
    "\n",
    "- 현재 시점(time step)을 t라고 하자.\n",
    "- RNN 셀은 t-1에서의 은닉 상태와 t에서의 입력 벡터를 입력으로 받는다.\n",
    "- 이를 이용하여 t에서의 은닉 상태를 만든다.\n",
    "- 이 때 t에서의 은닉 상태는 바로 위에 또 다른 은닉층이나 출력층이 존재할 경우에는 위의 층으로 보내거나, 필요 없으면 값을 무시할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hBXRt3OKHJiP"
   },
   "source": [
    "- 그리고 RNN 셀은 다음 시점에 해당하는 t+1의 RNN 셀의 입력으로 현재 t에서의 은닉 상태를 입력으로 보낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RFhxcTDrNCOb"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.1.7 인코더에서의 컨텍스트 벡터의 의미\n",
    "\n",
    "- 이런 구조에서 현재 시점 t에서의 은닉 상태는 과거 시점의 동일한 RNN 셀에서의 모든 은닉 상태의 값들의 영향을 누적해서 받아온 값이라고 할 수 있다.\n",
    "- 그렇기 때문에 앞서 우리가 언급했던 **컨텍스트 벡터**는 사실 **인코더에서의 마지막 RNN 셀의 은닉 상태값**을 말하는 것이다.\n",
    "- 이는 입력 문장의 모든 단어 토큰들의 정보를 요약해서 담고 있다고 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vtmxC2CqNmuM"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.1.8 디코더에서의 컨텍스트 벡터의 의미\n",
    "\n",
    "- 디코더는 인코더의 마지막 RNN 셀의 은닉 상태인 컨텍스트 벡터를 첫 번째 은닉 상태의 값으로 사용한다.\n",
    "- 디코더의 첫 번째 RNN 셀은 이 첫 번째 은닉 상태의 값과, 현재 t에서의 입력값인 `<sos>`로부터 다음에 등장할 단어를 예측한다.\n",
    "- 그리고 이 예측된 단어는 다음 시점인 t+1 RNN에서의 입력값이 된다.\n",
    "- 이 t+1에서의 RNN 또한 이 입력값과 t에서의 은닉 상태로부터 t+1에서의 출력 벡터, 즉 또 다시 다음에 등장할 단어를 예측하게 될 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V0xYMeZFN2tX"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.1.9 디코더의 다음 단어 예측\n",
    "\n",
    "- 이제 디코더가 다음에 등장할 단어를 예측하는 부분을 확대해보자.\n",
    "\n",
    "$\\quad$ ![](https://wikidocs.net/images/page/24996/decodernextwordprediction.PNG)\n",
    "\n",
    "- 출력 단어로 나올 수 있는 단어들은 다양한 단어들이 있다.\n",
    "- seq2seq 모델은 선택될 수 있는 모든 단어들로부터 하나의 단어를 골라서 예측해야 한다.\n",
    "- 이를 예측하기 위해서 사용할 수 있는 함수가 바로 **소프트맥스 함수**이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y9wS6d6xPRwe"
   },
   "source": [
    "- 디코더에서 각 시점(time step)의 RNN 셀에서 출력 벡터가 나오면, 해당 벡터는 소프트맥스 함수를 통해서 출력 시퀀스의 각 단어별 확률값을 반환하고,디코더는 출력 단어를 결정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J27yHsRqPdWz"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.1.10 seq2seq의 활용\n",
    "\n",
    "- 지금까지 가장 기본적인 seq2seq에 대해서 배웠다.\n",
    "- 사실 seq2seq는 어떻게 구현하느냐에 따라서 충분히 더 복잡해질 수 있다.\n",
    "  - 컨텍스트 벡터를 디코더의 초기 은닉 상태로만 사용하는 경우\n",
    "  - 컨텍스트 벡터를 디코더가 단어를 예측하는 매 시점마다 하나의 입력으로 사용하는 경우\n",
    "  - 어텐션 메커니즘이라는 방법을 통해 지금 알고있는 컨텍스트 벡터보다 더욱 문맥을 반영할 수 있는 컨텍스트 벡터를 구하여 매 시점마다 하나의 입력으로 사용하는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mU-c2qJ3P7V3"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 1.2 글자 레벨 기계 번역기(Character-Level Neural Machine Translation) 구현을 위한 전처리\n",
    "\n",
    "- 이제 seq2seq를 이용해서 기계 번역기를 만들어보도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zeyob_S_QUGx"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.2.1 좋은 게시물 소개\n",
    "\n",
    "- 인터넷에 케라스로 seq2seq를 구현하는 많은 유사 예제들이 나와있다.\n",
    "- 하지만 대부분은 케라스 개발자 프랑수아 숄레의 블로그의 유명 게시물인 'sequence-to-sequence 10분만에 이해하기'가 원본이다.\n",
    "- 이번 실습 또한 해당 게시물의 예제에 많이 영향받았다.\n",
    "- [해당 게시물 링크](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yChUxJZqQiEp"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.2.2 실습 데이터\n",
    "\n",
    "- 실제 성능이 좋은 기계 번역기를 구현하려면 정말 방대한 데이터가 필요하다.\n",
    "- 여기서는 방금 배운 seq2seq를 실습해보는 수준에서 아주 간단한 기계 번역기를 구축해보도록 한다.\n",
    "- 기계 번역기를 훈련시키기 위해서는 훈련 데이터로 **병렬 코퍼스(parallel corpus)가 필요**하다.\n",
    "- 병렬 코퍼스란, 두 개 이상의 언어가 병렬적으로 구성된 코퍼스를 의미한다.\n",
    "- [다운로드 링크](http://www.manythings.org/anki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z6K9XlvLQ5SA"
   },
   "source": [
    "- 이번 실습에서는 프랑스-영어 병렬 코퍼스인 `fra-eng.zip` 파일을 사용한다.\n",
    "- 위의 링크에서 해당 파일을 다운받으면 된다.\n",
    "- 해당 파일의 압축을 풀면 `fra.txt`라는 파일이 있는데 이 파일이 이번 실습에서 사용할 파일이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mz53w9WzREGn"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.2.3 병렬 코퍼스 데이터에 대한 이해\n",
    "\n",
    "- 병렬 데이터라고 하면 앞서 수행한 태깅 작업의 데이터를 생각할 수 있다.\n",
    "- 하지만 이 태깅 작업의 병렬 데이터와 seq2seq가 사용하는 병렬 데이터는 성격이 조금 다르다.\n",
    "  - 태깅 작업의 병렬 데이터 : 쌍이 되는 모든 데이터가 길이가 같음\n",
    "  - seq2seq의 병렬 데이터 : 쌍이 된다고 해서 길이가 같지 않음\n",
    "- 실제 번역기를 생각해보면 구글 번역기에 `'나는 학생이다.'` 라는 토큰의 개수가 2인 문장을 넣었을 때, `'I am a student.'` 라는 토큰의 개수가 4인 문장이 나오는 것과 같은 이치이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJEP7SfpRLIS"
   },
   "source": [
    "- seq2seq는 기본적으로 입력 시퀀스와 출력 시퀀스의 길이가 다를 수 있다고 가정한다.\n",
    "- 지금은 기계 번역기가 예제이지만, seq2seq의 또 다른 유명한 예제 중 하나인 챗봇을 만든다고 가정해보면, 대답의 길이가 질문의 길이와 항상 똑같아야 한다고 하면 그 또한 이상하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fSOl4O5fUurP"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.2.4 사용할 데이터의 구조\n",
    "\n",
    "\n",
    "```\n",
    "Watch me.           Regardez-moi !\n",
    "```\n",
    "\n",
    "- 여기서 사용할 `fra.txt` 데이터는 위와 같이 왼쪽의 영어 문장과 오른쪽의 프랑스어 문장 사이에 탭으로 구분되는 구조가 하나의 샘플이다.\n",
    "- 그리고 이와 같은 형식의 양 16만개의 병렬 문장 샘플을 포함하고 있다.\n",
    "- 해당 데이터를 읽고 전처리를 진행해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ymkmmQcVFXg"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.2.5 데이터 불러오기\n",
    "\n",
    "- 앞으로의 코드에서 `src`는 `source`의 줄임말로 입력 문장을 나타낸다.\n",
    "- 그리고 `tar`는 `target`의 줄임말로 변역하고자 하는 문장을 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "cb9h5TTHNvec",
    "outputId": "1f5dce5f-8a4d-4ed5-afcc-7611c5aced03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RdswGGnMVP3d",
    "outputId": "27bbb7ec-45c0-4f47-8e0e-79e424bbf196"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175623"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lines = pd.read_csv('/content/drive/My Drive/Colab Notebooks/datasets/fra-eng/fra.txt', names=['src', 'tar'], sep='\\t')\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "65CmxZJnVX_u"
   },
   "source": [
    "- 총 샘플의 개수는 약 16만 7천 개이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lY3Y3KfYVemN"
   },
   "source": [
    "- 여기서는 간단히 60,000개의 샘플만 가지고 기계 번역기를 구축해보도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gOtYuXEuVkJt",
    "outputId": "b720b334-0c01-46be-8908-9544c03ff5d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[0:60000]\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2w46h0mqVoO2"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.2.6 데이터의 구성\n",
    "\n",
    "- 현재 데이터가 어떤 구성이 되었는 지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lc1kC17kVubd"
   },
   "outputs": [],
   "source": [
    "lines.reset_index(inplace=True)\n",
    "lines.rename(columns={\"tar\": \"NOT_USE\", \"src\": \"tar\", \"index\": \"src\"}, inplace=True)\n",
    "lines.drop([\"NOT_USE\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "colab_type": "code",
    "id": "MxFMPqRUVvzN",
    "outputId": "78abb765-9fe0-4769-aa6c-5772dfde6058"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18725</th>\n",
       "      <td>I brought a salad.</td>\n",
       "      <td>J'ai apporté une salade.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15168</th>\n",
       "      <td>I'm 18 years old.</td>\n",
       "      <td>J'ai dix-huit ans.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23326</th>\n",
       "      <td>He lied to my face.</td>\n",
       "      <td>Il me mentit effrontément.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37273</th>\n",
       "      <td>That one's all yours.</td>\n",
       "      <td>Celui-ci est tout à toi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21886</th>\n",
       "      <td>Where's our stuff?</td>\n",
       "      <td>Où sont nos affaires ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13738</th>\n",
       "      <td>Do you work here?</td>\n",
       "      <td>Est-ce que vous travaillez ici ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6050</th>\n",
       "      <td>It was a test.</td>\n",
       "      <td>C'était un examen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18915</th>\n",
       "      <td>I got off lightly.</td>\n",
       "      <td>Je m'en suis tirée à bon compte.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9465</th>\n",
       "      <td>We're fighting.</td>\n",
       "      <td>Nous sommes en train de nous battre.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46166</th>\n",
       "      <td>Come on. Give it a try.</td>\n",
       "      <td>Allez. Essaye.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           src                                   tar\n",
       "18725       I brought a salad.              J'ai apporté une salade.\n",
       "15168        I'm 18 years old.                    J'ai dix-huit ans.\n",
       "23326      He lied to my face.            Il me mentit effrontément.\n",
       "37273    That one's all yours.              Celui-ci est tout à toi.\n",
       "21886       Where's our stuff?                Où sont nos affaires ?\n",
       "13738        Do you work here?      Est-ce que vous travaillez ici ?\n",
       "6050            It was a test.                    C'était un examen.\n",
       "18915       I got off lightly.      Je m'en suis tirée à bon compte.\n",
       "9465           We're fighting.  Nous sommes en train de nous battre.\n",
       "46166  Come on. Give it a try.                        Allez. Essaye."
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fXxQtBTOV7l8"
   },
   "source": [
    "- 위의 테이블은 랜덤으로 선택된 10개의 샘플을 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0XyqtzxlOicZ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.2.7 시작 심볼 및 종료 심볼 추가\n",
    "\n",
    "- 번역 문장에 해당되는 프랑스어 데이터는 앞서 배웠듯이 시작을 의미하는 심볼 `<sos>`와 종료를 의미하는 심볼 `<eos>`을 넣어주어야 한다.\n",
    "- 여기서는 `<sos>`와 `<eos>` 대신 `\\t`를 시작 심볼, `\\n`을 종료 심볼로 간주하여 추가하고 다시 데이터를 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "colab_type": "code",
    "id": "5-haqrlhOqgY",
    "outputId": "1ae427ec-c071-4238-a081-e35b112e128d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14447</th>\n",
       "      <td>I did that a lot.</td>\n",
       "      <td>\\t J'ai beaucoup fait ça. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34143</th>\n",
       "      <td>He doesn't tell lies.</td>\n",
       "      <td>\\t Il ne dit pas de mensonges. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26499</th>\n",
       "      <td>Tom twisted my arm.</td>\n",
       "      <td>\\t Tom m'a tordu le bras. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50768</th>\n",
       "      <td>Tom didn't go very far.</td>\n",
       "      <td>\\t Tom n'alla pas bien loin. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47985</th>\n",
       "      <td>I looked in the window.</td>\n",
       "      <td>\\t J'ai regardé dans la vitrine. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20906</th>\n",
       "      <td>There's no change.</td>\n",
       "      <td>\\t Il n'y a pas de changement. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28759</th>\n",
       "      <td>He's on sentry duty.</td>\n",
       "      <td>\\t Il est de garde. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5083</th>\n",
       "      <td>God bless you!</td>\n",
       "      <td>\\t Dieu vous bénisse ! \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31503</th>\n",
       "      <td>There's no elevator.</td>\n",
       "      <td>\\t Il n'y a pas d'ascenseur. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27039</th>\n",
       "      <td>Where do I go then?</td>\n",
       "      <td>\\t Où vais-je, alors ? \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           src                                  tar\n",
       "14447        I did that a lot.         \\t J'ai beaucoup fait ça. \\n\n",
       "34143    He doesn't tell lies.    \\t Il ne dit pas de mensonges. \\n\n",
       "26499      Tom twisted my arm.         \\t Tom m'a tordu le bras. \\n\n",
       "50768  Tom didn't go very far.      \\t Tom n'alla pas bien loin. \\n\n",
       "47985  I looked in the window.  \\t J'ai regardé dans la vitrine. \\n\n",
       "20906       There's no change.    \\t Il n'y a pas de changement. \\n\n",
       "28759     He's on sentry duty.               \\t Il est de garde. \\n\n",
       "5083            God bless you!            \\t Dieu vous bénisse ! \\n\n",
       "31503     There's no elevator.      \\t Il n'y a pas d'ascenseur. \\n\n",
       "27039      Where do I go then?            \\t Où vais-je, alors ? \\n"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.tar = lines.tar.apply(lambda x: '\\t ' + x + ' \\n')\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tcb1cxvbOyxi"
   },
   "source": [
    "- 프랑스어 데이터엣서 시작 심볼과 종료 심볼이 추가된 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NDm-9zpnO4vM"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.2.8 글자 집합 생성\n",
    "\n",
    "- 이제 글자 집합을 생성해보자.\n",
    "- 단어 집합이 아니라 글자 집합이라고 하는 이유는 토큰 단위가 단어가 아니라 글자이기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GLHj3VWXPCdH"
   },
   "outputs": [],
   "source": [
    "# 글자 집합 구축\n",
    "src_vocab = set()\n",
    "\n",
    "for line in lines.src:\n",
    "    for char in line:\n",
    "        src_vocab.add(char)\n",
    "\n",
    "tar_vocab = set()\n",
    "\n",
    "for line in lines.tar:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PvRf7loYPQ-h"
   },
   "source": [
    "<br>\n",
    "\n",
    "- 글자 집합의 크기를 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "9S8fpFK1PUdt",
    "outputId": "3902b3df-d500-4557-a58b-a0f8d25bf6e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(src_vocab) + 1\n",
    "tar_vocab_size = len(tar_vocab) + 1\n",
    "print(src_vocab_size)\n",
    "print(tar_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G7MPXhwTPdfw"
   },
   "source": [
    "- 영어와 프랑스어는 각각 약 80개와 100개의 글자가 존재한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Yuf46zSPkEd"
   },
   "source": [
    "<br>\n",
    "\n",
    "- 이 중에서 인덱스를 임의로 부여하여 일부만 출력해보자.\n",
    "- 현 상태에서 인덱스를 사용하려고 하면 에러가 난다.\n",
    "- 하지만 정렬하여 순서를 정해준 뒤에 인덱스를 사용하여 출력해주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "4qpT87UFPv2w",
    "outputId": "1a6690d1-6183-4c7a-e359-2dfbf256d731"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w']\n"
     ]
    }
   ],
   "source": [
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "print(src_vocab[45:75])\n",
    "print(tar_vocab[45:75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oicp5XvPP6gx"
   },
   "source": [
    "- 글자 집합에 글자 단위로 저장된 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iB0TARFZP_6G"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.2.9 글자 집합 인덱스 부여\n",
    "\n",
    "- 이제 각 글자에 인덱스를 부여한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YLvBpa6SQGN8"
   },
   "outputs": [],
   "source": [
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "3vuYeecjQQx_",
    "outputId": "521c03ba-0faf-46e3-b60c-0944000fb41a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, 'é': 76, '’': 77, '€': 78}\n"
     ]
    }
   ],
   "source": [
    "print(src_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "AbbWx0sCQY-V",
    "outputId": "0d7dd63a-a2ee-48c9-ce05-012c31328bae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, ',': 12, '-': 13, '.': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, 'a': 53, 'b': 54, 'c': 55, 'd': 56, 'e': 57, 'f': 58, 'g': 59, 'h': 60, 'i': 61, 'j': 62, 'k': 63, 'l': 64, 'm': 65, 'n': 66, 'o': 67, 'p': 68, 'q': 69, 'r': 70, 's': 71, 't': 72, 'u': 73, 'v': 74, 'w': 75, 'x': 76, 'y': 77, 'z': 78, '\\xa0': 79, '«': 80, '»': 81, 'À': 82, 'Ç': 83, 'É': 84, 'Ê': 85, 'Ô': 86, 'à': 87, 'â': 88, 'ç': 89, 'è': 90, 'é': 91, 'ê': 92, 'ë': 93, 'î': 94, 'ï': 95, 'ô': 96, 'ù': 97, 'û': 98, 'œ': 99, 'С': 100, '\\u2009': 101, '\\u200b': 102, '‘': 103, '’': 104, '\\u202f': 105}\n"
     ]
    }
   ],
   "source": [
    "print(tar_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AWSuCwbyQjim"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.2.10 정수 인코딩\n",
    "\n",
    "- 이제 인덱스가 부여된 글자 집합으로부터 갖고 있는 훈련 데이터에 정수 인코딩을 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oG3qElHHRdr3"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 1.2.10.1 인코더 입력 정수 인코딩\n",
    "\n",
    "- 우선 인코더의 입력이 될 영어 문장 샘플에 대해서 정수 인코딩을 수행해보고, 5개의 샘플을 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jC3ObABZQzHe"
   },
   "outputs": [],
   "source": [
    "encoder_input = []\n",
    "\n",
    "for line in lines.src:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        temp_X.append(src_to_index[w])\n",
    "    encoder_input.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Pj60peDzQ-sX",
    "outputId": "5e5257b4-cd2a-4da4-e0dd-1d399ea431f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30, 64, 10], [31, 58, 10], [31, 58, 10], [41, 70, 63, 2], [41, 70, 63, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a2RtvodRRByc"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 1.2.10.2 디코더 입력 정수 인코딩\n",
    "\n",
    "- 이제 디코더의 입력이 될 프랑스어 데이터에 대해서 정수 인코딩을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFQ8uKcSRJw9"
   },
   "outputs": [],
   "source": [
    "decoder_input = []\n",
    "\n",
    "for line in lines.tar:\n",
    "    temp_X = []\n",
    "\n",
    "    for w in line:\n",
    "        temp_X.append(tar_to_index[w])\n",
    "    decoder_input.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Zz3JUpFERUxo",
    "outputId": "be974e0c-d91a-429a-e006-b36f0d5f8d61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 48, 53, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 14, 3, 2], [1, 3, 29, 67, 73, 70, 71, 105, 4, 3, 2], [1, 3, 29, 67, 73, 70, 57, 78, 105, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g4sM1wcjRXMF"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 1.2.10.3 실제값 정수 인코딩\n",
    "\n",
    "- 아직 정수 인코딩을 수행해야 할 데이터가 하나 더 남아있다.\n",
    "- 디코더의 예측값과 비교하기 위한 실제값이 필요하다.\n",
    "- 그런데 이 실제값에는 시작 심볼에 해당되는 `<sos>`가 있을 필요가 없다.\n",
    "- 그래서 이번에는 정수 인코딩 과정에서 `<sos>`를 제거한다.\n",
    "- 즉, 모든 프랑스어 문장의 맨 앞에 붙어있는 `\\t`를 제거하도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5YP8tP2iSAu2"
   },
   "outputs": [],
   "source": [
    "decoder_target = []\n",
    "\n",
    "for line in lines.tar:\n",
    "    t = 0\n",
    "    temp_X = []\n",
    "\n",
    "    for w in line:\n",
    "        if t > 0:\n",
    "            temp_X.append(tar_to_index[w])\n",
    "        t = t+1\n",
    "\n",
    "    decoder_target.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "xR05Po-SSVBI",
    "outputId": "e5358a0f-2e8f-4b67-9690-8a5855d82c1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 48, 53, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 14, 3, 2], [3, 29, 67, 73, 70, 71, 105, 4, 3, 2], [3, 29, 67, 73, 70, 57, 78, 105, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_target[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FIdb1BBnSXFt"
   },
   "source": [
    "- 앞서 먼저 만들었던 디코더의 입력값에 해당되는 `decoder_input` 데이터와 비교하면 `decoder_input`에서는 모든 문장의 앞에 붙어있던 숫자 `1`이 `decoder_target`에서는 제거된 것을 볼 수 있다.\n",
    "- `\\t`의 인덱스가 `1`이므로 정상적으로 제거된 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4JrcoQU6Sk4d"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.2.11 패딩\n",
    "\n",
    "- 이제 모든 데이터에 대해서 정수 인덱스로 변경하였으니 패딩 작업을 수행한다.\n",
    "- 패딩을 위해서 영어 문장과 프랑스어 문장 각각에 대해서 가장 길이가 긴 샘플의 길이를 알아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "jtij6ZQYSzad",
    "outputId": "df9c5bb8-f01b-431a-87fc-4d7efe8d9047"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print(max_src_len)\n",
    "print(max_tar_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BURYHl-_S-FB"
   },
   "source": [
    "- 이번 병렬 데이터는 영어와 프랑스어의 길이는 하나의 쌍이라고 하더라도 전부 다르므로 패딩을 할 때도 이 두 개의 데이터의 길이를 전부 동일하게 맞춰줄 필요는 없다.\n",
    "- 영어 데이터는 영어 샘플들끼리, 프랑스어는 프랑스어 샘플들끼리 길이를 맞춰서 패딩을 하면 된다.\n",
    "- 여기서는 가장 긴 샘플의 길이에 맞춰서 영어 데이터의 샘플은 전부 길이가 25가 되도록 패딩하고, 프랑스어 데이터의 샘플은 전부 길이가 76이 되도록 패딩한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Je0fFhXJTalP"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "encoder_input = pad_sequences(encoder_input, padding='post', maxlen=max_src_len)\n",
    "decoder_input = pad_sequences(decoder_input, padding='post', maxlen=max_tar_len)\n",
    "decoder_target = pad_sequences(decoder_target, padding='post', maxlen=max_tar_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-AViJ9D2TvBm"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.2.12 원-핫 인코딩\n",
    "\n",
    "- 이제 모든 값에 대해서 원-핫 인코딩을 수행한다.\n",
    "- 글자 단위 번역기이므로 워드 임베딩은 별도로 사용되지 않는다.\n",
    "- 예측값과의 오차 측정에 사용되는 실제값뿐만 아니라 입력값도 원-핫 벡터를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6iYhdat9T-7w"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pAx-Wf8jUJcm"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 1.3 글자 레벨 기계 번역기(Character-Level Neural Machine Translation) 구현을 위한 seq2seq 모델 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "71Yb-yZIUgX1"
   },
   "source": [
    "### 1.3.1 교사 강요 (Teacher forcing)\n",
    "\n",
    "- 모델을 설계하기 전에 의아한 점이 있다.\n",
    "- 현재 시점의 디코더 셀의 입력은 오직 이전 디코더 셀의 출력을 입력으로 받는다고 했는데 `decoder_input`이 왜 필요할까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q0pY37hAUu5u"
   },
   "source": [
    "- 훈련 과정에서는 이전 시점의 디코더 셀의 출력을 현재 시점의 디코더 셀의 입력으로 넣어주지 않고, 이전 시점의 실제값을 현재 시점의 디코더 셀의 입력값으로 하는 방법을 사용할 것이다.\n",
    "- 그 이유는 이전 시점의 디코더 셀의 예측이 틀렸는 데 이를 현재 시점의 디코더 셀의 입력으로 사용하면 현재 시점의 디코더 셀의 예측도 잘못될 가능성이 높고 이는 연쇄 작용으로 디코더 전체의 예측을 어렵게 한다.\n",
    "- 이런 상황이 반복되면 훈련 시간이 느려진다.\n",
    "- 만약 이 상황을 원하지 않는다면 이전 시점의 디코더 셀의 예측값 대신 실제값을 현재 시점의 디코더 셀의 입력으로 사용하는 방법을 사용할 수 있다.\n",
    "- 이와 같이 RNN의 모든 시점에 대해서 이전 시점의 예측값 대신 실제값을 입력으로 주는 방법을 **교사 강요**라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl0wfWe2VU7w"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.3.2 seq2seq 기계 번역기 훈련 시키기\n",
    "\n",
    "- 이제 seq2seq 모델을 설계하고 교사 강요를 사용하여 훈련시켜보도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "60vmkGmsWY0m"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 1.3.2.1 인코더 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HyVmjN4uVeiu"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm = LSTM(units=256, return_state=True)\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "# encoder_outputs도 같이 리턴받기는 했지만 여기서는 필요없으므로 이 값은 버림\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# LSTM은 바닐라 RNN과 달리 상태가 두개이다. (은닉 상태, 셀 상태)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdLu6G75WGla"
   },
   "source": [
    "- 인코더를 주목해보면 functional API를 사용한다는 것 외에는 앞서 다른 실습에서 본 LSTM 설계와 크게 다르지는 않다.\n",
    "- 우선 LSTM의 은닉 상태 크기는 256으로 선택했다.\n",
    "- 인코더의 내부 상태를 디코더로 넘겨주어야 하기 때문에 `return_state=True`로 설정한다.\n",
    "- 이제 인코더에 입력을 넣으면 내부 상태를 리턴한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EObbiNY-WjhF"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 1.3.2.2 컨텍스트 벡터\n",
    "\n",
    "- LSTM에서 `state_h`, `state_c`를 리턴받는다.\n",
    "- 이는 각각 LSTM 챕터에서 배운 **은닉 상태**와 **셀 상태**에 해당된다.\n",
    "- 앞서 이론을 설명할 때는 셀 상태는 설명에서 생략하고 은닉 상태만 언급하였으나 사실 LSTM은 은닉 상태와 셀 상태라는 두 가지 상태를 가진다는 사실을 기억해야 한다.\n",
    "- 은닉 상태만 전달하는 게 아니라 은닉 상태와 셀 상태 두 가지를 전달한다고 생각하면 된다.\n",
    "- 이 두 가지 상태를 `encoder_states`에 저장한다.\n",
    "- `encoder_states`를 디코더에 전달하므로서 이 두 가지 상태 모두를 디코더로 전달한다.\n",
    "- 이 `encoder_states`가 바로 앞서 배운 **컨텍스트 벡터**입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WEOZnws5W71k"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 1.3.2.3 디코더 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nvU0FLrSXFF4"
   },
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "\n",
    "# initial_state=encoder_states : 디코더의 첫 상태를 인코더의 은닉 상태, 셀 상태로 한다.\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "084g622FX8AT"
   },
   "source": [
    "- 디코더는 인코더의 마지막 은닉 상태를 초기 은닉 상태로 사용한다.\n",
    "- 위에서 `initial_state`의 인자값으로 `encoder_states`를 주는 코드가 이에 해당된다.\n",
    "- 또한 동일하게 디코더의 은닉 상태 크기도 256으로 주었다.\n",
    "- 디코더도 은닉 상태, 셀 상태를 리턴하기는 하지만 훈련 과정에서는 사용하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OePAgwvuYEKf"
   },
   "source": [
    "- 그 후 출력층에 프랑스어의 단어 집합의 크기만큼 뉴런을 배치한 후 소프트맥스 함수를 사용하여 실제값과의 오차를 구한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gGP_5HcJXmdX"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 1.3.2.4 인코더와 디코더 결합 및 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1y1doJTsXuBV"
   },
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHI0XuAJYMOF"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 1.3.2.5 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wMzqv8LtYQko"
   },
   "outputs": [],
   "source": [
    "model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VEwJg7kkYbQk"
   },
   "source": [
    "- 입력으로는 인코더 입력과 디코더 입력이 들어가고, 디코더의 실제값인 `decoder_target`도 필요하다.\n",
    "- 배치 크기는 64로 하였으며 총 50 에포크를 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C_4uDFepYizO"
   },
   "source": [
    "- 위에서 설정한 은닉 상태의 크기와 에포크 수는 실제로는 훈련 데이터에 과적합 상태를 불러온다.\n",
    "- 중간부터 검증 데이터에 대한 오차인 `val_loss`의 값이 올라간다.\n",
    "- 사실 이번 실습에서는 주어진 데이터의 양과 태스크의 특성으로 인해 훈련 과정에서 훈련 데이터의 정확도와 과적합 방지라는 두 마리 토끼를 동시에 잡기에는 쉽지 않다.\n",
    "- 여기서는 우선 seq2seq의 메커니즘과 짧은 문장과 긴 문장에 대한 성능 차이에 대한 확인을 중점으로 두고 훈련 데이터에 과적합 된 상태로 동작 단계로 넘어간다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l0aej5iKYrA2"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 1.3.3 seq2seq 기계 번역기 동작시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B-uk1-0WYwZ2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch13_v01_seq2seq.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
