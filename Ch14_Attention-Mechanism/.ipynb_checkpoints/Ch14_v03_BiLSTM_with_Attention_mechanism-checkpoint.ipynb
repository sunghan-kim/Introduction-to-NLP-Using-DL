{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "by679DCfaLGv"
   },
   "source": [
    "# Ch14. 어텐션 메커니즘 (Attention Mechanism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ySTsQMUMaOF3"
   },
   "source": [
    "# v03. 양방향 LSTM과 어텐션 메커니즘(BiLSTM with Attention mechanism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_kBY1nxTaT0H"
   },
   "source": [
    "- 단방향 LSTM으로 텍스트 분류를 수행할 수도 있지만 때로는 양방향 LSTM을 사용하는 것이 더 강력하다.\n",
    "- 여기에 추가적으로 어텐션 메커니즘을 사용할 수 있다.\n",
    "- 양방향 LSTM과 어텐션 메커니즘으로 IMDB 리뷰 감성 분류하기를 수행해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pEg7rxfDa6wm"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 3.1 IMDB 리뷰 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dRDkh8Ija9Wy"
   },
   "source": [
    "### 3.1.1 필요한 모듈 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xYVVKDd1bBZx"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BqIOJPLobLPf"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.1.2 데이터 불러오기\n",
    "\n",
    "- 최대 단어 개수를 10,000으로 제한하고 훈련 데이터와 테스트 데이터를 받아온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "Qd9hM6MJbSBF",
    "outputId": "6c6bd1dd-636d-43e3-a964-8cff2a1aeb3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EtxU3iKRbYkN"
   },
   "source": [
    "- 훈련 데이터와 이에 대한 레이블이 각각 `X_train`, `y_train`에 저장되었다.\n",
    "- 테스트 데이터와 이에 대한 레이블이 각각 `X_test`, `y_test`에 저장되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dYtmbB4ubm50"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.1.3 정수 인코딩\n",
    "\n",
    "- IMDB 리뷰 데이터는 이미 정수 인코딩이 된 상태이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "drC1l5XmbrXl"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.1.4 리뷰 데이터의 길이 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "v-qID6bHbzck",
    "outputId": "b62e3313-922f-4411-dd96-a81528e2ec3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 2494\n",
      "리뷰의 평균 길이 : 238.71364\n"
     ]
    }
   ],
   "source": [
    "print('리뷰의 최대 길이 : {}'.format(max(len(l) for l in X_train)))\n",
    "print('리뷰의 평균 길이 : {}'.format(sum(map(len, X_train)) / len(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UsUYLvmRb_2w"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.1.5 리뷰 데이터 패딩\n",
    "\n",
    "- 평균 길이가 약 238이므로 이보다 조금 크게 500으로 데이터를 패딩한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2kXdXDdbcH9z"
   },
   "outputs": [],
   "source": [
    "max_len = 500\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XRpLbFTycQSD"
   },
   "source": [
    "- 훈련용 리뷰와 테스트용 리뷰의 길이가 둘 다 500이 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UcAimSRtcTPb"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.1.6 레이블 데이터 원-핫 인코딩\n",
    "\n",
    "- 이진 분류를 위해 소프트맥스 함수를 사용할 것이다.\n",
    "- 그러므로 `y_train`과 `y_test` 모두 원-핫 인코딩을 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3HwO5whPcgPk"
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0f4VOl7cjnr"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 3.2 바다나우 어텐션 (Bahdanau Attention)\n",
    "\n",
    "- 여기서 사용할 어텐션은 바다나우 어텐션(Bahdanau attention)이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OvxU8Fn2cyn7"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.2.1 닷 프로덕트 어텐션의 어텐션 스코어 함수\n",
    "\n",
    "- 이를 이해하기 위해 앞서 배운 가장 쉬운 어텐션이였던 닷 프로덕트 어텐션과 어텐션 스코어 함수의 정의를 상기해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q50Omt21c5TS"
   },
   "source": [
    "- 어텐션 스코어 함수란 주어진 query와 모든 key에 대해서 유사도를 측정하는 함수를 말한다.\n",
    "- 닷 프로덕트 어텐션에서는 query와 key의 유사도를 구하는 방법이 내적(dot product)이었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4jZ3EYCsdEjq"
   },
   "source": [
    "- 다음은 닷 프로덕트 어텐션의 어텐션 스코어 함수를 보여준다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "score(query, \\; key) = query^T \\, key\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_pY7JiSYdNii"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.2.2 바다나우 어텐션의 어텐션 스코어 함수\n",
    "\n",
    "- 바다나우 어텐션은 아래와 같은 어텐션 스코어 함수를 사용한다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "score(query, \\; key) = V^T \\, tanh \\left( W_1 \\, key + W_2 \\, query \\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IYOv0gzUdeKK"
   },
   "source": [
    "- 이 어텐션 스코어 함수를 사용하여 어텐션 메커니즘을 구현하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xt0P5dWWgW3d"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.2.3 텍스트 분류에 어텐션 메커니즘을 사용하는 이유\n",
    "\n",
    "- 텍스트 분류에서 어텐션 메커니즘을 사용하는 이유는 무엇일까?\n",
    "- RNN의 마지막 은닉 상태는 예측을 위해 사용된다.\n",
    "- 그런데 이 RNN의 마지막 은닉 상태는 몇 가지 유용한 정보들을 손실한 상태이다.\n",
    "- 그래서 RNN이 time step을 지나며 손실했던 정보들을 다시 참고하고자 한다.\n",
    "- 이는 다시 말해 RNN의 모든 은닉 상태들을 다시 한 번 참고하겠다는 것이다.\n",
    "- 그리고 이를 위해 어텐션 메커니즘을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XtAw_0O3gwKW"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.2.4 바다나우 어텐션 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1JlsRLEcgzWH"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, values, query): # 단, key와 value는 같음\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        # we are going this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-47w09FUiN0N"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 3.3 양방향 LSTM + 어텐션 메커니즘(BiLSTM with Attention Mechanism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3SkSxLm8iTG6"
   },
   "source": [
    "### 3.3.1 필요 모듈 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ASYDq5qLiVg8"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Concatenate, BatchNormalization\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import optimizers\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JakbdOsmih_5"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.2 모델 설계\n",
    "\n",
    "- 여기서는 케라스의 함수형 API를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I1AsOK7Nio55"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.3.2.1 입력층과 임베딩층 설계\n",
    "\n",
    "- 10,000개의 단어들을 128차원의 임베딩 벡터로 임베딩 하도록 설계한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0IX036Iiirjs"
   },
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(max_len, ), dtype='int32')\n",
    "embedded_sequences = Embedding(vocab_size, 128, input_length=max_len)(sequence_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ut3RP4J7i1Lo"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.3.2.2 양방향 LSTM 설계\n",
    "\n",
    "- 순방향 LSTM의 은닉 상태와 셀상태를 `forward_h`, `forward_c`에 저장한다.\n",
    "- 역방향 LSTM의 은닉 상태와 셀상태를 `backward_h`, `backward_c`에 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "ZogR55DskATu",
    "outputId": "504b247f-faaf-4f7f-9ede-9bbdaae8dc03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional(\n",
    "    LSTM(\n",
    "        128,\n",
    "         dropout=0.3,\n",
    "         return_sequences=True,\n",
    "         return_state=True,\n",
    "         recurrent_activation='relu',\n",
    "         recurrent_initializer='glorot_uniform'\n",
    "    )\n",
    ")(embedded_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UUspO-0tkVFe"
   },
   "source": [
    "<br>\n",
    "\n",
    "- 각 상태의 크기(shape)를 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "7CeF5ZbTkZLe",
    "outputId": "ba59afc6-c79b-46c5-fbd9-3c4327c0547a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 500, 256)\n"
     ]
    }
   ],
   "source": [
    "print(lstm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "418hK5TylFX8"
   },
   "source": [
    "- `lstm`의 경우에는 `(500 x 256)`의 크기를 가진다.\n",
    "- 이는 forward 방향과 backward 방향이 연결된 hidden state 벡터가 모든 시점에 대해서 존재함을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "FWK38r0Vkagm",
    "outputId": "dcd5b6a2-c014-4581-cfe2-b82dcf9c2caf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 128)\n",
      "(None, 128)\n"
     ]
    }
   ],
   "source": [
    "print(forward_h.shape)\n",
    "print(forward_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "iMsZMVPlkl_1",
    "outputId": "ab0edff2-fa53-46b0-c088-b781ae3b93a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 128)\n",
      "(None, 128)\n"
     ]
    }
   ],
   "source": [
    "print(backward_h.shape)\n",
    "print(backward_c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NiraIr8Okqp1"
   },
   "source": [
    "- 각 은닉 상태나 셀 상태의 경우에는 128차원을 가진다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3f7zsowolN9R"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.3.2.3 LSTM 상태들 연결(concatenate)\n",
    "\n",
    "- 양방향 LSTM을 사용할 경우에는 순방향 LSTM과 역방향 LSTM 각각 은닉 상태와 셀 상태를 가진다.\n",
    "- 그러므로 양방향 LSTM의 은닉 상태와 셀 상태를 사용하려면 두 방향의 LSTM의 상태들을 연결(concatenate)해주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hY5B6cerlf98"
   },
   "outputs": [],
   "source": [
    "state_h = Concatenate()([forward_h, backward_h]) # 은닉 상태\n",
    "state_c = Concatenate()([forward_c, backward_c]) # 셀 상태"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g_hcvVb2lqw_"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.3.2.4 어텐션 메커니즘 이용 컨텍스트 벡터 생성\n",
    "\n",
    "- 어텐션 메커니즘에서는 은닉 상태를 사용한다.\n",
    "- 이를 입력으로 컨텍스트 벡터(context vector)를 얻는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jtHdGHbMl0Sc"
   },
   "outputs": [],
   "source": [
    "attention = BahdanauAttention(128) # 가중치 크기 정의\n",
    "context_vector, attention_weights = attention(lstm, state_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "20cXxZ7Wmk9c"
   },
   "outputs": [],
   "source": [
    "hidden = BatchNormalization()(context_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AKBsk90ImoGD"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.3.2.5 출력층 설계\n",
    "\n",
    "- 이진 분류이므로 출력층에 2개의 뉴런을 배치한다.\n",
    "- 활성화 함수로는 소프트맥스 함수를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fJIDB8pyDFTl"
   },
   "outputs": [],
   "source": [
    "output = Dense(2, activation='softmax')(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "namqd301DTcb"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.3.2.6 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q5B14rz8JLwr"
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=sequence_input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "colab_type": "code",
    "id": "O1ghQb9gRa71",
    "outputId": "df73dfbc-15a0-418b-a1e5-185279e78d49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 500, 128)     1280000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   [(None, 500, 256), ( 263168      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           bidirectional[0][1]              \n",
      "                                                                 bidirectional[0][3]              \n",
      "__________________________________________________________________________________________________\n",
      "bahdanau_attention (BahdanauAtt ((None, 256), (None, 65921       bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 256)          1024        bahdanau_attention[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            514         batch_normalization[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 1,610,627\n",
      "Trainable params: 1,610,115\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puRBHt2yJPHK"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.3 옵티마이저 정의\n",
    "\n",
    "- 옵티마이저로 아담 옵티마이저를 정의한다.\n",
    "- `tf.keras.optimizers`에 있는 옵티마이저들은 그래디언트 클리핑을 위한 두 개의 매개변수를 제공한다.\n",
    "  1. `clipnorm` : L2 노름의 임계값을 지정\n",
    "  2. `clipvalue` : 절대값으로 임계값을 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1GwMALgPJnEk"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.3.3.1 `clipnorm`\n",
    "\n",
    "- `clipnorm` 매개변수가 설정되면 그래디언트의 L2 노름이 `clipnorm`보다 클 경우 다음과 같이 클리핑된 그래디언트를 계산한다.\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "\\text{클리핑된 그래디언트} = \\text{그래디언트} \\times \\text{clipnorm} \\; / \\; \\text{그래디언트의 L2 노름}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xhz_yyrgKCE3"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.3.3.2 `clipvalue`\n",
    "\n",
    "- `clipvalue` 매개변수가 설정되면\n",
    "  - `-clipvalue`보다 작은 그래디언트는 `-clipvalue`가 됨\n",
    "  - `clipvalue`보다 큰 그래디언트는 `clipvalue`로 만듬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ThMPrjosKNtq"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### 3.3.3.3 그래디언트 클리핑을 적용한 옵티마이저 정의\n",
    "\n",
    "- 위 두 클리핑 방식을 동시에 사용할 수도 있다.\n",
    "- 여기서는 `clipnorm` 매개변수만 지정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8fz9VYGKKie3"
   },
   "outputs": [],
   "source": [
    "Adam = optimizers.Adam(lr=0.0001, clipnorm=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EesbRTrDKlrn"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.4 모델 컴파일\n",
    "\n",
    "- 정의된 옵티마이저를 사용하여 모델을 컴파일한다.\n",
    "- 소프트맥스 함수를 사용하므로 손실 함수로 `categorical_crossentropy`를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6eAkDbc3KrcG"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Y_UfdTAKxm-"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.5 모델 훈련\n",
    "\n",
    "- 검증 데이터로 테스트 데이터를 사용하여 에포크가 끝날 때마다 테스트 데이터에 대한 정확도를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "colab_type": "code",
    "id": "XhkM9cmyLCOt",
    "outputId": "dfe73613-8690-43a0-e9f8-dd493ffeacb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "196/196 [==============================] - 310s 2s/step - loss: 0.6193 - accuracy: 0.6496 - val_loss: 0.6501 - val_accuracy: 0.7122\n",
      "Epoch 2/10\n",
      "196/196 [==============================] - 306s 2s/step - loss: 0.4450 - accuracy: 0.7934 - val_loss: 0.5286 - val_accuracy: 0.8205\n",
      "Epoch 3/10\n",
      "196/196 [==============================] - 306s 2s/step - loss: 0.3433 - accuracy: 0.8519 - val_loss: 0.3610 - val_accuracy: 0.8608\n",
      "Epoch 4/10\n",
      "196/196 [==============================] - 306s 2s/step - loss: 0.2744 - accuracy: 0.8868 - val_loss: 0.3524 - val_accuracy: 0.8442\n",
      "Epoch 5/10\n",
      "196/196 [==============================] - 306s 2s/step - loss: 0.2361 - accuracy: 0.9046 - val_loss: 0.3490 - val_accuracy: 0.8547\n",
      "Epoch 6/10\n",
      "196/196 [==============================] - 306s 2s/step - loss: 0.2048 - accuracy: 0.9182 - val_loss: 0.3845 - val_accuracy: 0.8548\n",
      "Epoch 7/10\n",
      "196/196 [==============================] - 306s 2s/step - loss: 0.1777 - accuracy: 0.9304 - val_loss: 0.3517 - val_accuracy: 0.8665\n",
      "Epoch 8/10\n",
      "196/196 [==============================] - 305s 2s/step - loss: 0.1623 - accuracy: 0.9374 - val_loss: 0.4843 - val_accuracy: 0.8330\n",
      "Epoch 9/10\n",
      "196/196 [==============================] - 305s 2s/step - loss: 0.1425 - accuracy: 0.9472 - val_loss: 0.3821 - val_accuracy: 0.8592\n",
      "Epoch 10/10\n",
      "196/196 [==============================] - 304s 2s/step - loss: 0.1252 - accuracy: 0.9528 - val_loss: 0.4283 - val_accuracy: 0.8488\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10, batch_size=128,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aq8jccbgLLh2"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.3.6 테스트 정확도 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "ZXZ0jJb-LW-1",
    "outputId": "0b67b4c2-03b6-4ce4-8db6-b8f9c976cd03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 86s 110ms/step - loss: 0.4278 - accuracy: 0.8488\n",
      "\n",
      " 테스트 정확도 : 0.8488\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n 테스트 정확도 : %.4f\" % (model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mXJwI39ZLdME"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 3.4 어텐션 메커니즘 참고 자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3HUUGWLLLiXt"
   },
   "source": [
    "### 3.4.1 NTM with Attention\n",
    "\n",
    "- [https://www.tensorflow.org/tutorials/text/nmt_with_attention](https://www.tensorflow.org/tutorials/text/nmt_with_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tOy5ifajLmRM"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.2 Text classification using BiLSTM with attention\n",
    "\n",
    "- [https://androidkt.com/text-classification-using-attention-mechanism-in-keras/](https://androidkt.com/text-classification-using-attention-mechanism-in-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FztmE2yzLt6s"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4.3 Neural Machine Translation With Attention Mechanism\n",
    "\n",
    "- [https://machinetalk.org/2019/03/29/neural-machine-translation-with-attention-mechanism/](https://machinetalk.org/2019/03/29/neural-machine-translation-with-attention-mechanism/)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Ch14_v03_BiLSTM-with-Attention-mechanism.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
