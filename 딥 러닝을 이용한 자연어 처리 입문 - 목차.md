# 목차

Chapter 01. 자연어 처리(Natural Language Processing)란?

- v01. 아나콘다(Anaconda)와 코랩(Colab)
- v02. 필요 프레임워크와 라이브러리
- v03. 자연어 처리를 위한 NLTK와 KoNLPy 설치하기
- v04. 판다스(Pandas) and 넘파이(Numpy) and 맷플롭립(Matplotlib)
- v05. 판다스 프로파일링(Pandas-Profiling)
- v06. 머신 러닝 워크플로우(Machine Learning Workflow)

<br>

Chapter 02. 텍스트 전처리 (Text preprocessing)

- v01. 토큰화(Tokenization)
- v02. 정제(Cleaning) and 정규화(Normalization)
- v03. 어간 추출(Stemming) and 표제어 추출(Lemmatization)
- v04. 불용어(Stopword)
- v05. 정규 표현식(Regular Expression)
- v06. 정수 인코딩(Integer Encoding)
- v07. 원-핫 인코딩(One-hot encoding)
- v08. 단어 분리하기(Byte Pair Encoding, BPE)
- v09. 데이터의 분리(Splitting Data)

<br>

Chapter 03. 언어 모델(Language Model)

- v01. 언어 모델(Language Model)이란?
- v02. 통계적 언어 모델(Statistical Language Model, SLM)
- v03. N-gram 언어 모델(N-gram Language Model)
- v04. 한국어에서의 언어 모델(Language Model for Korean Sentences)
- v05. 펄플렉서티(Perplexity)
- v06. 조건부 확률(Conditional Probability)

<br>

Chapter 04. 카운트 기반의 단어 표현(Count based word Representation)

- v01. 다양한 단어의 표현 방법
- v02. Bag of Words(BoW)
- v03. 문서 단어 행렬(Document-Term Matrix, DTM)
- v04. TF-IDF(Term Frequency-Inverse Document Frequency)

<br>

Chapter 05. 문서 유사도(Document Similarity)

- v01. 코사인 유사도(Cosine Similarity)
- v02. 여러가지 유사도 기법

<br>

Chapter 06. 토픽 모델링(Topic Modeling)

- v01. 잠재 의미 분석(Latent Semantic Analysis, LSA)
- v02. 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA)
- v03. 잠재 디리클레 할당(LDA) 실습2

<br>

Chapter 07. 머신 러닝(Machine Learning) 개요

- v01. 머신 러닝이란(What is Machine Learning?)
- v02. 머신 러닝 훑어보기
- v03. 선형 회귀(Linear Regression)
- v04. 로지스틱 회귀(Logistic Regression) - 이진 분류
- v05. 다중 입력에 대한 실습
- v06. 벡터와 행렬 연산
- v07. 소프트맥스 회귀(Softmax Regression) - 다중 클래스 분류

<br>

Chapter 08. 딥 러닝(Deep Learning) 개요

- v01. 퍼셉트론(Perceptron)
- v02. 인공 신경망(Artificial Neural Network) 훑어보기
- v03. 딥 러닝의 학습 방법
- v03-4. 역전파(BackPropagation) 이해하기
- v04. 과적합(Overfitting)을 막는 방법들
- v05. 기울기 소실(Gradient Vanishing)과 폭주(Exploding)
- v06. 케라스(Keras) 훑어보기
- v06-7.케라스의 함수형 API(Keras Functional API)
- v07. 다층 퍼셉트론(MultiLayer Perceptron, MLP)으로 텍스트 분류하기
- v08. 피드 포워드 신경망 언어 모델(Neural Network Language Model, NNLM)

<br>

Chapter 09. 순환 신경망(Recurrent Neural Network)

- v01. 순환 신경망(Recurrent Neural Network, RNN)
- v02. 장단기 메모리(Long Short-Term Memory, LSTM)
- v03. 게이트 순환 유닛(Gated Recurrent Unit, GRU)
- v04. RNN 언어 모델(Recurrent Neural Network Language Model, RNNLM)
- v05. RNN을 이용한 텍스트 생성(Text Generation using RNN)
- v06. 글자 단위 RNN(Char RNN)

<br>

Chapter 10. 워드 임베딩(Word Embedding)



